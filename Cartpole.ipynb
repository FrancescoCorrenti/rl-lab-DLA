{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a18c83b-448a-4679-8f19-0fa8d2c95508",
   "metadata": {},
   "source": [
    "# Getting up to speed with DRL\n",
    "\n",
    "In this notebook I provide a simple example of implementing a policy gradient Deep Reinforcement Learning algorithm to solve a control problem with continuous state space and discrete action space -- the venerable [CartPole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/). You should study the implementation in this notebook in preparation for the laboratory next Wednesday.\n",
    "\n",
    "This notebook should run in an environment with at least the following packages installed (the gpu version of PyTorch is not mandatory):\n",
    "\n",
    "     conda create -n DRL -c conda-forge gymnasium pytorch-gpu matplotlib pygame jupyterlab\n",
    "     \n",
    "Some background reading to get you started:\n",
    "\n",
    "1. We will be using the [Gymnasium](https://gymnasium.farama.org/) framework for all of our experiments. This framework provides a consistent interface to a broad range of reinforcement learning environments (including CartPole). You should familiarize yourself with how it works, how environments are specified, how to instantiate them, and how to interact with them.\n",
    "\n",
    "2. [This excellent blog post](http://karpathy.github.io/2016/05/31/rl/) is a great introduction to policy gradients, where they come from and how they work. Give it a read and I am sure it will help understand better what is going on in this notebook.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "We start with our standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b2a363-e3cd-4b22-8741-f3545e150281",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    }
   ],
   "source": [
    "# Standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Plus one non standard one -- we need this to sample from policies.\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# This *might* help PyGame crash less...\n",
    "import pygame\n",
    "_ = pygame.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd56607-0c95-449c-8724-8db4e54788fd",
   "metadata": {},
   "source": [
    "And also some utility functions useful for what comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c56fab-8522-40fa-b5f2-895b042c1c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Given an environment, observation, and policy, sample from pi(a | obs). Returns the\n",
    "# selected action and the log probability of that action (needed for policy gradient).\n",
    "def select_action(env, obs, policy):\n",
    "    dist = Categorical(policy(obs))\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return (action.item(), log_prob.reshape(1))\n",
    "\n",
    "# Utility to compute the discounted total reward. Torch doesn't like flipped arrays, so we need to\n",
    "# .copy() the final numpy array. There's probably a better way to do this.\n",
    "def compute_returns(rewards, gamma):\n",
    "    return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy()\n",
    "\n",
    "# Given an environment and a policy, run it up to the maximum number of steps.\n",
    "def run_episode(env, policy, maxlen=500):\n",
    "    # Collect just about everything.\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    \n",
    "    # Reset the environment and start the episode.\n",
    "    (obs, info) = env.reset()\n",
    "    for i in range(maxlen):\n",
    "        # Get the current observation, run the policy and select an action.\n",
    "        obs = torch.tensor(obs)\n",
    "        (action, log_prob) = select_action(env, obs, policy)\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # Advance the episode by executing the selected action.\n",
    "        (obs, reward, term, trunc, info) = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if term or trunc:\n",
    "            break\n",
    "    return (observations, actions, torch.cat(log_probs), rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dea044-14e1-44f1-a851-ef6e19452692",
   "metadata": {},
   "source": [
    "## The Policy network\n",
    "\n",
    "Here I provide a simple policy network which should work with any environment with continuous observations and discrete action spaces. Note how it uses the *specification* of the environment to configure its input and output spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8c3360-3671-4075-8ebe-9dfd7f1d1a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A simple, but generic, policy network with one hidden layer.\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], 128)\n",
    "        self.fc2 = nn.Linear(128, env.action_space.n)\n",
    "        \n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = F.softmax(self.fc2(s), dim=-1)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e2af5-62fd-422e-8fb7-06295c49ddc7",
   "metadata": {},
   "source": [
    "## The `REINFORCE` Algorithm\n",
    "\n",
    "This is a very simple implementation of the most basic policy gradient DRL algorithm: `REINFORCE`. It is a very direct implementation of the policy gradient update (although I use Adam instead of SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09393c40-def8-47ca-9a3c-8c7689d5f87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A direct, inefficient, and probably buggy of the REINFORCE policy gradient algorithm.\n",
    "def reinforce(policy, env, env_render=None, gamma=0.99, num_episodes=10):\n",
    "    # The only non-vanilla part: we use Adam instead of SGD.\n",
    "    opt = torch.optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "    # Track episode rewards in a list.\n",
    "    running_rewards = [0.0]\n",
    "    \n",
    "    # The main training loop.\n",
    "    policy.train()\n",
    "    for episode in range(num_episodes):\n",
    "        # Run an episode of the environment, collect everything needed for policy update.\n",
    "        (observations, actions, log_probs, rewards) = run_episode(env, policy)\n",
    "        # Compute the discounted reward for every step of the episode. \n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "        # Keep a running average of total discounted rewards for the whole episode.\n",
    "        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])\n",
    "        print(log_probs.shape,log_probs.max(),log_probs.min())\n",
    "        # Standardize returns.\n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "        # Make an optimization step\n",
    "        opt.zero_grad()\n",
    "        loss = (-log_probs * returns).mean()\n",
    "        print(f'Episode {episode}, Loss: {loss.item()}')\n",
    "\n",
    "        loss.backward()\n",
    "        for name, param in policy.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f'Gradient for {name}: {param.grad.norm().item()}')\n",
    "        opt.step()\n",
    "        \n",
    "        # Render an episode after every 100 policy updates.\n",
    "        if not episode % 100:\n",
    "            if env_render:\n",
    "                policy.eval()\n",
    "                run_episode(env_render, policy)\n",
    "                policy.train()\n",
    "            print(f'Running reward: {running_rewards[-1]}')\n",
    "    \n",
    "    # Return the running rewards.\n",
    "    policy.eval()\n",
    "    return running_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7086f041-a315-4f4e-9db6-01cb446fffa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add compatibility for different NumPy versions\n",
    "import numpy as np\n",
    "if not hasattr(np, 'bool8'):\n",
    "    np.bool8 = np.bool_\n",
    "\n",
    "# Instantiate a (rendering) CartPole environment.\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "pygame.display.init()  # Might help PyGame not crash...\n",
    "\n",
    "# Make a policy network and run a few episodes to see how well random initialization works.\n",
    "policy = PolicyNet(env_render)\n",
    "for _ in range(10):\n",
    "    run_episode(env_render, policy,maxlen=1000)\n",
    "    \n",
    "# If you don't close the environment, the PyGame window stays visible.\n",
    "env_render.close()\n",
    "\n",
    "# Again we pray PyGame doesn't crash...\n",
    "pygame.display.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adeffa1e-d23d-46ea-a3e4-e65251592529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52]) tensor(-0.5593, grad_fn=<MaxBackward1>) tensor(-0.8495, grad_fn=<MinBackward1>)\n",
      "Episode 0, Loss: 0.0068023731000721455\n",
      "Gradient for fc1.weight: 0.026546576991677284\n",
      "Gradient for fc1.bias: 0.02611197717487812\n",
      "Gradient for fc2.weight: 0.23256033658981323\n",
      "Gradient for fc2.bias: 0.05720098316669464\n",
      "Running reward: 2.0148160934448245\n",
      "torch.Size([30]) tensor(-0.4940, grad_fn=<MaxBackward1>) tensor(-0.9581, grad_fn=<MinBackward1>)\n",
      "Episode 1, Loss: -0.04129428789019585\n",
      "Gradient for fc1.weight: 0.10030333697795868\n",
      "Gradient for fc1.bias: 0.06113168224692345\n",
      "Gradient for fc2.weight: 0.7922154068946838\n",
      "Gradient for fc2.bias: 0.16190460324287415\n",
      "torch.Size([28]) tensor(-0.4653, grad_fn=<MaxBackward1>) tensor(-0.9194, grad_fn=<MinBackward1>)\n",
      "Episode 2, Loss: -0.00900153536349535\n",
      "Gradient for fc1.weight: 0.013807246461510658\n",
      "Gradient for fc1.bias: 0.014051605015993118\n",
      "Gradient for fc2.weight: 0.06945296376943588\n",
      "Gradient for fc2.bias: 0.011169900186359882\n",
      "torch.Size([36]) tensor(-0.2265, grad_fn=<MaxBackward1>) tensor(-1.3329, grad_fn=<MinBackward1>)\n",
      "Episode 3, Loss: -0.03933017700910568\n",
      "Gradient for fc1.weight: 0.06702251732349396\n",
      "Gradient for fc1.bias: 0.08080058544874191\n",
      "Gradient for fc2.weight: 0.49084731936454773\n",
      "Gradient for fc2.bias: 0.17026010155677795\n",
      "torch.Size([28]) tensor(-0.2219, grad_fn=<MaxBackward1>) tensor(-1.2897, grad_fn=<MinBackward1>)\n",
      "Episode 4, Loss: -0.012068213894963264\n",
      "Gradient for fc1.weight: 0.020831728354096413\n",
      "Gradient for fc1.bias: 0.023184383288025856\n",
      "Gradient for fc2.weight: 0.07586970925331116\n",
      "Gradient for fc2.bias: 0.015051908791065216\n",
      "torch.Size([48]) tensor(-0.2270, grad_fn=<MaxBackward1>) tensor(-1.2247, grad_fn=<MinBackward1>)\n",
      "Episode 5, Loss: 0.0005762465298175812\n",
      "Gradient for fc1.weight: 0.017717383801937103\n",
      "Gradient for fc1.bias: 0.028971565887331963\n",
      "Gradient for fc2.weight: 0.10593906044960022\n",
      "Gradient for fc2.bias: 0.04352220147848129\n",
      "torch.Size([69]) tensor(-0.1924, grad_fn=<MaxBackward1>) tensor(-1.3642, grad_fn=<MinBackward1>)\n",
      "Episode 6, Loss: 0.003550673834979534\n",
      "Gradient for fc1.weight: 0.03931678459048271\n",
      "Gradient for fc1.bias: 0.07020817697048187\n",
      "Gradient for fc2.weight: 0.294499933719635\n",
      "Gradient for fc2.bias: 0.1187523677945137\n",
      "torch.Size([37]) tensor(-0.1505, grad_fn=<MaxBackward1>) tensor(-1.4228, grad_fn=<MinBackward1>)\n",
      "Episode 7, Loss: -0.0006230223807506263\n",
      "Gradient for fc1.weight: 0.014204724691808224\n",
      "Gradient for fc1.bias: 0.020396089181303978\n",
      "Gradient for fc2.weight: 0.05367046222090721\n",
      "Gradient for fc2.bias: 0.012002266943454742\n",
      "torch.Size([45]) tensor(-0.1538, grad_fn=<MaxBackward1>) tensor(-1.3256, grad_fn=<MinBackward1>)\n",
      "Episode 8, Loss: -0.005841704085469246\n",
      "Gradient for fc1.weight: 0.017011841759085655\n",
      "Gradient for fc1.bias: 0.05227936804294586\n",
      "Gradient for fc2.weight: 0.14926472306251526\n",
      "Gradient for fc2.bias: 0.06963243335485458\n",
      "torch.Size([55]) tensor(-0.1266, grad_fn=<MaxBackward1>) tensor(-1.6833, grad_fn=<MinBackward1>)\n",
      "Episode 9, Loss: 0.025077037513256073\n",
      "Gradient for fc1.weight: 0.019441712647676468\n",
      "Gradient for fc1.bias: 0.02468702755868435\n",
      "Gradient for fc2.weight: 0.09060589224100113\n",
      "Gradient for fc2.bias: 0.03143113851547241\n",
      "torch.Size([57]) tensor(-0.1051, grad_fn=<MaxBackward1>) tensor(-1.6390, grad_fn=<MinBackward1>)\n",
      "Episode 10, Loss: 0.018974417820572853\n",
      "Gradient for fc1.weight: 0.01399421226233244\n",
      "Gradient for fc1.bias: 0.030393902212381363\n",
      "Gradient for fc2.weight: 0.09396728873252869\n",
      "Gradient for fc2.bias: 0.03990619257092476\n",
      "torch.Size([59]) tensor(-0.1287, grad_fn=<MaxBackward1>) tensor(-1.5746, grad_fn=<MinBackward1>)\n",
      "Episode 11, Loss: 0.0006502479081973433\n",
      "Gradient for fc1.weight: 0.011594490148127079\n",
      "Gradient for fc1.bias: 0.03044002689421177\n",
      "Gradient for fc2.weight: 0.051318760961294174\n",
      "Gradient for fc2.bias: 0.02157924696803093\n",
      "torch.Size([82]) tensor(-0.0753, grad_fn=<MaxBackward1>) tensor(-2.0962, grad_fn=<MinBackward1>)\n",
      "Episode 12, Loss: -0.0018891416257247329\n",
      "Gradient for fc1.weight: 0.02404993772506714\n",
      "Gradient for fc1.bias: 0.041158754378557205\n",
      "Gradient for fc2.weight: 0.14214305579662323\n",
      "Gradient for fc2.bias: 0.05807260423898697\n",
      "torch.Size([80]) tensor(-0.0719, grad_fn=<MaxBackward1>) tensor(-1.7932, grad_fn=<MinBackward1>)\n",
      "Episode 13, Loss: 0.03692462667822838\n",
      "Gradient for fc1.weight: 0.02342831715941429\n",
      "Gradient for fc1.bias: 0.04384877532720566\n",
      "Gradient for fc2.weight: 0.10051806271076202\n",
      "Gradient for fc2.bias: 0.02963131293654442\n",
      "torch.Size([44]) tensor(-0.1499, grad_fn=<MaxBackward1>) tensor(-1.4093, grad_fn=<MinBackward1>)\n",
      "Episode 14, Loss: -0.007203665096312761\n",
      "Gradient for fc1.weight: 0.01557162869721651\n",
      "Gradient for fc1.bias: 0.02655358612537384\n",
      "Gradient for fc2.weight: 0.039709873497486115\n",
      "Gradient for fc2.bias: 0.014372231438755989\n",
      "torch.Size([64]) tensor(-0.0678, grad_fn=<MaxBackward1>) tensor(-1.8266, grad_fn=<MinBackward1>)\n",
      "Episode 15, Loss: 0.0031012799590826035\n",
      "Gradient for fc1.weight: 0.01720861718058586\n",
      "Gradient for fc1.bias: 0.030709508806467056\n",
      "Gradient for fc2.weight: 0.08320468664169312\n",
      "Gradient for fc2.bias: 0.036148279905319214\n",
      "torch.Size([73]) tensor(-0.0935, grad_fn=<MaxBackward1>) tensor(-1.5855, grad_fn=<MinBackward1>)\n",
      "Episode 16, Loss: -0.029961517080664635\n",
      "Gradient for fc1.weight: 0.05568299815058708\n",
      "Gradient for fc1.bias: 0.06171926483511925\n",
      "Gradient for fc2.weight: 0.20006853342056274\n",
      "Gradient for fc2.bias: 0.06305588036775589\n",
      "torch.Size([54]) tensor(-0.0738, grad_fn=<MaxBackward1>) tensor(-1.6874, grad_fn=<MinBackward1>)\n",
      "Episode 17, Loss: 0.02304842695593834\n",
      "Gradient for fc1.weight: 0.032549016177654266\n",
      "Gradient for fc1.bias: 0.046862732619047165\n",
      "Gradient for fc2.weight: 0.15422120690345764\n",
      "Gradient for fc2.bias: 0.05407275632023811\n",
      "torch.Size([55]) tensor(-0.0832, grad_fn=<MaxBackward1>) tensor(-1.6870, grad_fn=<MinBackward1>)\n",
      "Episode 18, Loss: -0.010040500201284885\n",
      "Gradient for fc1.weight: 0.020412327721714973\n",
      "Gradient for fc1.bias: 0.027944615110754967\n",
      "Gradient for fc2.weight: 0.05776397883892059\n",
      "Gradient for fc2.bias: 0.00558886444196105\n",
      "torch.Size([51]) tensor(-0.1275, grad_fn=<MaxBackward1>) tensor(-1.5195, grad_fn=<MinBackward1>)\n",
      "Episode 19, Loss: -0.00941174291074276\n",
      "Gradient for fc1.weight: 0.016332237049937248\n",
      "Gradient for fc1.bias: 0.02253797836601734\n",
      "Gradient for fc2.weight: 0.07673219591379166\n",
      "Gradient for fc2.bias: 0.027258194983005524\n",
      "torch.Size([45]) tensor(-0.1251, grad_fn=<MaxBackward1>) tensor(-1.2843, grad_fn=<MinBackward1>)\n",
      "Episode 20, Loss: -0.00917579885572195\n",
      "Gradient for fc1.weight: 0.02999696135520935\n",
      "Gradient for fc1.bias: 0.03997448459267616\n",
      "Gradient for fc2.weight: 0.10371408611536026\n",
      "Gradient for fc2.bias: 0.03155360370874405\n",
      "torch.Size([31]) tensor(-0.0887, grad_fn=<MaxBackward1>) tensor(-1.5388, grad_fn=<MinBackward1>)\n",
      "Episode 21, Loss: 0.013393129222095013\n",
      "Gradient for fc1.weight: 0.05432955548167229\n",
      "Gradient for fc1.bias: 0.17108489573001862\n",
      "Gradient for fc2.weight: 0.4886310398578644\n",
      "Gradient for fc2.bias: 0.21605123579502106\n",
      "torch.Size([34]) tensor(-0.0488, grad_fn=<MaxBackward1>) tensor(-1.8979, grad_fn=<MinBackward1>)\n",
      "Episode 22, Loss: 0.003611757420003414\n",
      "Gradient for fc1.weight: 0.023545153439044952\n",
      "Gradient for fc1.bias: 0.040602706372737885\n",
      "Gradient for fc2.weight: 0.090220607817173\n",
      "Gradient for fc2.bias: 0.03518187254667282\n",
      "torch.Size([21]) tensor(-0.1079, grad_fn=<MaxBackward1>) tensor(-1.4195, grad_fn=<MinBackward1>)\n",
      "Episode 23, Loss: -0.012417761608958244\n",
      "Gradient for fc1.weight: 0.04154806211590767\n",
      "Gradient for fc1.bias: 0.10085189342498779\n",
      "Gradient for fc2.weight: 0.2919034957885742\n",
      "Gradient for fc2.bias: 0.12353996187448502\n",
      "torch.Size([37]) tensor(-0.0503, grad_fn=<MaxBackward1>) tensor(-1.8397, grad_fn=<MinBackward1>)\n",
      "Episode 24, Loss: 0.011983112432062626\n",
      "Gradient for fc1.weight: 0.021327299997210503\n",
      "Gradient for fc1.bias: 0.04174189642071724\n",
      "Gradient for fc2.weight: 0.04719327390193939\n",
      "Gradient for fc2.bias: 0.019758164882659912\n",
      "torch.Size([56]) tensor(-0.0603, grad_fn=<MaxBackward1>) tensor(-1.5961, grad_fn=<MinBackward1>)\n",
      "Episode 25, Loss: 0.02705121412873268\n",
      "Gradient for fc1.weight: 0.03957256302237511\n",
      "Gradient for fc1.bias: 0.0479518286883831\n",
      "Gradient for fc2.weight: 0.13642580807209015\n",
      "Gradient for fc2.bias: 0.03162059187889099\n",
      "torch.Size([19]) tensor(-0.1581, grad_fn=<MaxBackward1>) tensor(-1.3592, grad_fn=<MinBackward1>)\n",
      "Episode 26, Loss: 0.024747014045715332\n",
      "Gradient for fc1.weight: 0.05218258127570152\n",
      "Gradient for fc1.bias: 0.08934494107961655\n",
      "Gradient for fc2.weight: 0.2916595935821533\n",
      "Gradient for fc2.bias: 0.11629767715930939\n",
      "torch.Size([34]) tensor(-0.0362, grad_fn=<MaxBackward1>) tensor(-2.1186, grad_fn=<MinBackward1>)\n",
      "Episode 27, Loss: 0.011643506586551666\n",
      "Gradient for fc1.weight: 0.023143388330936432\n",
      "Gradient for fc1.bias: 0.03171832114458084\n",
      "Gradient for fc2.weight: 0.03986842930316925\n",
      "Gradient for fc2.bias: 0.008815187029540539\n",
      "torch.Size([18]) tensor(-0.2513, grad_fn=<MaxBackward1>) tensor(-1.0630, grad_fn=<MinBackward1>)\n",
      "Episode 28, Loss: -0.07959827780723572\n",
      "Gradient for fc1.weight: 0.10872241854667664\n",
      "Gradient for fc1.bias: 0.14061929285526276\n",
      "Gradient for fc2.weight: 0.4505265951156616\n",
      "Gradient for fc2.bias: 0.15792085230350494\n",
      "torch.Size([18]) tensor(-0.1940, grad_fn=<MaxBackward1>) tensor(-1.2343, grad_fn=<MinBackward1>)\n",
      "Episode 29, Loss: 0.014468159526586533\n",
      "Gradient for fc1.weight: 0.055519960820674896\n",
      "Gradient for fc1.bias: 0.11709200590848923\n",
      "Gradient for fc2.weight: 0.38097044825553894\n",
      "Gradient for fc2.bias: 0.15615695714950562\n",
      "torch.Size([20]) tensor(-0.1543, grad_fn=<MaxBackward1>) tensor(-1.4934, grad_fn=<MinBackward1>)\n",
      "Episode 30, Loss: 0.0004315957485232502\n",
      "Gradient for fc1.weight: 0.028505533933639526\n",
      "Gradient for fc1.bias: 0.06518562138080597\n",
      "Gradient for fc2.weight: 0.180669903755188\n",
      "Gradient for fc2.bias: 0.07370567321777344\n",
      "torch.Size([29]) tensor(-0.0724, grad_fn=<MaxBackward1>) tensor(-1.4720, grad_fn=<MinBackward1>)\n",
      "Episode 31, Loss: 0.024459730833768845\n",
      "Gradient for fc1.weight: 0.02989606373012066\n",
      "Gradient for fc1.bias: 0.11643006652593613\n",
      "Gradient for fc2.weight: 0.2505419850349426\n",
      "Gradient for fc2.bias: 0.11653805524110794\n",
      "torch.Size([25]) tensor(-0.1495, grad_fn=<MaxBackward1>) tensor(-1.1022, grad_fn=<MinBackward1>)\n",
      "Episode 32, Loss: -0.03650228679180145\n",
      "Gradient for fc1.weight: 0.04027704894542694\n",
      "Gradient for fc1.bias: 0.04363314062356949\n",
      "Gradient for fc2.weight: 0.1590612828731537\n",
      "Gradient for fc2.bias: 0.046611227095127106\n",
      "torch.Size([31]) tensor(-0.1516, grad_fn=<MaxBackward1>) tensor(-1.5669, grad_fn=<MinBackward1>)\n",
      "Episode 33, Loss: -0.09251686930656433\n",
      "Gradient for fc1.weight: 0.10065906494855881\n",
      "Gradient for fc1.bias: 0.10807335376739502\n",
      "Gradient for fc2.weight: 0.5133841633796692\n",
      "Gradient for fc2.bias: 0.16089244186878204\n",
      "torch.Size([21]) tensor(-0.0592, grad_fn=<MaxBackward1>) tensor(-1.6773, grad_fn=<MinBackward1>)\n",
      "Episode 34, Loss: -0.08778385818004608\n",
      "Gradient for fc1.weight: 0.15917274355888367\n",
      "Gradient for fc1.bias: 0.2345988154411316\n",
      "Gradient for fc2.weight: 0.9006742835044861\n",
      "Gradient for fc2.bias: 0.3296562731266022\n",
      "torch.Size([27]) tensor(-0.0561, grad_fn=<MaxBackward1>) tensor(-1.5637, grad_fn=<MinBackward1>)\n",
      "Episode 35, Loss: 0.0018325006822124124\n",
      "Gradient for fc1.weight: 0.05790199711918831\n",
      "Gradient for fc1.bias: 0.17541193962097168\n",
      "Gradient for fc2.weight: 0.4713025987148285\n",
      "Gradient for fc2.bias: 0.211153045296669\n",
      "torch.Size([36]) tensor(-0.1470, grad_fn=<MaxBackward1>) tensor(-1.4068, grad_fn=<MinBackward1>)\n",
      "Episode 36, Loss: -0.007283244747668505\n",
      "Gradient for fc1.weight: 0.033984169363975525\n",
      "Gradient for fc1.bias: 0.09822654724121094\n",
      "Gradient for fc2.weight: 0.2449190318584442\n",
      "Gradient for fc2.bias: 0.10835187882184982\n",
      "torch.Size([30]) tensor(-0.0719, grad_fn=<MaxBackward1>) tensor(-1.4338, grad_fn=<MinBackward1>)\n",
      "Episode 37, Loss: -0.013924646191298962\n",
      "Gradient for fc1.weight: 0.024015914648771286\n",
      "Gradient for fc1.bias: 0.05744050815701485\n",
      "Gradient for fc2.weight: 0.12046752870082855\n",
      "Gradient for fc2.bias: 0.05186240375041962\n",
      "torch.Size([23]) tensor(-0.0451, grad_fn=<MaxBackward1>) tensor(-2.4749, grad_fn=<MinBackward1>)\n",
      "Episode 38, Loss: -0.11021032184362411\n",
      "Gradient for fc1.weight: 0.08856302499771118\n",
      "Gradient for fc1.bias: 0.12792494893074036\n",
      "Gradient for fc2.weight: 0.511653482913971\n",
      "Gradient for fc2.bias: 0.17998279631137848\n",
      "torch.Size([60]) tensor(-0.0619, grad_fn=<MaxBackward1>) tensor(-2.0562, grad_fn=<MinBackward1>)\n",
      "Episode 39, Loss: -0.023338962346315384\n",
      "Gradient for fc1.weight: 0.01906065084040165\n",
      "Gradient for fc1.bias: 0.041965074837207794\n",
      "Gradient for fc2.weight: 0.03843513876199722\n",
      "Gradient for fc2.bias: 0.006654120050370693\n",
      "torch.Size([33]) tensor(-0.0538, grad_fn=<MaxBackward1>) tensor(-2.1043, grad_fn=<MinBackward1>)\n",
      "Episode 40, Loss: 0.07215962558984756\n",
      "Gradient for fc1.weight: 0.050693683326244354\n",
      "Gradient for fc1.bias: 0.09489646553993225\n",
      "Gradient for fc2.weight: 0.2613923251628876\n",
      "Gradient for fc2.bias: 0.10113712400197983\n",
      "torch.Size([40]) tensor(-0.0424, grad_fn=<MaxBackward1>) tensor(-1.6629, grad_fn=<MinBackward1>)\n",
      "Episode 41, Loss: -0.057156872004270554\n",
      "Gradient for fc1.weight: 0.044863469898700714\n",
      "Gradient for fc1.bias: 0.1250246912240982\n",
      "Gradient for fc2.weight: 0.2222571074962616\n",
      "Gradient for fc2.bias: 0.09945226460695267\n",
      "torch.Size([95]) tensor(-0.0158, grad_fn=<MaxBackward1>) tensor(-2.6339, grad_fn=<MinBackward1>)\n",
      "Episode 42, Loss: 0.03737124800682068\n",
      "Gradient for fc1.weight: 0.020297063514590263\n",
      "Gradient for fc1.bias: 0.03098735399544239\n",
      "Gradient for fc2.weight: 0.0766923651099205\n",
      "Gradient for fc2.bias: 0.02495517022907734\n",
      "torch.Size([49]) tensor(-0.0370, grad_fn=<MaxBackward1>) tensor(-2.3893, grad_fn=<MinBackward1>)\n",
      "Episode 43, Loss: 0.006964308675378561\n",
      "Gradient for fc1.weight: 0.04014482721686363\n",
      "Gradient for fc1.bias: 0.06576792150735855\n",
      "Gradient for fc2.weight: 0.10879282653331757\n",
      "Gradient for fc2.bias: 0.042493827641010284\n",
      "torch.Size([53]) tensor(-0.0089, grad_fn=<MaxBackward1>) tensor(-3.3151, grad_fn=<MinBackward1>)\n",
      "Episode 44, Loss: -0.002677013399079442\n",
      "Gradient for fc1.weight: 0.013083459809422493\n",
      "Gradient for fc1.bias: 0.06706450879573822\n",
      "Gradient for fc2.weight: 0.08948670327663422\n",
      "Gradient for fc2.bias: 0.04512612521648407\n",
      "torch.Size([34]) tensor(-0.0299, grad_fn=<MaxBackward1>) tensor(-2.5004, grad_fn=<MinBackward1>)\n",
      "Episode 45, Loss: -0.14434537291526794\n",
      "Gradient for fc1.weight: 0.08570797741413116\n",
      "Gradient for fc1.bias: 0.12218396365642548\n",
      "Gradient for fc2.weight: 0.34997138381004333\n",
      "Gradient for fc2.bias: 0.12046077102422714\n",
      "torch.Size([55]) tensor(-0.0416, grad_fn=<MaxBackward1>) tensor(-1.5753, grad_fn=<MinBackward1>)\n",
      "Episode 46, Loss: -0.0019332917872816324\n",
      "Gradient for fc1.weight: 0.020825857296586037\n",
      "Gradient for fc1.bias: 0.03810038045048714\n",
      "Gradient for fc2.weight: 0.054108425974845886\n",
      "Gradient for fc2.bias: 0.02160075306892395\n",
      "torch.Size([92]) tensor(-0.0035, grad_fn=<MaxBackward1>) tensor(-4.1655, grad_fn=<MinBackward1>)\n",
      "Episode 47, Loss: 0.013930569402873516\n",
      "Gradient for fc1.weight: 0.015976503491401672\n",
      "Gradient for fc1.bias: 0.040076423436403275\n",
      "Gradient for fc2.weight: 0.06438108533620834\n",
      "Gradient for fc2.bias: 0.027791112661361694\n",
      "torch.Size([45]) tensor(-0.0725, grad_fn=<MaxBackward1>) tensor(-1.3656, grad_fn=<MinBackward1>)\n",
      "Episode 48, Loss: -0.01915374957025051\n",
      "Gradient for fc1.weight: 0.023730777204036713\n",
      "Gradient for fc1.bias: 0.03701663762331009\n",
      "Gradient for fc2.weight: 0.04557538405060768\n",
      "Gradient for fc2.bias: 0.009496106766164303\n",
      "torch.Size([88]) tensor(-0.0141, grad_fn=<MaxBackward1>) tensor(-2.5146, grad_fn=<MinBackward1>)\n",
      "Episode 49, Loss: -0.026416398584842682\n",
      "Gradient for fc1.weight: 0.03996766358613968\n",
      "Gradient for fc1.bias: 0.09667734056711197\n",
      "Gradient for fc2.weight: 0.17715591192245483\n",
      "Gradient for fc2.bias: 0.07634080946445465\n",
      "torch.Size([160]) tensor(-0.0152, grad_fn=<MaxBackward1>) tensor(-2.4825, grad_fn=<MinBackward1>)\n",
      "Episode 50, Loss: -0.04059537127614021\n",
      "Gradient for fc1.weight: 0.027576671913266182\n",
      "Gradient for fc1.bias: 0.053760088980197906\n",
      "Gradient for fc2.weight: 0.11487862467765808\n",
      "Gradient for fc2.bias: 0.043275609612464905\n",
      "torch.Size([51]) tensor(-0.0393, grad_fn=<MaxBackward1>) tensor(-1.8995, grad_fn=<MinBackward1>)\n",
      "Episode 51, Loss: -0.0430125966668129\n",
      "Gradient for fc1.weight: 0.023433413356542587\n",
      "Gradient for fc1.bias: 0.08587760478258133\n",
      "Gradient for fc2.weight: 0.1413808912038803\n",
      "Gradient for fc2.bias: 0.0665951669216156\n",
      "torch.Size([71]) tensor(-0.0082, grad_fn=<MaxBackward1>) tensor(-3.1281, grad_fn=<MinBackward1>)\n",
      "Episode 52, Loss: 0.06216918304562569\n",
      "Gradient for fc1.weight: 0.06105761229991913\n",
      "Gradient for fc1.bias: 0.07905891537666321\n",
      "Gradient for fc2.weight: 0.16999536752700806\n",
      "Gradient for fc2.bias: 0.06073090061545372\n",
      "torch.Size([50]) tensor(-0.0305, grad_fn=<MaxBackward1>) tensor(-1.8978, grad_fn=<MinBackward1>)\n",
      "Episode 53, Loss: -0.03098582662642002\n",
      "Gradient for fc1.weight: 0.02620554342865944\n",
      "Gradient for fc1.bias: 0.15483616292476654\n",
      "Gradient for fc2.weight: 0.22027437388896942\n",
      "Gradient for fc2.bias: 0.1104193925857544\n",
      "torch.Size([95]) tensor(-0.0190, grad_fn=<MaxBackward1>) tensor(-2.5538, grad_fn=<MinBackward1>)\n",
      "Episode 54, Loss: -0.030353402718901634\n",
      "Gradient for fc1.weight: 0.05267125740647316\n",
      "Gradient for fc1.bias: 0.1162521094083786\n",
      "Gradient for fc2.weight: 0.212018221616745\n",
      "Gradient for fc2.bias: 0.08598676323890686\n",
      "torch.Size([48]) tensor(-0.0240, grad_fn=<MaxBackward1>) tensor(-2.0524, grad_fn=<MinBackward1>)\n",
      "Episode 55, Loss: -0.018922584131360054\n",
      "Gradient for fc1.weight: 0.034154657274484634\n",
      "Gradient for fc1.bias: 0.08945115655660629\n",
      "Gradient for fc2.weight: 0.15115545690059662\n",
      "Gradient for fc2.bias: 0.07089171558618546\n",
      "torch.Size([68]) tensor(-0.0081, grad_fn=<MaxBackward1>) tensor(-2.9140, grad_fn=<MinBackward1>)\n",
      "Episode 56, Loss: -0.005569671746343374\n",
      "Gradient for fc1.weight: 0.01689714938402176\n",
      "Gradient for fc1.bias: 0.06546279042959213\n",
      "Gradient for fc2.weight: 0.0891839787364006\n",
      "Gradient for fc2.bias: 0.04280933737754822\n",
      "torch.Size([63]) tensor(-0.0141, grad_fn=<MaxBackward1>) tensor(-2.7991, grad_fn=<MinBackward1>)\n",
      "Episode 57, Loss: -0.07897887378931046\n",
      "Gradient for fc1.weight: 0.0396917387843132\n",
      "Gradient for fc1.bias: 0.08869270980358124\n",
      "Gradient for fc2.weight: 0.14083676040172577\n",
      "Gradient for fc2.bias: 0.058006756007671356\n",
      "torch.Size([36]) tensor(-0.0121, grad_fn=<MaxBackward1>) tensor(-3.0191, grad_fn=<MinBackward1>)\n",
      "Episode 58, Loss: -0.009289715439081192\n",
      "Gradient for fc1.weight: 0.04804880917072296\n",
      "Gradient for fc1.bias: 0.14437855780124664\n",
      "Gradient for fc2.weight: 0.21913765370845795\n",
      "Gradient for fc2.bias: 0.10506411641836166\n",
      "torch.Size([43]) tensor(-0.0133, grad_fn=<MaxBackward1>) tensor(-2.6359, grad_fn=<MinBackward1>)\n",
      "Episode 59, Loss: 0.1174302026629448\n",
      "Gradient for fc1.weight: 0.046138063073158264\n",
      "Gradient for fc1.bias: 0.05789307504892349\n",
      "Gradient for fc2.weight: 0.11928953230381012\n",
      "Gradient for fc2.bias: 0.0037453032564371824\n",
      "torch.Size([71]) tensor(-0.0080, grad_fn=<MaxBackward1>) tensor(-3.0836, grad_fn=<MinBackward1>)\n",
      "Episode 60, Loss: 0.041953638195991516\n",
      "Gradient for fc1.weight: 0.028173865750432014\n",
      "Gradient for fc1.bias: 0.046673115342855453\n",
      "Gradient for fc2.weight: 0.09183257818222046\n",
      "Gradient for fc2.bias: 0.03608012944459915\n",
      "torch.Size([35]) tensor(-0.0097, grad_fn=<MaxBackward1>) tensor(-3.1529, grad_fn=<MinBackward1>)\n",
      "Episode 61, Loss: -0.019661137834191322\n",
      "Gradient for fc1.weight: 0.027884146198630333\n",
      "Gradient for fc1.bias: 0.05974084511399269\n",
      "Gradient for fc2.weight: 0.10002685338258743\n",
      "Gradient for fc2.bias: 0.04319722205400467\n",
      "torch.Size([56]) tensor(-0.0226, grad_fn=<MaxBackward1>) tensor(-2.1685, grad_fn=<MinBackward1>)\n",
      "Episode 62, Loss: -0.005409306846559048\n",
      "Gradient for fc1.weight: 0.03441397473216057\n",
      "Gradient for fc1.bias: 0.13116183876991272\n",
      "Gradient for fc2.weight: 0.20703382790088654\n",
      "Gradient for fc2.bias: 0.10040897130966187\n",
      "torch.Size([107]) tensor(-0.0115, grad_fn=<MaxBackward1>) tensor(-2.5423, grad_fn=<MinBackward1>)\n",
      "Episode 63, Loss: 0.0332353450357914\n",
      "Gradient for fc1.weight: 0.016042081639170647\n",
      "Gradient for fc1.bias: 0.1574646234512329\n",
      "Gradient for fc2.weight: 0.2083449810743332\n",
      "Gradient for fc2.bias: 0.1039925068616867\n",
      "torch.Size([97]) tensor(-0.0174, grad_fn=<MaxBackward1>) tensor(-2.0918, grad_fn=<MinBackward1>)\n",
      "Episode 64, Loss: -0.018102163448929787\n",
      "Gradient for fc1.weight: 0.012767213396728039\n",
      "Gradient for fc1.bias: 0.07073967903852463\n",
      "Gradient for fc2.weight: 0.09162257611751556\n",
      "Gradient for fc2.bias: 0.04572315514087677\n",
      "torch.Size([46]) tensor(-0.0353, grad_fn=<MaxBackward1>) tensor(-1.6937, grad_fn=<MinBackward1>)\n",
      "Episode 65, Loss: -0.0014764141524210572\n",
      "Gradient for fc1.weight: 0.0630703940987587\n",
      "Gradient for fc1.bias: 0.16026245057582855\n",
      "Gradient for fc2.weight: 0.27136796712875366\n",
      "Gradient for fc2.bias: 0.12627282738685608\n",
      "torch.Size([34]) tensor(-0.0099, grad_fn=<MaxBackward1>) tensor(-3.0488, grad_fn=<MinBackward1>)\n",
      "Episode 66, Loss: 0.05329321324825287\n",
      "Gradient for fc1.weight: 0.037923187017440796\n",
      "Gradient for fc1.bias: 0.051132429391145706\n",
      "Gradient for fc2.weight: 0.06818192452192307\n",
      "Gradient for fc2.bias: 0.003411276265978813\n",
      "torch.Size([69]) tensor(-0.0264, grad_fn=<MaxBackward1>) tensor(-1.7701, grad_fn=<MinBackward1>)\n",
      "Episode 67, Loss: 0.006559340748935938\n",
      "Gradient for fc1.weight: 0.024897240102291107\n",
      "Gradient for fc1.bias: 0.08805142343044281\n",
      "Gradient for fc2.weight: 0.11269651353359222\n",
      "Gradient for fc2.bias: 0.054944880306720734\n",
      "torch.Size([125]) tensor(-0.0159, grad_fn=<MaxBackward1>) tensor(-2.4080, grad_fn=<MinBackward1>)\n",
      "Episode 68, Loss: 0.037761881947517395\n",
      "Gradient for fc1.weight: 0.017627041786909103\n",
      "Gradient for fc1.bias: 0.09062010049819946\n",
      "Gradient for fc2.weight: 0.12732842564582825\n",
      "Gradient for fc2.bias: 0.05973197519779205\n",
      "torch.Size([31]) tensor(-0.0026, grad_fn=<MaxBackward1>) tensor(-4.2481, grad_fn=<MinBackward1>)\n",
      "Episode 69, Loss: 0.0067674508318305016\n",
      "Gradient for fc1.weight: 0.015984296798706055\n",
      "Gradient for fc1.bias: 0.07702472805976868\n",
      "Gradient for fc2.weight: 0.08719684183597565\n",
      "Gradient for fc2.bias: 0.0469888411462307\n",
      "torch.Size([91]) tensor(-0.0098, grad_fn=<MaxBackward1>) tensor(-3.0601, grad_fn=<MinBackward1>)\n",
      "Episode 70, Loss: -0.025240564718842506\n",
      "Gradient for fc1.weight: 0.0213770754635334\n",
      "Gradient for fc1.bias: 0.04532874375581741\n",
      "Gradient for fc2.weight: 0.058556169271469116\n",
      "Gradient for fc2.bias: 0.024183155968785286\n",
      "torch.Size([43]) tensor(-0.0063, grad_fn=<MaxBackward1>) tensor(-3.2188, grad_fn=<MinBackward1>)\n",
      "Episode 71, Loss: -0.04580267518758774\n",
      "Gradient for fc1.weight: 0.08452441543340683\n",
      "Gradient for fc1.bias: 0.16087347269058228\n",
      "Gradient for fc2.weight: 0.30826643109321594\n",
      "Gradient for fc2.bias: 0.12428375333547592\n",
      "torch.Size([41]) tensor(-0.0189, grad_fn=<MaxBackward1>) tensor(-2.0058, grad_fn=<MinBackward1>)\n",
      "Episode 72, Loss: -0.05162504315376282\n",
      "Gradient for fc1.weight: 0.04631925746798515\n",
      "Gradient for fc1.bias: 0.044112805277109146\n",
      "Gradient for fc2.weight: 0.10110003501176834\n",
      "Gradient for fc2.bias: 0.023677943274378777\n",
      "torch.Size([34]) tensor(-0.0110, grad_fn=<MaxBackward1>) tensor(-2.8100, grad_fn=<MinBackward1>)\n",
      "Episode 73, Loss: 0.03125010430812836\n",
      "Gradient for fc1.weight: 0.028023460879921913\n",
      "Gradient for fc1.bias: 0.18070562183856964\n",
      "Gradient for fc2.weight: 0.2706417143344879\n",
      "Gradient for fc2.bias: 0.12949220836162567\n",
      "torch.Size([46]) tensor(-0.0364, grad_fn=<MaxBackward1>) tensor(-1.6193, grad_fn=<MinBackward1>)\n",
      "Episode 74, Loss: -0.01715955138206482\n",
      "Gradient for fc1.weight: 0.08628176152706146\n",
      "Gradient for fc1.bias: 0.19332648813724518\n",
      "Gradient for fc2.weight: 0.3171640932559967\n",
      "Gradient for fc2.bias: 0.13374485075473785\n",
      "torch.Size([69]) tensor(-0.0159, grad_fn=<MaxBackward1>) tensor(-2.4167, grad_fn=<MinBackward1>)\n",
      "Episode 75, Loss: -0.014921708032488823\n",
      "Gradient for fc1.weight: 0.018396221101284027\n",
      "Gradient for fc1.bias: 0.023514997214078903\n",
      "Gradient for fc2.weight: 0.03478046506643295\n",
      "Gradient for fc2.bias: 0.003503891173750162\n",
      "torch.Size([78]) tensor(-0.0144, grad_fn=<MaxBackward1>) tensor(-2.5454, grad_fn=<MinBackward1>)\n",
      "Episode 76, Loss: -0.03802845999598503\n",
      "Gradient for fc1.weight: 0.04688650369644165\n",
      "Gradient for fc1.bias: 0.12836991250514984\n",
      "Gradient for fc2.weight: 0.21622693538665771\n",
      "Gradient for fc2.bias: 0.09820560365915298\n",
      "torch.Size([53]) tensor(-0.0196, grad_fn=<MaxBackward1>) tensor(-1.9339, grad_fn=<MinBackward1>)\n",
      "Episode 77, Loss: -0.030934814363718033\n",
      "Gradient for fc1.weight: 0.05986832082271576\n",
      "Gradient for fc1.bias: 0.1905003786087036\n",
      "Gradient for fc2.weight: 0.2901950478553772\n",
      "Gradient for fc2.bias: 0.131191685795784\n",
      "torch.Size([84]) tensor(-0.0200, grad_fn=<MaxBackward1>) tensor(-2.3519, grad_fn=<MinBackward1>)\n",
      "Episode 78, Loss: -0.08842523396015167\n",
      "Gradient for fc1.weight: 0.05060848221182823\n",
      "Gradient for fc1.bias: 0.07912681996822357\n",
      "Gradient for fc2.weight: 0.14681389927864075\n",
      "Gradient for fc2.bias: 0.0572386234998703\n",
      "torch.Size([50]) tensor(-0.0089, grad_fn=<MaxBackward1>) tensor(-2.8316, grad_fn=<MinBackward1>)\n",
      "Episode 79, Loss: -0.04801298305392265\n",
      "Gradient for fc1.weight: 0.03646795079112053\n",
      "Gradient for fc1.bias: 0.06550702452659607\n",
      "Gradient for fc2.weight: 0.07533767819404602\n",
      "Gradient for fc2.bias: 0.021106358617544174\n",
      "torch.Size([52]) tensor(-0.0067, grad_fn=<MaxBackward1>) tensor(-3.3036, grad_fn=<MinBackward1>)\n",
      "Episode 80, Loss: 0.022846726700663567\n",
      "Gradient for fc1.weight: 0.030943233519792557\n",
      "Gradient for fc1.bias: 0.12032319605350494\n",
      "Gradient for fc2.weight: 0.18373380601406097\n",
      "Gradient for fc2.bias: 0.08526545017957687\n",
      "torch.Size([65]) tensor(-0.0030, grad_fn=<MaxBackward1>) tensor(-4.1961, grad_fn=<MinBackward1>)\n",
      "Episode 81, Loss: -0.02852381393313408\n",
      "Gradient for fc1.weight: 0.04035746678709984\n",
      "Gradient for fc1.bias: 0.06545957922935486\n",
      "Gradient for fc2.weight: 0.13117481768131256\n",
      "Gradient for fc2.bias: 0.05412604287266731\n",
      "torch.Size([33]) tensor(-0.0143, grad_fn=<MaxBackward1>) tensor(-2.6180, grad_fn=<MinBackward1>)\n",
      "Episode 82, Loss: -0.07025720924139023\n",
      "Gradient for fc1.weight: 0.06793855130672455\n",
      "Gradient for fc1.bias: 0.2583810091018677\n",
      "Gradient for fc2.weight: 0.40392395853996277\n",
      "Gradient for fc2.bias: 0.1828264445066452\n",
      "torch.Size([62]) tensor(-0.0174, grad_fn=<MaxBackward1>) tensor(-1.9917, grad_fn=<MinBackward1>)\n",
      "Episode 83, Loss: -0.022255532443523407\n",
      "Gradient for fc1.weight: 0.02676638588309288\n",
      "Gradient for fc1.bias: 0.025811754167079926\n",
      "Gradient for fc2.weight: 0.04956948012113571\n",
      "Gradient for fc2.bias: 0.009228109382092953\n",
      "torch.Size([75]) tensor(-0.0080, grad_fn=<MaxBackward1>) tensor(-3.1714, grad_fn=<MinBackward1>)\n",
      "Episode 84, Loss: -0.05806358903646469\n",
      "Gradient for fc1.weight: 0.035655468702316284\n",
      "Gradient for fc1.bias: 0.12187442183494568\n",
      "Gradient for fc2.weight: 0.17985093593597412\n",
      "Gradient for fc2.bias: 0.0825769305229187\n",
      "torch.Size([55]) tensor(-0.0036, grad_fn=<MaxBackward1>) tensor(-3.8409, grad_fn=<MinBackward1>)\n",
      "Episode 85, Loss: -0.10326848179101944\n",
      "Gradient for fc1.weight: 0.06317105889320374\n",
      "Gradient for fc1.bias: 0.09454859048128128\n",
      "Gradient for fc2.weight: 0.18749001622200012\n",
      "Gradient for fc2.bias: 0.0626349076628685\n",
      "torch.Size([30]) tensor(-0.0130, grad_fn=<MaxBackward1>) tensor(-2.1368, grad_fn=<MinBackward1>)\n",
      "Episode 86, Loss: 0.02381056360900402\n",
      "Gradient for fc1.weight: 0.025998136028647423\n",
      "Gradient for fc1.bias: 0.0358082614839077\n",
      "Gradient for fc2.weight: 0.046693768352270126\n",
      "Gradient for fc2.bias: 0.01194152794778347\n",
      "torch.Size([45]) tensor(-0.0159, grad_fn=<MaxBackward1>) tensor(-2.0225, grad_fn=<MinBackward1>)\n",
      "Episode 87, Loss: -0.05615708976984024\n",
      "Gradient for fc1.weight: 0.02541922591626644\n",
      "Gradient for fc1.bias: 0.032852791249752045\n",
      "Gradient for fc2.weight: 0.05073590204119682\n",
      "Gradient for fc2.bias: 0.0014650485245510936\n",
      "torch.Size([35]) tensor(-0.0361, grad_fn=<MaxBackward1>) tensor(-1.5899, grad_fn=<MinBackward1>)\n",
      "Episode 88, Loss: -0.08200649172067642\n",
      "Gradient for fc1.weight: 0.06398534774780273\n",
      "Gradient for fc1.bias: 0.10436689853668213\n",
      "Gradient for fc2.weight: 0.18696831166744232\n",
      "Gradient for fc2.bias: 0.07031736522912979\n",
      "torch.Size([55]) tensor(-0.0130, grad_fn=<MaxBackward1>) tensor(-1.9946, grad_fn=<MinBackward1>)\n",
      "Episode 89, Loss: 0.017389444634318352\n",
      "Gradient for fc1.weight: 0.02688133530318737\n",
      "Gradient for fc1.bias: 0.06787683069705963\n",
      "Gradient for fc2.weight: 0.10159162431955338\n",
      "Gradient for fc2.bias: 0.042747579514980316\n",
      "torch.Size([43]) tensor(-0.0254, grad_fn=<MaxBackward1>) tensor(-1.6981, grad_fn=<MinBackward1>)\n",
      "Episode 90, Loss: -0.009451594203710556\n",
      "Gradient for fc1.weight: 0.01628836616873741\n",
      "Gradient for fc1.bias: 0.048011742532253265\n",
      "Gradient for fc2.weight: 0.07079890370368958\n",
      "Gradient for fc2.bias: 0.030972080305218697\n",
      "torch.Size([23]) tensor(-0.0274, grad_fn=<MaxBackward1>) tensor(-1.9719, grad_fn=<MinBackward1>)\n",
      "Episode 91, Loss: -0.13217711448669434\n",
      "Gradient for fc1.weight: 0.10870780050754547\n",
      "Gradient for fc1.bias: 0.13807854056358337\n",
      "Gradient for fc2.weight: 0.29510945081710815\n",
      "Gradient for fc2.bias: 0.09462899714708328\n",
      "torch.Size([40]) tensor(-0.0041, grad_fn=<MaxBackward1>) tensor(-3.1718, grad_fn=<MinBackward1>)\n",
      "Episode 92, Loss: 0.062380265444517136\n",
      "Gradient for fc1.weight: 0.04727254435420036\n",
      "Gradient for fc1.bias: 0.050328921526670456\n",
      "Gradient for fc2.weight: 0.07639537751674652\n",
      "Gradient for fc2.bias: 0.01166264247149229\n",
      "torch.Size([25]) tensor(-0.0090, grad_fn=<MaxBackward1>) tensor(-3.0777, grad_fn=<MinBackward1>)\n",
      "Episode 93, Loss: 0.16801872849464417\n",
      "Gradient for fc1.weight: 0.1011747494339943\n",
      "Gradient for fc1.bias: 0.1662241816520691\n",
      "Gradient for fc2.weight: 0.38815605640411377\n",
      "Gradient for fc2.bias: 0.1321435421705246\n",
      "torch.Size([23]) tensor(-0.0130, grad_fn=<MaxBackward1>) tensor(-2.5935, grad_fn=<MinBackward1>)\n",
      "Episode 94, Loss: 0.004710202571004629\n",
      "Gradient for fc1.weight: 0.034465957432985306\n",
      "Gradient for fc1.bias: 0.07180524617433548\n",
      "Gradient for fc2.weight: 0.06087680160999298\n",
      "Gradient for fc2.bias: 0.02697833441197872\n",
      "torch.Size([19]) tensor(-0.0359, grad_fn=<MaxBackward1>) tensor(-1.0811, grad_fn=<MinBackward1>)\n",
      "Episode 95, Loss: 0.02693922258913517\n",
      "Gradient for fc1.weight: 0.12380228191614151\n",
      "Gradient for fc1.bias: 0.2572551369667053\n",
      "Gradient for fc2.weight: 0.3997265100479126\n",
      "Gradient for fc2.bias: 0.15860751271247864\n",
      "torch.Size([25]) tensor(-0.0240, grad_fn=<MaxBackward1>) tensor(-1.8663, grad_fn=<MinBackward1>)\n",
      "Episode 96, Loss: -0.0858694389462471\n",
      "Gradient for fc1.weight: 0.05134625360369682\n",
      "Gradient for fc1.bias: 0.15530140697956085\n",
      "Gradient for fc2.weight: 0.20403869450092316\n",
      "Gradient for fc2.bias: 0.0870167538523674\n",
      "torch.Size([31]) tensor(-0.0209, grad_fn=<MaxBackward1>) tensor(-1.3932, grad_fn=<MinBackward1>)\n",
      "Episode 97, Loss: -0.044729266315698624\n",
      "Gradient for fc1.weight: 0.07426692545413971\n",
      "Gradient for fc1.bias: 0.15139466524124146\n",
      "Gradient for fc2.weight: 0.2617320120334625\n",
      "Gradient for fc2.bias: 0.1022602841258049\n",
      "torch.Size([21]) tensor(-0.0145, grad_fn=<MaxBackward1>) tensor(-1.6373, grad_fn=<MinBackward1>)\n",
      "Episode 98, Loss: 0.08140160143375397\n",
      "Gradient for fc1.weight: 0.06443272531032562\n",
      "Gradient for fc1.bias: 0.21508046984672546\n",
      "Gradient for fc2.weight: 0.24916662275791168\n",
      "Gradient for fc2.bias: 0.11297129094600677\n",
      "torch.Size([30]) tensor(-0.0144, grad_fn=<MaxBackward1>) tensor(-1.5650, grad_fn=<MinBackward1>)\n",
      "Episode 99, Loss: -0.03428645431995392\n",
      "Gradient for fc1.weight: 0.02457105554640293\n",
      "Gradient for fc1.bias: 0.0330403670668602\n",
      "Gradient for fc2.weight: 0.017552819103002548\n",
      "Gradient for fc2.bias: 2.521789247111883e-05\n",
      "torch.Size([24]) tensor(-0.0023, grad_fn=<MaxBackward1>) tensor(-3.8158, grad_fn=<MinBackward1>)\n",
      "Episode 100, Loss: 0.08579903841018677\n",
      "Gradient for fc1.weight: 0.14514562487602234\n",
      "Gradient for fc1.bias: 0.1656215637922287\n",
      "Gradient for fc2.weight: 0.40684711933135986\n",
      "Gradient for fc2.bias: 0.11904580891132355\n",
      "Running reward: 33.283746319066154\n",
      "torch.Size([25]) tensor(-0.0020, grad_fn=<MaxBackward1>) tensor(-3.9847, grad_fn=<MinBackward1>)\n",
      "Episode 101, Loss: 0.15935122966766357\n",
      "Gradient for fc1.weight: 0.09829308092594147\n",
      "Gradient for fc1.bias: 0.29945385456085205\n",
      "Gradient for fc2.weight: 0.5287635922431946\n",
      "Gradient for fc2.bias: 0.22033141553401947\n",
      "torch.Size([32]) tensor(-0.0018, grad_fn=<MaxBackward1>) tensor(-4.0693, grad_fn=<MinBackward1>)\n",
      "Episode 102, Loss: 0.1833595633506775\n",
      "Gradient for fc1.weight: 0.09747181087732315\n",
      "Gradient for fc1.bias: 0.08371951431035995\n",
      "Gradient for fc2.weight: 0.23437120020389557\n",
      "Gradient for fc2.bias: 0.025572899729013443\n",
      "torch.Size([26]) tensor(-0.0161, grad_fn=<MaxBackward1>) tensor(-1.6817, grad_fn=<MinBackward1>)\n",
      "Episode 103, Loss: 0.07967352122068405\n",
      "Gradient for fc1.weight: 0.07147945463657379\n",
      "Gradient for fc1.bias: 0.20338572561740875\n",
      "Gradient for fc2.weight: 0.2588372230529785\n",
      "Gradient for fc2.bias: 0.1163446456193924\n",
      "torch.Size([21]) tensor(-0.0234, grad_fn=<MaxBackward1>) tensor(-1.6483, grad_fn=<MinBackward1>)\n",
      "Episode 104, Loss: -0.13207659125328064\n",
      "Gradient for fc1.weight: 0.07429453730583191\n",
      "Gradient for fc1.bias: 0.19578920304775238\n",
      "Gradient for fc2.weight: 0.2322510927915573\n",
      "Gradient for fc2.bias: 0.10072711855173111\n",
      "torch.Size([26]) tensor(-0.0208, grad_fn=<MaxBackward1>) tensor(-1.5207, grad_fn=<MinBackward1>)\n",
      "Episode 105, Loss: 0.05336705595254898\n",
      "Gradient for fc1.weight: 0.04914962127804756\n",
      "Gradient for fc1.bias: 0.09905283153057098\n",
      "Gradient for fc2.weight: 0.07860147207975388\n",
      "Gradient for fc2.bias: 0.00999348796904087\n",
      "torch.Size([36]) tensor(-0.0041, grad_fn=<MaxBackward1>) tensor(-3.5854, grad_fn=<MinBackward1>)\n",
      "Episode 106, Loss: -0.12198559194803238\n",
      "Gradient for fc1.weight: 0.057050660252571106\n",
      "Gradient for fc1.bias: 0.056278739124536514\n",
      "Gradient for fc2.weight: 0.11345602571964264\n",
      "Gradient for fc2.bias: 0.008927909657359123\n",
      "torch.Size([19]) tensor(-0.0221, grad_fn=<MaxBackward1>) tensor(-1.5532, grad_fn=<MinBackward1>)\n",
      "Episode 107, Loss: -0.10375728458166122\n",
      "Gradient for fc1.weight: 0.05837007984519005\n",
      "Gradient for fc1.bias: 0.07524394989013672\n",
      "Gradient for fc2.weight: 0.18670335412025452\n",
      "Gradient for fc2.bias: 0.04900725558400154\n",
      "torch.Size([21]) tensor(-0.0270, grad_fn=<MaxBackward1>) tensor(-1.8860, grad_fn=<MinBackward1>)\n",
      "Episode 108, Loss: -0.18128786981105804\n",
      "Gradient for fc1.weight: 0.07730858027935028\n",
      "Gradient for fc1.bias: 0.1368468701839447\n",
      "Gradient for fc2.weight: 0.15107662975788116\n",
      "Gradient for fc2.bias: 0.01054904330521822\n",
      "torch.Size([17]) tensor(-0.0154, grad_fn=<MaxBackward1>) tensor(-1.3769, grad_fn=<MinBackward1>)\n",
      "Episode 109, Loss: -0.04234800487756729\n",
      "Gradient for fc1.weight: 0.05485668033361435\n",
      "Gradient for fc1.bias: 0.07168606668710709\n",
      "Gradient for fc2.weight: 0.08106362074613571\n",
      "Gradient for fc2.bias: 0.029486393555998802\n",
      "torch.Size([17]) tensor(-0.0226, grad_fn=<MaxBackward1>) tensor(-1.5209, grad_fn=<MinBackward1>)\n",
      "Episode 110, Loss: -0.0539570152759552\n",
      "Gradient for fc1.weight: 0.03480556607246399\n",
      "Gradient for fc1.bias: 0.05065865442156792\n",
      "Gradient for fc2.weight: 0.030432824045419693\n",
      "Gradient for fc2.bias: 0.006515412125736475\n",
      "torch.Size([16]) tensor(-0.0259, grad_fn=<MaxBackward1>) tensor(-1.7730, grad_fn=<MinBackward1>)\n",
      "Episode 111, Loss: -0.049174945801496506\n",
      "Gradient for fc1.weight: 0.053425002843141556\n",
      "Gradient for fc1.bias: 0.09870432317256927\n",
      "Gradient for fc2.weight: 0.15016472339630127\n",
      "Gradient for fc2.bias: 0.0558135062456131\n",
      "torch.Size([26]) tensor(-0.0078, grad_fn=<MaxBackward1>) tensor(-2.7360, grad_fn=<MinBackward1>)\n",
      "Episode 112, Loss: -0.059163257479667664\n",
      "Gradient for fc1.weight: 0.08303412795066833\n",
      "Gradient for fc1.bias: 0.11855676770210266\n",
      "Gradient for fc2.weight: 0.20649626851081848\n",
      "Gradient for fc2.bias: 0.06928492337465286\n",
      "torch.Size([30]) tensor(-0.0169, grad_fn=<MaxBackward1>) tensor(-2.1345, grad_fn=<MinBackward1>)\n",
      "Episode 113, Loss: 0.0028952162247151136\n",
      "Gradient for fc1.weight: 0.028077084571123123\n",
      "Gradient for fc1.bias: 0.04166817665100098\n",
      "Gradient for fc2.weight: 0.04146454110741615\n",
      "Gradient for fc2.bias: 0.007279898039996624\n",
      "torch.Size([15]) tensor(-0.0151, grad_fn=<MaxBackward1>) tensor(-0.6866, grad_fn=<MinBackward1>)\n",
      "Episode 114, Loss: -0.06490296870470047\n",
      "Gradient for fc1.weight: 0.0772632360458374\n",
      "Gradient for fc1.bias: 0.10745003074407578\n",
      "Gradient for fc2.weight: 0.1756087988615036\n",
      "Gradient for fc2.bias: 0.06331867724657059\n",
      "torch.Size([22]) tensor(-0.0196, grad_fn=<MaxBackward1>) tensor(-1.6102, grad_fn=<MinBackward1>)\n",
      "Episode 115, Loss: -0.03534558787941933\n",
      "Gradient for fc1.weight: 0.14586305618286133\n",
      "Gradient for fc1.bias: 0.23269136250019073\n",
      "Gradient for fc2.weight: 0.46264493465423584\n",
      "Gradient for fc2.bias: 0.16774308681488037\n",
      "torch.Size([41]) tensor(-0.0056, grad_fn=<MaxBackward1>) tensor(-3.0364, grad_fn=<MinBackward1>)\n",
      "Episode 116, Loss: 0.04058699682354927\n",
      "Gradient for fc1.weight: 0.059061165899038315\n",
      "Gradient for fc1.bias: 0.15782853960990906\n",
      "Gradient for fc2.weight: 0.2058718204498291\n",
      "Gradient for fc2.bias: 0.09464286267757416\n",
      "torch.Size([16]) tensor(-0.0210, grad_fn=<MaxBackward1>) tensor(-1.7095, grad_fn=<MinBackward1>)\n",
      "Episode 117, Loss: 0.030474305152893066\n",
      "Gradient for fc1.weight: 0.160273939371109\n",
      "Gradient for fc1.bias: 0.2356397956609726\n",
      "Gradient for fc2.weight: 0.5198812484741211\n",
      "Gradient for fc2.bias: 0.1789650171995163\n",
      "torch.Size([22]) tensor(-0.0283, grad_fn=<MaxBackward1>) tensor(-1.6770, grad_fn=<MinBackward1>)\n",
      "Episode 118, Loss: 0.040955934673547745\n",
      "Gradient for fc1.weight: 0.07464274764060974\n",
      "Gradient for fc1.bias: 0.09454300254583359\n",
      "Gradient for fc2.weight: 0.23640763759613037\n",
      "Gradient for fc2.bias: 0.07629918307065964\n",
      "torch.Size([20]) tensor(-0.0290, grad_fn=<MaxBackward1>) tensor(-1.4613, grad_fn=<MinBackward1>)\n",
      "Episode 119, Loss: -0.12748581171035767\n",
      "Gradient for fc1.weight: 0.1716107428073883\n",
      "Gradient for fc1.bias: 0.20355457067489624\n",
      "Gradient for fc2.weight: 0.5132426023483276\n",
      "Gradient for fc2.bias: 0.15999159216880798\n",
      "torch.Size([29]) tensor(-0.0148, grad_fn=<MaxBackward1>) tensor(-2.1148, grad_fn=<MinBackward1>)\n",
      "Episode 120, Loss: -0.05845225229859352\n",
      "Gradient for fc1.weight: 0.07307182997465134\n",
      "Gradient for fc1.bias: 0.1134197786450386\n",
      "Gradient for fc2.weight: 0.2130785584449768\n",
      "Gradient for fc2.bias: 0.07813374698162079\n",
      "torch.Size([20]) tensor(-0.0090, grad_fn=<MaxBackward1>) tensor(-2.5261, grad_fn=<MinBackward1>)\n",
      "Episode 121, Loss: -0.05073036998510361\n",
      "Gradient for fc1.weight: 0.05450529232621193\n",
      "Gradient for fc1.bias: 0.14522795379161835\n",
      "Gradient for fc2.weight: 0.22225458920001984\n",
      "Gradient for fc2.bias: 0.09550150483846664\n",
      "torch.Size([19]) tensor(-0.0167, grad_fn=<MaxBackward1>) tensor(-1.9689, grad_fn=<MinBackward1>)\n",
      "Episode 122, Loss: -0.0739910751581192\n",
      "Gradient for fc1.weight: 0.20281238853931427\n",
      "Gradient for fc1.bias: 0.36500030755996704\n",
      "Gradient for fc2.weight: 0.7101081013679504\n",
      "Gradient for fc2.bias: 0.2710212171077728\n",
      "torch.Size([26]) tensor(-0.0196, grad_fn=<MaxBackward1>) tensor(-2.5578, grad_fn=<MinBackward1>)\n",
      "Episode 123, Loss: 0.07294205576181412\n",
      "Gradient for fc1.weight: 0.10024227201938629\n",
      "Gradient for fc1.bias: 0.1458515226840973\n",
      "Gradient for fc2.weight: 0.3993007242679596\n",
      "Gradient for fc2.bias: 0.1352233737707138\n",
      "torch.Size([23]) tensor(-0.0298, grad_fn=<MaxBackward1>) tensor(-1.8653, grad_fn=<MinBackward1>)\n",
      "Episode 124, Loss: -0.004334678873419762\n",
      "Gradient for fc1.weight: 0.060294754803180695\n",
      "Gradient for fc1.bias: 0.10656192898750305\n",
      "Gradient for fc2.weight: 0.188471719622612\n",
      "Gradient for fc2.bias: 0.07166514545679092\n",
      "torch.Size([16]) tensor(-0.0250, grad_fn=<MaxBackward1>) tensor(-1.6789, grad_fn=<MinBackward1>)\n",
      "Episode 125, Loss: -0.07591453939676285\n",
      "Gradient for fc1.weight: 0.04710619896650314\n",
      "Gradient for fc1.bias: 0.1040177270770073\n",
      "Gradient for fc2.weight: 0.13605958223342896\n",
      "Gradient for fc2.bias: 0.05415625125169754\n",
      "torch.Size([15]) tensor(-0.0163, grad_fn=<MaxBackward1>) tensor(-2.0904, grad_fn=<MinBackward1>)\n",
      "Episode 126, Loss: -0.06540749222040176\n",
      "Gradient for fc1.weight: 0.10058585554361343\n",
      "Gradient for fc1.bias: 0.09995318204164505\n",
      "Gradient for fc2.weight: 0.1394682079553604\n",
      "Gradient for fc2.bias: 0.023064827546477318\n",
      "torch.Size([16]) tensor(-0.0194, grad_fn=<MaxBackward1>) tensor(-1.4848, grad_fn=<MinBackward1>)\n",
      "Episode 127, Loss: -0.06544184684753418\n",
      "Gradient for fc1.weight: 0.0627409815788269\n",
      "Gradient for fc1.bias: 0.07837401330471039\n",
      "Gradient for fc2.weight: 0.11442238837480545\n",
      "Gradient for fc2.bias: 0.033967096358537674\n",
      "torch.Size([24]) tensor(-0.0019, grad_fn=<MaxBackward1>) tensor(-4.1189, grad_fn=<MinBackward1>)\n",
      "Episode 128, Loss: 0.1764906793832779\n",
      "Gradient for fc1.weight: 0.057983290404081345\n",
      "Gradient for fc1.bias: 0.18192137777805328\n",
      "Gradient for fc2.weight: 0.3310879170894623\n",
      "Gradient for fc2.bias: 0.135765939950943\n",
      "torch.Size([24]) tensor(-0.0196, grad_fn=<MaxBackward1>) tensor(-1.6869, grad_fn=<MinBackward1>)\n",
      "Episode 129, Loss: -0.013304755091667175\n",
      "Gradient for fc1.weight: 0.05168861523270607\n",
      "Gradient for fc1.bias: 0.046698667109012604\n",
      "Gradient for fc2.weight: 0.1360068917274475\n",
      "Gradient for fc2.bias: 0.027387704700231552\n",
      "torch.Size([21]) tensor(-0.0024, grad_fn=<MaxBackward1>) tensor(-3.9624, grad_fn=<MinBackward1>)\n",
      "Episode 130, Loss: 0.20380692183971405\n",
      "Gradient for fc1.weight: 0.085422083735466\n",
      "Gradient for fc1.bias: 0.2783031761646271\n",
      "Gradient for fc2.weight: 0.3306684195995331\n",
      "Gradient for fc2.bias: 0.1537306010723114\n",
      "torch.Size([27]) tensor(-0.0099, grad_fn=<MaxBackward1>) tensor(-2.6549, grad_fn=<MinBackward1>)\n",
      "Episode 131, Loss: -0.28602251410484314\n",
      "Gradient for fc1.weight: 0.15654027462005615\n",
      "Gradient for fc1.bias: 0.2892095744609833\n",
      "Gradient for fc2.weight: 0.48209843039512634\n",
      "Gradient for fc2.bias: 0.17998619377613068\n",
      "torch.Size([12]) tensor(-0.0200, grad_fn=<MaxBackward1>) tensor(-1.6323, grad_fn=<MinBackward1>)\n",
      "Episode 132, Loss: -0.11663416773080826\n",
      "Gradient for fc1.weight: 0.07597962766885757\n",
      "Gradient for fc1.bias: 0.06368835270404816\n",
      "Gradient for fc2.weight: 0.2319795936346054\n",
      "Gradient for fc2.bias: 0.04025016352534294\n",
      "torch.Size([16]) tensor(-0.0202, grad_fn=<MaxBackward1>) tensor(-1.1182, grad_fn=<MinBackward1>)\n",
      "Episode 133, Loss: -0.10628269612789154\n",
      "Gradient for fc1.weight: 0.12321364879608154\n",
      "Gradient for fc1.bias: 0.12845899164676666\n",
      "Gradient for fc2.weight: 0.30103495717048645\n",
      "Gradient for fc2.bias: 0.09056682139635086\n",
      "torch.Size([25]) tensor(-0.0185, grad_fn=<MaxBackward1>) tensor(-1.5776, grad_fn=<MinBackward1>)\n",
      "Episode 134, Loss: -0.1427735835313797\n",
      "Gradient for fc1.weight: 0.15528292953968048\n",
      "Gradient for fc1.bias: 0.17259736359119415\n",
      "Gradient for fc2.weight: 0.5272396802902222\n",
      "Gradient for fc2.bias: 0.1555308848619461\n",
      "torch.Size([17]) tensor(-0.0105, grad_fn=<MaxBackward1>) tensor(-2.3088, grad_fn=<MinBackward1>)\n",
      "Episode 135, Loss: -0.06463399529457092\n",
      "Gradient for fc1.weight: 0.19004864990711212\n",
      "Gradient for fc1.bias: 0.24283424019813538\n",
      "Gradient for fc2.weight: 0.7785347700119019\n",
      "Gradient for fc2.bias: 0.24735170602798462\n",
      "torch.Size([15]) tensor(-0.0027, grad_fn=<MaxBackward1>) tensor(-3.8121, grad_fn=<MinBackward1>)\n",
      "Episode 136, Loss: 0.32895973324775696\n",
      "Gradient for fc1.weight: 0.0783548578619957\n",
      "Gradient for fc1.bias: 0.1816369742155075\n",
      "Gradient for fc2.weight: 0.300586074590683\n",
      "Gradient for fc2.bias: 0.06462866812944412\n",
      "torch.Size([25]) tensor(-0.0280, grad_fn=<MaxBackward1>) tensor(-2.3333, grad_fn=<MinBackward1>)\n",
      "Episode 137, Loss: -0.15113148093223572\n",
      "Gradient for fc1.weight: 0.1736714094877243\n",
      "Gradient for fc1.bias: 0.22392107546329498\n",
      "Gradient for fc2.weight: 0.6366302371025085\n",
      "Gradient for fc2.bias: 0.20174507796764374\n",
      "torch.Size([22]) tensor(-0.0228, grad_fn=<MaxBackward1>) tensor(-1.4231, grad_fn=<MinBackward1>)\n",
      "Episode 138, Loss: -0.09087053686380386\n",
      "Gradient for fc1.weight: 0.10966806858778\n",
      "Gradient for fc1.bias: 0.21570844948291779\n",
      "Gradient for fc2.weight: 0.3388703167438507\n",
      "Gradient for fc2.bias: 0.13624361157417297\n",
      "torch.Size([25]) tensor(-0.0326, grad_fn=<MaxBackward1>) tensor(-1.3691, grad_fn=<MinBackward1>)\n",
      "Episode 139, Loss: -0.02952435426414013\n",
      "Gradient for fc1.weight: 0.016823582351207733\n",
      "Gradient for fc1.bias: 0.021667102351784706\n",
      "Gradient for fc2.weight: 0.01517345942556858\n",
      "Gradient for fc2.bias: 0.003103009657934308\n",
      "torch.Size([15]) tensor(-0.0285, grad_fn=<MaxBackward1>) tensor(-2.1193, grad_fn=<MinBackward1>)\n",
      "Episode 140, Loss: 0.01883797161281109\n",
      "Gradient for fc1.weight: 0.04262951761484146\n",
      "Gradient for fc1.bias: 0.0610385425388813\n",
      "Gradient for fc2.weight: 0.11601440608501434\n",
      "Gradient for fc2.bias: 0.042563796043395996\n",
      "torch.Size([26]) tensor(-0.0102, grad_fn=<MaxBackward1>) tensor(-2.3848, grad_fn=<MinBackward1>)\n",
      "Episode 141, Loss: -0.10002239793539047\n",
      "Gradient for fc1.weight: 0.06567687541246414\n",
      "Gradient for fc1.bias: 0.0867990180850029\n",
      "Gradient for fc2.weight: 0.17178545892238617\n",
      "Gradient for fc2.bias: 0.05648205056786537\n",
      "torch.Size([32]) tensor(-0.0175, grad_fn=<MaxBackward1>) tensor(-2.3801, grad_fn=<MinBackward1>)\n",
      "Episode 142, Loss: -0.04014105349779129\n",
      "Gradient for fc1.weight: 0.02445250377058983\n",
      "Gradient for fc1.bias: 0.038197338581085205\n",
      "Gradient for fc2.weight: 0.05884988605976105\n",
      "Gradient for fc2.bias: 0.01930816099047661\n",
      "torch.Size([26]) tensor(-0.0217, grad_fn=<MaxBackward1>) tensor(-1.8146, grad_fn=<MinBackward1>)\n",
      "Episode 143, Loss: -0.0172099769115448\n",
      "Gradient for fc1.weight: 0.11648803949356079\n",
      "Gradient for fc1.bias: 0.24979600310325623\n",
      "Gradient for fc2.weight: 0.37007465958595276\n",
      "Gradient for fc2.bias: 0.15657471120357513\n",
      "torch.Size([20]) tensor(-0.0113, grad_fn=<MaxBackward1>) tensor(-2.7443, grad_fn=<MinBackward1>)\n",
      "Episode 144, Loss: 0.052321381866931915\n",
      "Gradient for fc1.weight: 0.058988843113183975\n",
      "Gradient for fc1.bias: 0.11028844118118286\n",
      "Gradient for fc2.weight: 0.14573077857494354\n",
      "Gradient for fc2.bias: 0.05691098794341087\n",
      "torch.Size([27]) tensor(-0.0080, grad_fn=<MaxBackward1>) tensor(-2.3996, grad_fn=<MinBackward1>)\n",
      "Episode 145, Loss: 0.06372726708650589\n",
      "Gradient for fc1.weight: 0.028253283351659775\n",
      "Gradient for fc1.bias: 0.07136241346597672\n",
      "Gradient for fc2.weight: 0.11309105902910233\n",
      "Gradient for fc2.bias: 0.046007189899683\n",
      "torch.Size([31]) tensor(-0.0178, grad_fn=<MaxBackward1>) tensor(-1.6563, grad_fn=<MinBackward1>)\n",
      "Episode 146, Loss: -0.04065519943833351\n",
      "Gradient for fc1.weight: 0.031295809894800186\n",
      "Gradient for fc1.bias: 0.06438391655683517\n",
      "Gradient for fc2.weight: 0.07805762439966202\n",
      "Gradient for fc2.bias: 0.03311651572585106\n",
      "torch.Size([49]) tensor(-0.0096, grad_fn=<MaxBackward1>) tensor(-2.2032, grad_fn=<MinBackward1>)\n",
      "Episode 147, Loss: -0.01190099772065878\n",
      "Gradient for fc1.weight: 0.04065782576799393\n",
      "Gradient for fc1.bias: 0.12964649498462677\n",
      "Gradient for fc2.weight: 0.15276938676834106\n",
      "Gradient for fc2.bias: 0.07329876720905304\n",
      "torch.Size([38]) tensor(-0.0071, grad_fn=<MaxBackward1>) tensor(-2.3521, grad_fn=<MinBackward1>)\n",
      "Episode 148, Loss: -0.11661828309297562\n",
      "Gradient for fc1.weight: 0.05511028692126274\n",
      "Gradient for fc1.bias: 0.05352785810828209\n",
      "Gradient for fc2.weight: 0.11875291913747787\n",
      "Gradient for fc2.bias: 0.02436421439051628\n",
      "torch.Size([26]) tensor(-0.0095, grad_fn=<MaxBackward1>) tensor(-2.1747, grad_fn=<MinBackward1>)\n",
      "Episode 149, Loss: 0.03058740310370922\n",
      "Gradient for fc1.weight: 0.05860498175024986\n",
      "Gradient for fc1.bias: 0.1401551067829132\n",
      "Gradient for fc2.weight: 0.2053671032190323\n",
      "Gradient for fc2.bias: 0.08701122552156448\n",
      "torch.Size([59]) tensor(-0.0072, grad_fn=<MaxBackward1>) tensor(-3.0741, grad_fn=<MinBackward1>)\n",
      "Episode 150, Loss: -0.10134351253509521\n",
      "Gradient for fc1.weight: 0.06449736654758453\n",
      "Gradient for fc1.bias: 0.1111370250582695\n",
      "Gradient for fc2.weight: 0.18224477767944336\n",
      "Gradient for fc2.bias: 0.06908143311738968\n",
      "torch.Size([30]) tensor(-0.0049, grad_fn=<MaxBackward1>) tensor(-2.6086, grad_fn=<MinBackward1>)\n",
      "Episode 151, Loss: 0.007492383476346731\n",
      "Gradient for fc1.weight: 0.023266643285751343\n",
      "Gradient for fc1.bias: 0.08697812259197235\n",
      "Gradient for fc2.weight: 0.1122041642665863\n",
      "Gradient for fc2.bias: 0.053622812032699585\n",
      "torch.Size([32]) tensor(-0.0196, grad_fn=<MaxBackward1>) tensor(-1.3090, grad_fn=<MinBackward1>)\n",
      "Episode 152, Loss: 0.04615616053342819\n",
      "Gradient for fc1.weight: 0.0435279905796051\n",
      "Gradient for fc1.bias: 0.07400702685117722\n",
      "Gradient for fc2.weight: 0.10054585337638855\n",
      "Gradient for fc2.bias: 0.03758401423692703\n",
      "torch.Size([53]) tensor(-0.0158, grad_fn=<MaxBackward1>) tensor(-1.4879, grad_fn=<MinBackward1>)\n",
      "Episode 153, Loss: 0.0064531294628977776\n",
      "Gradient for fc1.weight: 0.025148915126919746\n",
      "Gradient for fc1.bias: 0.02160387486219406\n",
      "Gradient for fc2.weight: 0.03682691603899002\n",
      "Gradient for fc2.bias: 0.006565894465893507\n",
      "torch.Size([50]) tensor(-0.0057, grad_fn=<MaxBackward1>) tensor(-2.9296, grad_fn=<MinBackward1>)\n",
      "Episode 154, Loss: -0.09857279062271118\n",
      "Gradient for fc1.weight: 0.030798176303505898\n",
      "Gradient for fc1.bias: 0.036903541535139084\n",
      "Gradient for fc2.weight: 0.07856468111276627\n",
      "Gradient for fc2.bias: 0.0005554184317588806\n",
      "torch.Size([39]) tensor(-0.0046, grad_fn=<MaxBackward1>) tensor(-2.2835, grad_fn=<MinBackward1>)\n",
      "Episode 155, Loss: 0.05058686062693596\n",
      "Gradient for fc1.weight: 0.05957004055380821\n",
      "Gradient for fc1.bias: 0.16670775413513184\n",
      "Gradient for fc2.weight: 0.23967061936855316\n",
      "Gradient for fc2.bias: 0.10144785791635513\n",
      "torch.Size([82]) tensor(-0.0031, grad_fn=<MaxBackward1>) tensor(-2.7720, grad_fn=<MinBackward1>)\n",
      "Episode 156, Loss: 0.08692049980163574\n",
      "Gradient for fc1.weight: 0.020453250035643578\n",
      "Gradient for fc1.bias: 0.05057736486196518\n",
      "Gradient for fc2.weight: 0.08490859717130661\n",
      "Gradient for fc2.bias: 0.02748221345245838\n",
      "torch.Size([107]) tensor(-0.0035, grad_fn=<MaxBackward1>) tensor(-2.5743, grad_fn=<MinBackward1>)\n",
      "Episode 157, Loss: 0.020862100645899773\n",
      "Gradient for fc1.weight: 0.011239392682909966\n",
      "Gradient for fc1.bias: 0.016440551728010178\n",
      "Gradient for fc2.weight: 0.023660531267523766\n",
      "Gradient for fc2.bias: 0.0008046103175729513\n",
      "torch.Size([51]) tensor(-0.0095, grad_fn=<MaxBackward1>) tensor(-1.8607, grad_fn=<MinBackward1>)\n",
      "Episode 158, Loss: -0.04664404317736626\n",
      "Gradient for fc1.weight: 0.020271051675081253\n",
      "Gradient for fc1.bias: 0.07240838557481766\n",
      "Gradient for fc2.weight: 0.09355047345161438\n",
      "Gradient for fc2.bias: 0.04498765245079994\n",
      "torch.Size([62]) tensor(-0.0162, grad_fn=<MaxBackward1>) tensor(-1.7369, grad_fn=<MinBackward1>)\n",
      "Episode 159, Loss: -0.012098940089344978\n",
      "Gradient for fc1.weight: 0.03191620483994484\n",
      "Gradient for fc1.bias: 0.04819992557168007\n",
      "Gradient for fc2.weight: 0.06986861675977707\n",
      "Gradient for fc2.bias: 0.03025316819548607\n",
      "torch.Size([48]) tensor(-0.0020, grad_fn=<MaxBackward1>) tensor(-4.2343, grad_fn=<MinBackward1>)\n",
      "Episode 160, Loss: -0.10309014469385147\n",
      "Gradient for fc1.weight: 0.06437381356954575\n",
      "Gradient for fc1.bias: 0.14054705202579498\n",
      "Gradient for fc2.weight: 0.2541257441043854\n",
      "Gradient for fc2.bias: 0.1049797311425209\n",
      "torch.Size([86]) tensor(-0.0022, grad_fn=<MaxBackward1>) tensor(-3.6342, grad_fn=<MinBackward1>)\n",
      "Episode 161, Loss: 0.02074763923883438\n",
      "Gradient for fc1.weight: 0.008236599154770374\n",
      "Gradient for fc1.bias: 0.018557365983724594\n",
      "Gradient for fc2.weight: 0.01412572804838419\n",
      "Gradient for fc2.bias: 0.003089117119088769\n",
      "torch.Size([54]) tensor(-0.0022, grad_fn=<MaxBackward1>) tensor(-3.0476, grad_fn=<MinBackward1>)\n",
      "Episode 162, Loss: 0.043430015444755554\n",
      "Gradient for fc1.weight: 0.027748331427574158\n",
      "Gradient for fc1.bias: 0.038752827793359756\n",
      "Gradient for fc2.weight: 0.04768272489309311\n",
      "Gradient for fc2.bias: 0.008313220925629139\n",
      "torch.Size([39]) tensor(-0.0027, grad_fn=<MaxBackward1>) tensor(-3.8919, grad_fn=<MinBackward1>)\n",
      "Episode 163, Loss: 0.09936688095331192\n",
      "Gradient for fc1.weight: 0.022793862968683243\n",
      "Gradient for fc1.bias: 0.11311990767717361\n",
      "Gradient for fc2.weight: 0.18914450705051422\n",
      "Gradient for fc2.bias: 0.07981313019990921\n",
      "torch.Size([39]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-4.0745, grad_fn=<MinBackward1>)\n",
      "Episode 164, Loss: 0.14875158667564392\n",
      "Gradient for fc1.weight: 0.042776573449373245\n",
      "Gradient for fc1.bias: 0.07244250178337097\n",
      "Gradient for fc2.weight: 0.1113503947854042\n",
      "Gradient for fc2.bias: 0.0316908061504364\n",
      "torch.Size([84]) tensor(-0.0025, grad_fn=<MaxBackward1>) tensor(-2.4370, grad_fn=<MinBackward1>)\n",
      "Episode 165, Loss: -0.009379616938531399\n",
      "Gradient for fc1.weight: 0.039494846016168594\n",
      "Gradient for fc1.bias: 0.06293310970067978\n",
      "Gradient for fc2.weight: 0.06763719767332077\n",
      "Gradient for fc2.bias: 0.027651600539684296\n",
      "torch.Size([61]) tensor(-0.0182, grad_fn=<MaxBackward1>) tensor(-1.2069, grad_fn=<MinBackward1>)\n",
      "Episode 166, Loss: -0.0013859994942322373\n",
      "Gradient for fc1.weight: 0.043229248374700546\n",
      "Gradient for fc1.bias: 0.1683177500963211\n",
      "Gradient for fc2.weight: 0.2126380205154419\n",
      "Gradient for fc2.bias: 0.0978761613368988\n",
      "torch.Size([64]) tensor(-0.0068, grad_fn=<MaxBackward1>) tensor(-1.2934, grad_fn=<MinBackward1>)\n",
      "Episode 167, Loss: 0.005710883066058159\n",
      "Gradient for fc1.weight: 0.006573343183845282\n",
      "Gradient for fc1.bias: 0.015950731933116913\n",
      "Gradient for fc2.weight: 0.010341177694499493\n",
      "Gradient for fc2.bias: 0.0037442869506776333\n",
      "torch.Size([50]) tensor(-0.0063, grad_fn=<MaxBackward1>) tensor(-1.6086, grad_fn=<MinBackward1>)\n",
      "Episode 168, Loss: -0.023662490770220757\n",
      "Gradient for fc1.weight: 0.054135534912347794\n",
      "Gradient for fc1.bias: 0.1407748907804489\n",
      "Gradient for fc2.weight: 0.19936048984527588\n",
      "Gradient for fc2.bias: 0.08374279737472534\n",
      "torch.Size([53]) tensor(-0.0062, grad_fn=<MaxBackward1>) tensor(-2.9568, grad_fn=<MinBackward1>)\n",
      "Episode 169, Loss: 0.05232938379049301\n",
      "Gradient for fc1.weight: 0.01769167184829712\n",
      "Gradient for fc1.bias: 0.043940469622612\n",
      "Gradient for fc2.weight: 0.05879886448383331\n",
      "Gradient for fc2.bias: 0.020850326865911484\n",
      "torch.Size([45]) tensor(-0.0019, grad_fn=<MaxBackward1>) tensor(-4.0731, grad_fn=<MinBackward1>)\n",
      "Episode 170, Loss: 0.18606184422969818\n",
      "Gradient for fc1.weight: 0.041809964925050735\n",
      "Gradient for fc1.bias: 0.20627039670944214\n",
      "Gradient for fc2.weight: 0.31594160199165344\n",
      "Gradient for fc2.bias: 0.13250583410263062\n",
      "torch.Size([75]) tensor(-0.0059, grad_fn=<MaxBackward1>) tensor(-1.8691, grad_fn=<MinBackward1>)\n",
      "Episode 171, Loss: -0.005175105761736631\n",
      "Gradient for fc1.weight: 0.01953599601984024\n",
      "Gradient for fc1.bias: 0.0798054188489914\n",
      "Gradient for fc2.weight: 0.0708066076040268\n",
      "Gradient for fc2.bias: 0.03686487302184105\n",
      "torch.Size([53]) tensor(-0.0033, grad_fn=<MaxBackward1>) tensor(-3.7242, grad_fn=<MinBackward1>)\n",
      "Episode 172, Loss: 0.08079464733600616\n",
      "Gradient for fc1.weight: 0.059258121997117996\n",
      "Gradient for fc1.bias: 0.20385631918907166\n",
      "Gradient for fc2.weight: 0.32881802320480347\n",
      "Gradient for fc2.bias: 0.14405155181884766\n",
      "torch.Size([39]) tensor(-0.0069, grad_fn=<MaxBackward1>) tensor(-1.7926, grad_fn=<MinBackward1>)\n",
      "Episode 173, Loss: -0.03130996972322464\n",
      "Gradient for fc1.weight: 0.08958423137664795\n",
      "Gradient for fc1.bias: 0.2725346088409424\n",
      "Gradient for fc2.weight: 0.37548011541366577\n",
      "Gradient for fc2.bias: 0.1648911088705063\n",
      "torch.Size([44]) tensor(-0.0025, grad_fn=<MaxBackward1>) tensor(-2.7876, grad_fn=<MinBackward1>)\n",
      "Episode 174, Loss: 0.09440232068300247\n",
      "Gradient for fc1.weight: 0.031613003462553024\n",
      "Gradient for fc1.bias: 0.07817884534597397\n",
      "Gradient for fc2.weight: 0.09689946472644806\n",
      "Gradient for fc2.bias: 0.04116997495293617\n",
      "torch.Size([53]) tensor(-0.0045, grad_fn=<MaxBackward1>) tensor(-3.0978, grad_fn=<MinBackward1>)\n",
      "Episode 175, Loss: -0.07424183934926987\n",
      "Gradient for fc1.weight: 0.05787031352519989\n",
      "Gradient for fc1.bias: 0.09969498217105865\n",
      "Gradient for fc2.weight: 0.17089827358722687\n",
      "Gradient for fc2.bias: 0.0729096531867981\n",
      "torch.Size([92]) tensor(-0.0028, grad_fn=<MaxBackward1>) tensor(-2.7954, grad_fn=<MinBackward1>)\n",
      "Episode 176, Loss: -0.066529281437397\n",
      "Gradient for fc1.weight: 0.02691430039703846\n",
      "Gradient for fc1.bias: 0.037121985107660294\n",
      "Gradient for fc2.weight: 0.05736703798174858\n",
      "Gradient for fc2.bias: 0.014203285798430443\n",
      "torch.Size([57]) tensor(-0.0112, grad_fn=<MaxBackward1>) tensor(-2.3098, grad_fn=<MinBackward1>)\n",
      "Episode 177, Loss: 0.07494558393955231\n",
      "Gradient for fc1.weight: 0.030529290437698364\n",
      "Gradient for fc1.bias: 0.13834860920906067\n",
      "Gradient for fc2.weight: 0.18264593183994293\n",
      "Gradient for fc2.bias: 0.08716614544391632\n",
      "torch.Size([60]) tensor(-0.0028, grad_fn=<MaxBackward1>) tensor(-2.5808, grad_fn=<MinBackward1>)\n",
      "Episode 178, Loss: 0.009179518558084965\n",
      "Gradient for fc1.weight: 0.04453990235924721\n",
      "Gradient for fc1.bias: 0.058530356734991074\n",
      "Gradient for fc2.weight: 0.10892587900161743\n",
      "Gradient for fc2.bias: 0.038006339222192764\n",
      "torch.Size([49]) tensor(-0.0026, grad_fn=<MaxBackward1>) tensor(-2.6604, grad_fn=<MinBackward1>)\n",
      "Episode 179, Loss: -0.0009125702199526131\n",
      "Gradient for fc1.weight: 0.03348167985677719\n",
      "Gradient for fc1.bias: 0.16146375238895416\n",
      "Gradient for fc2.weight: 0.20173124969005585\n",
      "Gradient for fc2.bias: 0.09840378910303116\n",
      "torch.Size([55]) tensor(-0.0026, grad_fn=<MaxBackward1>) tensor(-3.1067, grad_fn=<MinBackward1>)\n",
      "Episode 180, Loss: 0.04314791411161423\n",
      "Gradient for fc1.weight: 0.018303388729691505\n",
      "Gradient for fc1.bias: 0.03243323415517807\n",
      "Gradient for fc2.weight: 0.04127700999379158\n",
      "Gradient for fc2.bias: 0.010276352986693382\n",
      "torch.Size([79]) tensor(-0.0037, grad_fn=<MaxBackward1>) tensor(-3.7263, grad_fn=<MinBackward1>)\n",
      "Episode 181, Loss: -0.01690380647778511\n",
      "Gradient for fc1.weight: 0.015901243314146996\n",
      "Gradient for fc1.bias: 0.03507903963327408\n",
      "Gradient for fc2.weight: 0.028901752084493637\n",
      "Gradient for fc2.bias: 0.013462222181260586\n",
      "torch.Size([71]) tensor(-0.0060, grad_fn=<MaxBackward1>) tensor(-2.0577, grad_fn=<MinBackward1>)\n",
      "Episode 182, Loss: -0.003436112077906728\n",
      "Gradient for fc1.weight: 0.008241011761128902\n",
      "Gradient for fc1.bias: 0.05201051011681557\n",
      "Gradient for fc2.weight: 0.05488960072398186\n",
      "Gradient for fc2.bias: 0.029789665713906288\n",
      "torch.Size([111]) tensor(-0.0067, grad_fn=<MaxBackward1>) tensor(-2.7707, grad_fn=<MinBackward1>)\n",
      "Episode 183, Loss: 0.024103710427880287\n",
      "Gradient for fc1.weight: 0.006255944725126028\n",
      "Gradient for fc1.bias: 0.0850553810596466\n",
      "Gradient for fc2.weight: 0.09692572802305222\n",
      "Gradient for fc2.bias: 0.05059657618403435\n",
      "torch.Size([28]) tensor(-0.0027, grad_fn=<MaxBackward1>) tensor(-3.0403, grad_fn=<MinBackward1>)\n",
      "Episode 184, Loss: 0.23045594990253448\n",
      "Gradient for fc1.weight: 0.07422950863838196\n",
      "Gradient for fc1.bias: 0.15369132161140442\n",
      "Gradient for fc2.weight: 0.2064618319272995\n",
      "Gradient for fc2.bias: 0.0761319026350975\n",
      "torch.Size([53]) tensor(-0.0060, grad_fn=<MaxBackward1>) tensor(-2.1649, grad_fn=<MinBackward1>)\n",
      "Episode 185, Loss: -0.04278188943862915\n",
      "Gradient for fc1.weight: 0.02217935025691986\n",
      "Gradient for fc1.bias: 0.050052981823682785\n",
      "Gradient for fc2.weight: 0.055126793682575226\n",
      "Gradient for fc2.bias: 0.025506269186735153\n",
      "torch.Size([41]) tensor(-0.0137, grad_fn=<MaxBackward1>) tensor(-1.1351, grad_fn=<MinBackward1>)\n",
      "Episode 186, Loss: -0.015589063055813313\n",
      "Gradient for fc1.weight: 0.018363654613494873\n",
      "Gradient for fc1.bias: 0.017875012010335922\n",
      "Gradient for fc2.weight: 0.02813495695590973\n",
      "Gradient for fc2.bias: 0.0020949828904122114\n",
      "torch.Size([56]) tensor(-0.0039, grad_fn=<MaxBackward1>) tensor(-2.7187, grad_fn=<MinBackward1>)\n",
      "Episode 187, Loss: -0.07243385165929794\n",
      "Gradient for fc1.weight: 0.014920161105692387\n",
      "Gradient for fc1.bias: 0.07270898669958115\n",
      "Gradient for fc2.weight: 0.11075232923030853\n",
      "Gradient for fc2.bias: 0.047432877123355865\n",
      "torch.Size([59]) tensor(-0.0064, grad_fn=<MaxBackward1>) tensor(-2.5252, grad_fn=<MinBackward1>)\n",
      "Episode 188, Loss: -0.06521647423505783\n",
      "Gradient for fc1.weight: 0.04651792719960213\n",
      "Gradient for fc1.bias: 0.08587367087602615\n",
      "Gradient for fc2.weight: 0.12687164545059204\n",
      "Gradient for fc2.bias: 0.059726372361183167\n",
      "torch.Size([76]) tensor(-0.0056, grad_fn=<MaxBackward1>) tensor(-2.7265, grad_fn=<MinBackward1>)\n",
      "Episode 189, Loss: -0.00837523490190506\n",
      "Gradient for fc1.weight: 0.012484565377235413\n",
      "Gradient for fc1.bias: 0.020349884405732155\n",
      "Gradient for fc2.weight: 0.011218427680432796\n",
      "Gradient for fc2.bias: 0.0010043311631307006\n",
      "torch.Size([28]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-4.6627, grad_fn=<MinBackward1>)\n",
      "Episode 190, Loss: -0.22981210052967072\n",
      "Gradient for fc1.weight: 0.10092370212078094\n",
      "Gradient for fc1.bias: 0.0835806280374527\n",
      "Gradient for fc2.weight: 0.1958000510931015\n",
      "Gradient for fc2.bias: 0.014129363931715488\n",
      "torch.Size([78]) tensor(-0.0070, grad_fn=<MaxBackward1>) tensor(-2.2282, grad_fn=<MinBackward1>)\n",
      "Episode 191, Loss: 0.005193538963794708\n",
      "Gradient for fc1.weight: 0.022242603823542595\n",
      "Gradient for fc1.bias: 0.01982763595879078\n",
      "Gradient for fc2.weight: 0.03870879113674164\n",
      "Gradient for fc2.bias: 0.007722672075033188\n",
      "torch.Size([49]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-4.7160, grad_fn=<MinBackward1>)\n",
      "Episode 192, Loss: -0.07459978759288788\n",
      "Gradient for fc1.weight: 0.05082640424370766\n",
      "Gradient for fc1.bias: 0.07107923179864883\n",
      "Gradient for fc2.weight: 0.12652587890625\n",
      "Gradient for fc2.bias: 0.04725499078631401\n",
      "torch.Size([57]) tensor(-0.0051, grad_fn=<MaxBackward1>) tensor(-2.0747, grad_fn=<MinBackward1>)\n",
      "Episode 193, Loss: -0.050102341920137405\n",
      "Gradient for fc1.weight: 0.019753331318497658\n",
      "Gradient for fc1.bias: 0.023015782237052917\n",
      "Gradient for fc2.weight: 0.03652162849903107\n",
      "Gradient for fc2.bias: 0.004471419379115105\n",
      "torch.Size([64]) tensor(-0.0066, grad_fn=<MaxBackward1>) tensor(-2.0477, grad_fn=<MinBackward1>)\n",
      "Episode 194, Loss: -0.05897601321339607\n",
      "Gradient for fc1.weight: 0.02032729983329773\n",
      "Gradient for fc1.bias: 0.026695694774389267\n",
      "Gradient for fc2.weight: 0.05130333453416824\n",
      "Gradient for fc2.bias: 0.005653160158544779\n",
      "torch.Size([81]) tensor(-0.0032, grad_fn=<MaxBackward1>) tensor(-3.1508, grad_fn=<MinBackward1>)\n",
      "Episode 195, Loss: -0.040507156401872635\n",
      "Gradient for fc1.weight: 0.04070109874010086\n",
      "Gradient for fc1.bias: 0.07337867468595505\n",
      "Gradient for fc2.weight: 0.10291510075330734\n",
      "Gradient for fc2.bias: 0.04694065824151039\n",
      "torch.Size([49]) tensor(-0.0029, grad_fn=<MaxBackward1>) tensor(-2.7255, grad_fn=<MinBackward1>)\n",
      "Episode 196, Loss: -0.03175380825996399\n",
      "Gradient for fc1.weight: 0.07430077344179153\n",
      "Gradient for fc1.bias: 0.051900144666433334\n",
      "Gradient for fc2.weight: 0.1259244978427887\n",
      "Gradient for fc2.bias: 0.023315493017435074\n",
      "torch.Size([43]) tensor(-0.0044, grad_fn=<MaxBackward1>) tensor(-2.3882, grad_fn=<MinBackward1>)\n",
      "Episode 197, Loss: -0.0057080513797700405\n",
      "Gradient for fc1.weight: 0.01807168498635292\n",
      "Gradient for fc1.bias: 0.02462165243923664\n",
      "Gradient for fc2.weight: 0.02918122336268425\n",
      "Gradient for fc2.bias: 0.00814781989902258\n",
      "torch.Size([128]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-3.7904, grad_fn=<MinBackward1>)\n",
      "Episode 198, Loss: 0.009821436367928982\n",
      "Gradient for fc1.weight: 0.019654521718621254\n",
      "Gradient for fc1.bias: 0.08655819296836853\n",
      "Gradient for fc2.weight: 0.11331643909215927\n",
      "Gradient for fc2.bias: 0.05567503347992897\n",
      "torch.Size([48]) tensor(-0.0039, grad_fn=<MaxBackward1>) tensor(-2.5160, grad_fn=<MinBackward1>)\n",
      "Episode 199, Loss: 0.03528358042240143\n",
      "Gradient for fc1.weight: 0.009709754958748817\n",
      "Gradient for fc1.bias: 0.06688424199819565\n",
      "Gradient for fc2.weight: 0.07089435309171677\n",
      "Gradient for fc2.bias: 0.035407181829214096\n",
      "torch.Size([68]) tensor(-0.0062, grad_fn=<MaxBackward1>) tensor(-1.9672, grad_fn=<MinBackward1>)\n",
      "Episode 200, Loss: 0.04387959465384483\n",
      "Gradient for fc1.weight: 0.03181546553969383\n",
      "Gradient for fc1.bias: 0.09023120999336243\n",
      "Gradient for fc2.weight: 0.09113053977489471\n",
      "Gradient for fc2.bias: 0.047478076070547104\n",
      "Running reward: 42.93734674983934\n",
      "torch.Size([45]) tensor(-0.0034, grad_fn=<MaxBackward1>) tensor(-2.6980, grad_fn=<MinBackward1>)\n",
      "Episode 201, Loss: 0.05004076287150383\n",
      "Gradient for fc1.weight: 0.04127509519457817\n",
      "Gradient for fc1.bias: 0.04334104806184769\n",
      "Gradient for fc2.weight: 0.07076911628246307\n",
      "Gradient for fc2.bias: 0.01476712990552187\n",
      "torch.Size([43]) tensor(-0.0035, grad_fn=<MaxBackward1>) tensor(-2.2532, grad_fn=<MinBackward1>)\n",
      "Episode 202, Loss: -0.08301062881946564\n",
      "Gradient for fc1.weight: 0.0618063285946846\n",
      "Gradient for fc1.bias: 0.10000083595514297\n",
      "Gradient for fc2.weight: 0.17864595353603363\n",
      "Gradient for fc2.bias: 0.06286514550447464\n",
      "torch.Size([37]) tensor(-0.0072, grad_fn=<MaxBackward1>) tensor(-1.4370, grad_fn=<MinBackward1>)\n",
      "Episode 203, Loss: -0.11812946945428848\n",
      "Gradient for fc1.weight: 0.029850654304027557\n",
      "Gradient for fc1.bias: 0.058575235307216644\n",
      "Gradient for fc2.weight: 0.09435394406318665\n",
      "Gradient for fc2.bias: 0.022103527560830116\n",
      "torch.Size([111]) tensor(-0.0046, grad_fn=<MaxBackward1>) tensor(-2.2676, grad_fn=<MinBackward1>)\n",
      "Episode 204, Loss: -0.012647882103919983\n",
      "Gradient for fc1.weight: 0.010304534807801247\n",
      "Gradient for fc1.bias: 0.06032116338610649\n",
      "Gradient for fc2.weight: 0.06609391421079636\n",
      "Gradient for fc2.bias: 0.03530697524547577\n",
      "torch.Size([42]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-4.2263, grad_fn=<MinBackward1>)\n",
      "Episode 205, Loss: 0.05259738489985466\n",
      "Gradient for fc1.weight: 0.022844022139906883\n",
      "Gradient for fc1.bias: 0.07294381409883499\n",
      "Gradient for fc2.weight: 0.0837159976363182\n",
      "Gradient for fc2.bias: 0.04330174997448921\n",
      "torch.Size([79]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-3.9129, grad_fn=<MinBackward1>)\n",
      "Episode 206, Loss: -0.06430886685848236\n",
      "Gradient for fc1.weight: 0.023692460730671883\n",
      "Gradient for fc1.bias: 0.045498237013816833\n",
      "Gradient for fc2.weight: 0.06241497024893761\n",
      "Gradient for fc2.bias: 0.02366979420185089\n",
      "torch.Size([85]) tensor(-0.0021, grad_fn=<MaxBackward1>) tensor(-3.4076, grad_fn=<MinBackward1>)\n",
      "Episode 207, Loss: -0.02173071913421154\n",
      "Gradient for fc1.weight: 0.038673147559165955\n",
      "Gradient for fc1.bias: 0.10517420619726181\n",
      "Gradient for fc2.weight: 0.14053024351596832\n",
      "Gradient for fc2.bias: 0.0712473914027214\n",
      "torch.Size([42]) tensor(-0.0082, grad_fn=<MaxBackward1>) tensor(-2.1445, grad_fn=<MinBackward1>)\n",
      "Episode 208, Loss: 0.057405732572078705\n",
      "Gradient for fc1.weight: 0.015128466300666332\n",
      "Gradient for fc1.bias: 0.09085696190595627\n",
      "Gradient for fc2.weight: 0.10045256465673447\n",
      "Gradient for fc2.bias: 0.051410917192697525\n",
      "torch.Size([76]) tensor(-0.0015, grad_fn=<MaxBackward1>) tensor(-3.4939, grad_fn=<MinBackward1>)\n",
      "Episode 209, Loss: -0.10565552115440369\n",
      "Gradient for fc1.weight: 0.023193499073386192\n",
      "Gradient for fc1.bias: 0.09147375822067261\n",
      "Gradient for fc2.weight: 0.13549518585205078\n",
      "Gradient for fc2.bias: 0.05504365265369415\n",
      "torch.Size([62]) tensor(-0.0037, grad_fn=<MaxBackward1>) tensor(-2.6481, grad_fn=<MinBackward1>)\n",
      "Episode 210, Loss: -0.027807071805000305\n",
      "Gradient for fc1.weight: 0.01658162847161293\n",
      "Gradient for fc1.bias: 0.04730872064828873\n",
      "Gradient for fc2.weight: 0.04455208033323288\n",
      "Gradient for fc2.bias: 0.022739842534065247\n",
      "torch.Size([52]) tensor(-0.0049, grad_fn=<MaxBackward1>) tensor(-3.2891, grad_fn=<MinBackward1>)\n",
      "Episode 211, Loss: 0.08499825745820999\n",
      "Gradient for fc1.weight: 0.0672198086977005\n",
      "Gradient for fc1.bias: 0.05946602299809456\n",
      "Gradient for fc2.weight: 0.09312021732330322\n",
      "Gradient for fc2.bias: 0.01735140196979046\n",
      "torch.Size([44]) tensor(-0.0107, grad_fn=<MaxBackward1>) tensor(-1.7688, grad_fn=<MinBackward1>)\n",
      "Episode 212, Loss: -0.01640327274799347\n",
      "Gradient for fc1.weight: 0.037977155297994614\n",
      "Gradient for fc1.bias: 0.08979306370019913\n",
      "Gradient for fc2.weight: 0.1244640201330185\n",
      "Gradient for fc2.bias: 0.05333716422319412\n",
      "torch.Size([34]) tensor(-0.0052, grad_fn=<MaxBackward1>) tensor(-2.8826, grad_fn=<MinBackward1>)\n",
      "Episode 213, Loss: -0.04455960541963577\n",
      "Gradient for fc1.weight: 0.01570543833076954\n",
      "Gradient for fc1.bias: 0.019083376973867416\n",
      "Gradient for fc2.weight: 0.03120081126689911\n",
      "Gradient for fc2.bias: 0.001097606378607452\n",
      "torch.Size([42]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-3.7783, grad_fn=<MinBackward1>)\n",
      "Episode 214, Loss: -0.13986985385417938\n",
      "Gradient for fc1.weight: 0.11064215749502182\n",
      "Gradient for fc1.bias: 0.20567773282527924\n",
      "Gradient for fc2.weight: 0.2905519902706146\n",
      "Gradient for fc2.bias: 0.12147177010774612\n",
      "torch.Size([35]) tensor(-0.0066, grad_fn=<MaxBackward1>) tensor(-1.3356, grad_fn=<MinBackward1>)\n",
      "Episode 215, Loss: 0.0069358134642243385\n",
      "Gradient for fc1.weight: 0.03530753403902054\n",
      "Gradient for fc1.bias: 0.04239539057016373\n",
      "Gradient for fc2.weight: 0.0667753592133522\n",
      "Gradient for fc2.bias: 0.023418108001351357\n",
      "torch.Size([53]) tensor(-0.0050, grad_fn=<MaxBackward1>) tensor(-1.5564, grad_fn=<MinBackward1>)\n",
      "Episode 216, Loss: 0.011086853221058846\n",
      "Gradient for fc1.weight: 0.03158164769411087\n",
      "Gradient for fc1.bias: 0.06791409105062485\n",
      "Gradient for fc2.weight: 0.0951678454875946\n",
      "Gradient for fc2.bias: 0.039958637207746506\n",
      "torch.Size([37]) tensor(-0.0033, grad_fn=<MaxBackward1>) tensor(-1.9787, grad_fn=<MinBackward1>)\n",
      "Episode 217, Loss: 0.0036257989704608917\n",
      "Gradient for fc1.weight: 0.0471855029463768\n",
      "Gradient for fc1.bias: 0.0948563739657402\n",
      "Gradient for fc2.weight: 0.13105648756027222\n",
      "Gradient for fc2.bias: 0.05498764291405678\n",
      "torch.Size([53]) tensor(-0.0121, grad_fn=<MaxBackward1>) tensor(-1.4376, grad_fn=<MinBackward1>)\n",
      "Episode 218, Loss: -0.09401936829090118\n",
      "Gradient for fc1.weight: 0.028008587658405304\n",
      "Gradient for fc1.bias: 0.04910343140363693\n",
      "Gradient for fc2.weight: 0.055096738040447235\n",
      "Gradient for fc2.bias: 0.011671209707856178\n",
      "torch.Size([44]) tensor(-0.0056, grad_fn=<MaxBackward1>) tensor(-1.9763, grad_fn=<MinBackward1>)\n",
      "Episode 219, Loss: 0.02983224391937256\n",
      "Gradient for fc1.weight: 0.03322778269648552\n",
      "Gradient for fc1.bias: 0.22329933941364288\n",
      "Gradient for fc2.weight: 0.23524679243564606\n",
      "Gradient for fc2.bias: 0.12061869353055954\n",
      "torch.Size([33]) tensor(-0.0034, grad_fn=<MaxBackward1>) tensor(-1.7618, grad_fn=<MinBackward1>)\n",
      "Episode 220, Loss: 0.03802499547600746\n",
      "Gradient for fc1.weight: 0.02529844641685486\n",
      "Gradient for fc1.bias: 0.05255524441599846\n",
      "Gradient for fc2.weight: 0.04285159707069397\n",
      "Gradient for fc2.bias: 0.015544397756457329\n",
      "torch.Size([114]) tensor(-0.0040, grad_fn=<MaxBackward1>) tensor(-3.5115, grad_fn=<MinBackward1>)\n",
      "Episode 221, Loss: 0.06724229454994202\n",
      "Gradient for fc1.weight: 0.021509194746613503\n",
      "Gradient for fc1.bias: 0.09230986982584\n",
      "Gradient for fc2.weight: 0.12735819816589355\n",
      "Gradient for fc2.bias: 0.05713379383087158\n",
      "torch.Size([37]) tensor(-0.0042, grad_fn=<MaxBackward1>) tensor(-1.4709, grad_fn=<MinBackward1>)\n",
      "Episode 222, Loss: -0.01119794137775898\n",
      "Gradient for fc1.weight: 0.04528757929801941\n",
      "Gradient for fc1.bias: 0.1400170624256134\n",
      "Gradient for fc2.weight: 0.17580673098564148\n",
      "Gradient for fc2.bias: 0.07963260263204575\n",
      "torch.Size([56]) tensor(-0.0031, grad_fn=<MaxBackward1>) tensor(-2.5765, grad_fn=<MinBackward1>)\n",
      "Episode 223, Loss: 0.015005732886493206\n",
      "Gradient for fc1.weight: 0.021725034341216087\n",
      "Gradient for fc1.bias: 0.057103678584098816\n",
      "Gradient for fc2.weight: 0.06708630174398422\n",
      "Gradient for fc2.bias: 0.033043451607227325\n",
      "torch.Size([76]) tensor(-0.0018, grad_fn=<MaxBackward1>) tensor(-3.0640, grad_fn=<MinBackward1>)\n",
      "Episode 224, Loss: -0.009485560469329357\n",
      "Gradient for fc1.weight: 0.06042082607746124\n",
      "Gradient for fc1.bias: 0.10463741421699524\n",
      "Gradient for fc2.weight: 0.14413103461265564\n",
      "Gradient for fc2.bias: 0.06436493247747421\n",
      "torch.Size([33]) tensor(-0.0012, grad_fn=<MaxBackward1>) tensor(-2.7937, grad_fn=<MinBackward1>)\n",
      "Episode 225, Loss: 0.05863447114825249\n",
      "Gradient for fc1.weight: 0.02630504034459591\n",
      "Gradient for fc1.bias: 0.19692417979240417\n",
      "Gradient for fc2.weight: 0.1985769122838974\n",
      "Gradient for fc2.bias: 0.10196990519762039\n",
      "torch.Size([55]) tensor(-0.0093, grad_fn=<MaxBackward1>) tensor(-1.6384, grad_fn=<MinBackward1>)\n",
      "Episode 226, Loss: -0.07114994525909424\n",
      "Gradient for fc1.weight: 0.043752845376729965\n",
      "Gradient for fc1.bias: 0.04495535418391228\n",
      "Gradient for fc2.weight: 0.06457845121622086\n",
      "Gradient for fc2.bias: 0.005357399117201567\n",
      "torch.Size([81]) tensor(-0.0035, grad_fn=<MaxBackward1>) tensor(-2.7722, grad_fn=<MinBackward1>)\n",
      "Episode 227, Loss: -0.022636985406279564\n",
      "Gradient for fc1.weight: 0.034539684653282166\n",
      "Gradient for fc1.bias: 0.02611306868493557\n",
      "Gradient for fc2.weight: 0.06383098661899567\n",
      "Gradient for fc2.bias: 0.016318876296281815\n",
      "torch.Size([53]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-3.8951, grad_fn=<MinBackward1>)\n",
      "Episode 228, Loss: 0.08662711083889008\n",
      "Gradient for fc1.weight: 0.060939669609069824\n",
      "Gradient for fc1.bias: 0.07473938167095184\n",
      "Gradient for fc2.weight: 0.10832814127206802\n",
      "Gradient for fc2.bias: 0.03374369442462921\n",
      "torch.Size([42]) tensor(-0.0051, grad_fn=<MaxBackward1>) tensor(-3.3421, grad_fn=<MinBackward1>)\n",
      "Episode 229, Loss: 0.10104478895664215\n",
      "Gradient for fc1.weight: 0.031192725524306297\n",
      "Gradient for fc1.bias: 0.12037332355976105\n",
      "Gradient for fc2.weight: 0.19973354041576385\n",
      "Gradient for fc2.bias: 0.083722323179245\n",
      "torch.Size([48]) tensor(-0.0047, grad_fn=<MaxBackward1>) tensor(-1.8725, grad_fn=<MinBackward1>)\n",
      "Episode 230, Loss: 0.014707289636135101\n",
      "Gradient for fc1.weight: 0.02542784810066223\n",
      "Gradient for fc1.bias: 0.18934936821460724\n",
      "Gradient for fc2.weight: 0.19920681416988373\n",
      "Gradient for fc2.bias: 0.10338371992111206\n",
      "torch.Size([47]) tensor(-0.0019, grad_fn=<MaxBackward1>) tensor(-2.3569, grad_fn=<MinBackward1>)\n",
      "Episode 231, Loss: -0.006919656880199909\n",
      "Gradient for fc1.weight: 0.018742835149168968\n",
      "Gradient for fc1.bias: 0.030488580465316772\n",
      "Gradient for fc2.weight: 0.019274311140179634\n",
      "Gradient for fc2.bias: 0.007017867173999548\n",
      "torch.Size([76]) tensor(-0.0026, grad_fn=<MaxBackward1>) tensor(-2.6996, grad_fn=<MinBackward1>)\n",
      "Episode 232, Loss: 0.03400007635354996\n",
      "Gradient for fc1.weight: 0.01170150563120842\n",
      "Gradient for fc1.bias: 0.08000461757183075\n",
      "Gradient for fc2.weight: 0.07624241709709167\n",
      "Gradient for fc2.bias: 0.04086911305785179\n",
      "torch.Size([99]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-3.4154, grad_fn=<MinBackward1>)\n",
      "Episode 233, Loss: 0.006955522578209639\n",
      "Gradient for fc1.weight: 0.041422516107559204\n",
      "Gradient for fc1.bias: 0.10344915091991425\n",
      "Gradient for fc2.weight: 0.1378077119588852\n",
      "Gradient for fc2.bias: 0.06596352159976959\n",
      "torch.Size([55]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-2.6069, grad_fn=<MinBackward1>)\n",
      "Episode 234, Loss: -0.043488070368766785\n",
      "Gradient for fc1.weight: 0.020643683150410652\n",
      "Gradient for fc1.bias: 0.09494003653526306\n",
      "Gradient for fc2.weight: 0.1332024335861206\n",
      "Gradient for fc2.bias: 0.059308748692274094\n",
      "torch.Size([86]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-4.4315, grad_fn=<MinBackward1>)\n",
      "Episode 235, Loss: 0.0016516876639798284\n",
      "Gradient for fc1.weight: 0.03160026669502258\n",
      "Gradient for fc1.bias: 0.16453620791435242\n",
      "Gradient for fc2.weight: 0.19486494362354279\n",
      "Gradient for fc2.bias: 0.09920377284288406\n",
      "torch.Size([69]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-4.6197, grad_fn=<MinBackward1>)\n",
      "Episode 236, Loss: 0.0062398044392466545\n",
      "Gradient for fc1.weight: 0.014317279681563377\n",
      "Gradient for fc1.bias: 0.02070603519678116\n",
      "Gradient for fc2.weight: 0.02980116195976734\n",
      "Gradient for fc2.bias: 0.008107787929475307\n",
      "torch.Size([42]) tensor(-0.0034, grad_fn=<MaxBackward1>) tensor(-2.2348, grad_fn=<MinBackward1>)\n",
      "Episode 237, Loss: -0.05790979042649269\n",
      "Gradient for fc1.weight: 0.1332201361656189\n",
      "Gradient for fc1.bias: 0.43867453932762146\n",
      "Gradient for fc2.weight: 0.6197313666343689\n",
      "Gradient for fc2.bias: 0.27234017848968506\n",
      "torch.Size([49]) tensor(-0.0026, grad_fn=<MaxBackward1>) tensor(-2.7833, grad_fn=<MinBackward1>)\n",
      "Episode 238, Loss: -0.06134440004825592\n",
      "Gradient for fc1.weight: 0.023332076147198677\n",
      "Gradient for fc1.bias: 0.19082914292812347\n",
      "Gradient for fc2.weight: 0.18545547127723694\n",
      "Gradient for fc2.bias: 0.09876149892807007\n",
      "torch.Size([47]) tensor(-0.0045, grad_fn=<MaxBackward1>) tensor(-1.3865, grad_fn=<MinBackward1>)\n",
      "Episode 239, Loss: -0.015527238138020039\n",
      "Gradient for fc1.weight: 0.05763374641537666\n",
      "Gradient for fc1.bias: 0.2514965236186981\n",
      "Gradient for fc2.weight: 0.2908543348312378\n",
      "Gradient for fc2.bias: 0.13902819156646729\n",
      "torch.Size([111]) tensor(-0.0034, grad_fn=<MaxBackward1>) tensor(-2.6429, grad_fn=<MinBackward1>)\n",
      "Episode 240, Loss: -0.008504719473421574\n",
      "Gradient for fc1.weight: 0.02232581377029419\n",
      "Gradient for fc1.bias: 0.1339016854763031\n",
      "Gradient for fc2.weight: 0.16457383334636688\n",
      "Gradient for fc2.bias: 0.07919144630432129\n",
      "torch.Size([44]) tensor(-0.0040, grad_fn=<MaxBackward1>) tensor(-1.7133, grad_fn=<MinBackward1>)\n",
      "Episode 241, Loss: -0.04236752912402153\n",
      "Gradient for fc1.weight: 0.019230445846915245\n",
      "Gradient for fc1.bias: 0.19310900568962097\n",
      "Gradient for fc2.weight: 0.21865397691726685\n",
      "Gradient for fc2.bias: 0.10699351131916046\n",
      "torch.Size([30]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-3.3445, grad_fn=<MinBackward1>)\n",
      "Episode 242, Loss: -0.07086218148469925\n",
      "Gradient for fc1.weight: 0.02799184061586857\n",
      "Gradient for fc1.bias: 0.2098749577999115\n",
      "Gradient for fc2.weight: 0.26051267981529236\n",
      "Gradient for fc2.bias: 0.12208930402994156\n",
      "torch.Size([47]) tensor(-0.0029, grad_fn=<MaxBackward1>) tensor(-2.6845, grad_fn=<MinBackward1>)\n",
      "Episode 243, Loss: -0.04222818464040756\n",
      "Gradient for fc1.weight: 0.08666595816612244\n",
      "Gradient for fc1.bias: 0.20009534060955048\n",
      "Gradient for fc2.weight: 0.2634871006011963\n",
      "Gradient for fc2.bias: 0.1275136023759842\n",
      "torch.Size([36]) tensor(-0.0094, grad_fn=<MaxBackward1>) tensor(-1.0574, grad_fn=<MinBackward1>)\n",
      "Episode 244, Loss: 0.06654301285743713\n",
      "Gradient for fc1.weight: 0.019150052219629288\n",
      "Gradient for fc1.bias: 0.06683877855539322\n",
      "Gradient for fc2.weight: 0.08284822106361389\n",
      "Gradient for fc2.bias: 0.03553013503551483\n",
      "torch.Size([67]) tensor(-0.0014, grad_fn=<MaxBackward1>) tensor(-2.9808, grad_fn=<MinBackward1>)\n",
      "Episode 245, Loss: 0.03349380940198898\n",
      "Gradient for fc1.weight: 0.04507069289684296\n",
      "Gradient for fc1.bias: 0.10748988389968872\n",
      "Gradient for fc2.weight: 0.12716183066368103\n",
      "Gradient for fc2.bias: 0.05805923417210579\n",
      "torch.Size([60]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-3.3514, grad_fn=<MinBackward1>)\n",
      "Episode 246, Loss: 0.06018814817070961\n",
      "Gradient for fc1.weight: 0.019392341375350952\n",
      "Gradient for fc1.bias: 0.0973718911409378\n",
      "Gradient for fc2.weight: 0.10142937302589417\n",
      "Gradient for fc2.bias: 0.0525105781853199\n",
      "torch.Size([41]) tensor(-0.0037, grad_fn=<MaxBackward1>) tensor(-3.4635, grad_fn=<MinBackward1>)\n",
      "Episode 247, Loss: 0.1192585676908493\n",
      "Gradient for fc1.weight: 0.028609726577997208\n",
      "Gradient for fc1.bias: 0.0847591906785965\n",
      "Gradient for fc2.weight: 0.11993777751922607\n",
      "Gradient for fc2.bias: 0.047974441200494766\n",
      "torch.Size([52]) tensor(-0.0016, grad_fn=<MaxBackward1>) tensor(-2.4310, grad_fn=<MinBackward1>)\n",
      "Episode 248, Loss: -0.019334005191922188\n",
      "Gradient for fc1.weight: 0.04086457937955856\n",
      "Gradient for fc1.bias: 0.05436075106263161\n",
      "Gradient for fc2.weight: 0.07355903834104538\n",
      "Gradient for fc2.bias: 0.0258652213960886\n",
      "torch.Size([40]) tensor(-0.0049, grad_fn=<MaxBackward1>) tensor(-1.4317, grad_fn=<MinBackward1>)\n",
      "Episode 249, Loss: 0.04827014356851578\n",
      "Gradient for fc1.weight: 0.03400076925754547\n",
      "Gradient for fc1.bias: 0.03391481563448906\n",
      "Gradient for fc2.weight: 0.04541435092687607\n",
      "Gradient for fc2.bias: 0.010927509516477585\n",
      "torch.Size([34]) tensor(-0.0034, grad_fn=<MaxBackward1>) tensor(-3.5374, grad_fn=<MinBackward1>)\n",
      "Episode 250, Loss: -0.22801241278648376\n",
      "Gradient for fc1.weight: 0.04776962846517563\n",
      "Gradient for fc1.bias: 0.11122120916843414\n",
      "Gradient for fc2.weight: 0.20078444480895996\n",
      "Gradient for fc2.bias: 0.06645651906728745\n",
      "torch.Size([52]) tensor(-0.0057, grad_fn=<MaxBackward1>) tensor(-2.4076, grad_fn=<MinBackward1>)\n",
      "Episode 251, Loss: -0.045105017721652985\n",
      "Gradient for fc1.weight: 0.06144062802195549\n",
      "Gradient for fc1.bias: 0.1185230240225792\n",
      "Gradient for fc2.weight: 0.139125257730484\n",
      "Gradient for fc2.bias: 0.06078445538878441\n",
      "torch.Size([143]) tensor(-0.0016, grad_fn=<MaxBackward1>) tensor(-2.8184, grad_fn=<MinBackward1>)\n",
      "Episode 252, Loss: 0.004559870809316635\n",
      "Gradient for fc1.weight: 0.008906242437660694\n",
      "Gradient for fc1.bias: 0.020764470100402832\n",
      "Gradient for fc2.weight: 0.023256825283169746\n",
      "Gradient for fc2.bias: 0.010246660560369492\n",
      "torch.Size([62]) tensor(-0.0014, grad_fn=<MaxBackward1>) tensor(-3.0236, grad_fn=<MinBackward1>)\n",
      "Episode 253, Loss: 0.09922991693019867\n",
      "Gradient for fc1.weight: 0.03326794132590294\n",
      "Gradient for fc1.bias: 0.07808886468410492\n",
      "Gradient for fc2.weight: 0.08400449901819229\n",
      "Gradient for fc2.bias: 0.03125669062137604\n",
      "torch.Size([34]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-2.7695, grad_fn=<MinBackward1>)\n",
      "Episode 254, Loss: -0.0610840879380703\n",
      "Gradient for fc1.weight: 0.039219193160533905\n",
      "Gradient for fc1.bias: 0.04982506483793259\n",
      "Gradient for fc2.weight: 0.057841960340738297\n",
      "Gradient for fc2.bias: 0.009608760476112366\n",
      "torch.Size([43]) tensor(-0.0025, grad_fn=<MaxBackward1>) tensor(-1.9194, grad_fn=<MinBackward1>)\n",
      "Episode 255, Loss: -0.04313698410987854\n",
      "Gradient for fc1.weight: 0.08466892689466476\n",
      "Gradient for fc1.bias: 0.06510774046182632\n",
      "Gradient for fc2.weight: 0.10814251005649567\n",
      "Gradient for fc2.bias: 0.022769350558519363\n",
      "torch.Size([37]) tensor(-0.0015, grad_fn=<MaxBackward1>) tensor(-2.6929, grad_fn=<MinBackward1>)\n",
      "Episode 256, Loss: -0.011194277554750443\n",
      "Gradient for fc1.weight: 0.01425447128713131\n",
      "Gradient for fc1.bias: 0.03113480471074581\n",
      "Gradient for fc2.weight: 0.02422555349767208\n",
      "Gradient for fc2.bias: 0.010258876718580723\n",
      "torch.Size([30]) tensor(-0.0055, grad_fn=<MaxBackward1>) tensor(-1.5605, grad_fn=<MinBackward1>)\n",
      "Episode 257, Loss: -0.08894669264554977\n",
      "Gradient for fc1.weight: 0.11252472549676895\n",
      "Gradient for fc1.bias: 0.06579957902431488\n",
      "Gradient for fc2.weight: 0.12899956107139587\n",
      "Gradient for fc2.bias: 0.015586077235639095\n",
      "torch.Size([30]) tensor(-0.0021, grad_fn=<MaxBackward1>) tensor(-1.9148, grad_fn=<MinBackward1>)\n",
      "Episode 258, Loss: 0.0503990575671196\n",
      "Gradient for fc1.weight: 0.04891035705804825\n",
      "Gradient for fc1.bias: 0.18220719695091248\n",
      "Gradient for fc2.weight: 0.19679546356201172\n",
      "Gradient for fc2.bias: 0.09602028131484985\n",
      "torch.Size([90]) tensor(-0.0019, grad_fn=<MaxBackward1>) tensor(-2.0877, grad_fn=<MinBackward1>)\n",
      "Episode 259, Loss: 0.005103054456412792\n",
      "Gradient for fc1.weight: 0.007463499438017607\n",
      "Gradient for fc1.bias: 0.05359118431806564\n",
      "Gradient for fc2.weight: 0.05252678692340851\n",
      "Gradient for fc2.bias: 0.027799183502793312\n",
      "torch.Size([38]) tensor(-0.0029, grad_fn=<MaxBackward1>) tensor(-3.4318, grad_fn=<MinBackward1>)\n",
      "Episode 260, Loss: -0.100152388215065\n",
      "Gradient for fc1.weight: 0.06234033778309822\n",
      "Gradient for fc1.bias: 0.061654385179281235\n",
      "Gradient for fc2.weight: 0.08805380016565323\n",
      "Gradient for fc2.bias: 0.022600799798965454\n",
      "torch.Size([106]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-3.1009, grad_fn=<MinBackward1>)\n",
      "Episode 261, Loss: 0.04173450544476509\n",
      "Gradient for fc1.weight: 0.02289046160876751\n",
      "Gradient for fc1.bias: 0.09635048359632492\n",
      "Gradient for fc2.weight: 0.09950508177280426\n",
      "Gradient for fc2.bias: 0.05099749192595482\n",
      "torch.Size([30]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-4.2255, grad_fn=<MinBackward1>)\n",
      "Episode 262, Loss: 0.1289115697145462\n",
      "Gradient for fc1.weight: 0.049459200352430344\n",
      "Gradient for fc1.bias: 0.12109994143247604\n",
      "Gradient for fc2.weight: 0.12506552040576935\n",
      "Gradient for fc2.bias: 0.058522067964076996\n",
      "torch.Size([109]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-3.5797, grad_fn=<MinBackward1>)\n",
      "Episode 263, Loss: 0.025147534906864166\n",
      "Gradient for fc1.weight: 0.06408336758613586\n",
      "Gradient for fc1.bias: 0.06758041679859161\n",
      "Gradient for fc2.weight: 0.10030633211135864\n",
      "Gradient for fc2.bias: 0.03131934255361557\n",
      "torch.Size([72]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-3.6403, grad_fn=<MinBackward1>)\n",
      "Episode 264, Loss: 0.17292392253875732\n",
      "Gradient for fc1.weight: 0.04446350783109665\n",
      "Gradient for fc1.bias: 0.07647594064474106\n",
      "Gradient for fc2.weight: 0.1087154671549797\n",
      "Gradient for fc2.bias: 0.027262520045042038\n",
      "torch.Size([60]) tensor(-0.0051, grad_fn=<MaxBackward1>) tensor(-1.2391, grad_fn=<MinBackward1>)\n",
      "Episode 265, Loss: -0.06351882219314575\n",
      "Gradient for fc1.weight: 0.03288756683468819\n",
      "Gradient for fc1.bias: 0.037844881415367126\n",
      "Gradient for fc2.weight: 0.055027008056640625\n",
      "Gradient for fc2.bias: 0.018934885039925575\n",
      "torch.Size([51]) tensor(-0.0017, grad_fn=<MaxBackward1>) tensor(-2.6625, grad_fn=<MinBackward1>)\n",
      "Episode 266, Loss: 0.07492926716804504\n",
      "Gradient for fc1.weight: 0.06400369852781296\n",
      "Gradient for fc1.bias: 0.21216896176338196\n",
      "Gradient for fc2.weight: 0.24128541350364685\n",
      "Gradient for fc2.bias: 0.1183323860168457\n",
      "torch.Size([44]) tensor(-0.0046, grad_fn=<MaxBackward1>) tensor(-2.3119, grad_fn=<MinBackward1>)\n",
      "Episode 267, Loss: 0.02431529574096203\n",
      "Gradient for fc1.weight: 0.02230171114206314\n",
      "Gradient for fc1.bias: 0.048264533281326294\n",
      "Gradient for fc2.weight: 0.0376085489988327\n",
      "Gradient for fc2.bias: 0.01821598783135414\n",
      "torch.Size([39]) tensor(-0.0102, grad_fn=<MaxBackward1>) tensor(-1.6135, grad_fn=<MinBackward1>)\n",
      "Episode 268, Loss: 0.053323183208703995\n",
      "Gradient for fc1.weight: 0.03300859034061432\n",
      "Gradient for fc1.bias: 0.033811137080192566\n",
      "Gradient for fc2.weight: 0.04091851785778999\n",
      "Gradient for fc2.bias: 0.004227995406836271\n",
      "torch.Size([32]) tensor(-0.0042, grad_fn=<MaxBackward1>) tensor(-2.3096, grad_fn=<MinBackward1>)\n",
      "Episode 269, Loss: -0.12490101158618927\n",
      "Gradient for fc1.weight: 0.026479918509721756\n",
      "Gradient for fc1.bias: 0.13708244264125824\n",
      "Gradient for fc2.weight: 0.1441715508699417\n",
      "Gradient for fc2.bias: 0.06675572693347931\n",
      "torch.Size([66]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-4.6639, grad_fn=<MinBackward1>)\n",
      "Episode 270, Loss: 0.032059721648693085\n",
      "Gradient for fc1.weight: 0.04340340197086334\n",
      "Gradient for fc1.bias: 0.08124059438705444\n",
      "Gradient for fc2.weight: 0.11777202785015106\n",
      "Gradient for fc2.bias: 0.05193387717008591\n",
      "torch.Size([40]) tensor(-0.0045, grad_fn=<MaxBackward1>) tensor(-1.9240, grad_fn=<MinBackward1>)\n",
      "Episode 271, Loss: 0.03675302118062973\n",
      "Gradient for fc1.weight: 0.04546722397208214\n",
      "Gradient for fc1.bias: 0.06985346227884293\n",
      "Gradient for fc2.weight: 0.08276426792144775\n",
      "Gradient for fc2.bias: 0.03435809537768364\n",
      "torch.Size([39]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-3.0148, grad_fn=<MinBackward1>)\n",
      "Episode 272, Loss: -0.04369060695171356\n",
      "Gradient for fc1.weight: 0.05812310054898262\n",
      "Gradient for fc1.bias: 0.079879529774189\n",
      "Gradient for fc2.weight: 0.11189652979373932\n",
      "Gradient for fc2.bias: 0.04280935227870941\n",
      "torch.Size([41]) tensor(-0.0024, grad_fn=<MaxBackward1>) tensor(-2.2320, grad_fn=<MinBackward1>)\n",
      "Episode 273, Loss: -0.014021630398929119\n",
      "Gradient for fc1.weight: 0.10513331741094589\n",
      "Gradient for fc1.bias: 0.17974361777305603\n",
      "Gradient for fc2.weight: 0.23226551711559296\n",
      "Gradient for fc2.bias: 0.09636754542589188\n",
      "torch.Size([58]) tensor(-0.0027, grad_fn=<MaxBackward1>) tensor(-1.5291, grad_fn=<MinBackward1>)\n",
      "Episode 274, Loss: 0.001007252256385982\n",
      "Gradient for fc1.weight: 0.015314266085624695\n",
      "Gradient for fc1.bias: 0.061404500156641006\n",
      "Gradient for fc2.weight: 0.06689456850290298\n",
      "Gradient for fc2.bias: 0.03317322954535484\n",
      "torch.Size([36]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-2.9350, grad_fn=<MinBackward1>)\n",
      "Episode 275, Loss: -0.04323172569274902\n",
      "Gradient for fc1.weight: 0.02209075726568699\n",
      "Gradient for fc1.bias: 0.04237063229084015\n",
      "Gradient for fc2.weight: 0.04478701576590538\n",
      "Gradient for fc2.bias: 0.011215737089514732\n",
      "torch.Size([47]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-2.7138, grad_fn=<MinBackward1>)\n",
      "Episode 276, Loss: -0.03927546367049217\n",
      "Gradient for fc1.weight: 0.04988463968038559\n",
      "Gradient for fc1.bias: 0.04192429035902023\n",
      "Gradient for fc2.weight: 0.078705795109272\n",
      "Gradient for fc2.bias: 0.018561629578471184\n",
      "torch.Size([89]) tensor(-0.0016, grad_fn=<MaxBackward1>) tensor(-2.7813, grad_fn=<MinBackward1>)\n",
      "Episode 277, Loss: -0.03163770213723183\n",
      "Gradient for fc1.weight: 0.09039291739463806\n",
      "Gradient for fc1.bias: 0.18650826811790466\n",
      "Gradient for fc2.weight: 0.23986166715621948\n",
      "Gradient for fc2.bias: 0.11202233284711838\n",
      "torch.Size([46]) tensor(-0.0026, grad_fn=<MaxBackward1>) tensor(-2.8480, grad_fn=<MinBackward1>)\n",
      "Episode 278, Loss: 0.06834965199232101\n",
      "Gradient for fc1.weight: 0.04654323682188988\n",
      "Gradient for fc1.bias: 0.22183144092559814\n",
      "Gradient for fc2.weight: 0.24168181419372559\n",
      "Gradient for fc2.bias: 0.12202604115009308\n",
      "torch.Size([61]) tensor(-0.0015, grad_fn=<MaxBackward1>) tensor(-2.1036, grad_fn=<MinBackward1>)\n",
      "Episode 279, Loss: -0.04676324501633644\n",
      "Gradient for fc1.weight: 0.020979011431336403\n",
      "Gradient for fc1.bias: 0.0362149141728878\n",
      "Gradient for fc2.weight: 0.0404181145131588\n",
      "Gradient for fc2.bias: 0.012209584936499596\n",
      "torch.Size([41]) tensor(-0.0055, grad_fn=<MaxBackward1>) tensor(-0.8521, grad_fn=<MinBackward1>)\n",
      "Episode 280, Loss: 0.01821630820631981\n",
      "Gradient for fc1.weight: 0.021894099190831184\n",
      "Gradient for fc1.bias: 0.11686209589242935\n",
      "Gradient for fc2.weight: 0.09728541970252991\n",
      "Gradient for fc2.bias: 0.05246039479970932\n",
      "torch.Size([47]) tensor(-0.0023, grad_fn=<MaxBackward1>) tensor(-1.7236, grad_fn=<MinBackward1>)\n",
      "Episode 281, Loss: 0.04553750157356262\n",
      "Gradient for fc1.weight: 0.04318201541900635\n",
      "Gradient for fc1.bias: 0.03747597336769104\n",
      "Gradient for fc2.weight: 0.0471557155251503\n",
      "Gradient for fc2.bias: 0.001364297466352582\n",
      "torch.Size([48]) tensor(-0.0015, grad_fn=<MaxBackward1>) tensor(-3.9917, grad_fn=<MinBackward1>)\n",
      "Episode 282, Loss: -0.10809063166379929\n",
      "Gradient for fc1.weight: 0.1187148317694664\n",
      "Gradient for fc1.bias: 0.1688152253627777\n",
      "Gradient for fc2.weight: 0.27079683542251587\n",
      "Gradient for fc2.bias: 0.0980115458369255\n",
      "torch.Size([53]) tensor(-0.0023, grad_fn=<MaxBackward1>) tensor(-1.7364, grad_fn=<MinBackward1>)\n",
      "Episode 283, Loss: 0.08775876462459564\n",
      "Gradient for fc1.weight: 0.018749099224805832\n",
      "Gradient for fc1.bias: 0.08545274287462234\n",
      "Gradient for fc2.weight: 0.09574275463819504\n",
      "Gradient for fc2.bias: 0.044700995087623596\n",
      "torch.Size([58]) tensor(-0.0017, grad_fn=<MaxBackward1>) tensor(-3.7777, grad_fn=<MinBackward1>)\n",
      "Episode 284, Loss: -0.1175968125462532\n",
      "Gradient for fc1.weight: 0.04452908784151077\n",
      "Gradient for fc1.bias: 0.08480653911828995\n",
      "Gradient for fc2.weight: 0.11653149873018265\n",
      "Gradient for fc2.bias: 0.04414457455277443\n",
      "torch.Size([68]) tensor(-0.0015, grad_fn=<MaxBackward1>) tensor(-2.9203, grad_fn=<MinBackward1>)\n",
      "Episode 285, Loss: -0.05941811203956604\n",
      "Gradient for fc1.weight: 0.014202979393303394\n",
      "Gradient for fc1.bias: 0.08341474086046219\n",
      "Gradient for fc2.weight: 0.08970984071493149\n",
      "Gradient for fc2.bias: 0.04200027137994766\n",
      "torch.Size([40]) tensor(-0.0025, grad_fn=<MaxBackward1>) tensor(-2.8528, grad_fn=<MinBackward1>)\n",
      "Episode 286, Loss: -0.030482351779937744\n",
      "Gradient for fc1.weight: 0.055014923214912415\n",
      "Gradient for fc1.bias: 0.12433312088251114\n",
      "Gradient for fc2.weight: 0.15413792431354523\n",
      "Gradient for fc2.bias: 0.07283345609903336\n",
      "torch.Size([68]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-3.4836, grad_fn=<MinBackward1>)\n",
      "Episode 287, Loss: -0.08879870176315308\n",
      "Gradient for fc1.weight: 0.029778968542814255\n",
      "Gradient for fc1.bias: 0.05676531791687012\n",
      "Gradient for fc2.weight: 0.07839486747980118\n",
      "Gradient for fc2.bias: 0.02321806736290455\n",
      "torch.Size([41]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-2.4268, grad_fn=<MinBackward1>)\n",
      "Episode 288, Loss: -0.043241117149591446\n",
      "Gradient for fc1.weight: 0.021741526201367378\n",
      "Gradient for fc1.bias: 0.04028097167611122\n",
      "Gradient for fc2.weight: 0.05445769801735878\n",
      "Gradient for fc2.bias: 0.01977413147687912\n",
      "torch.Size([57]) tensor(-0.0026, grad_fn=<MaxBackward1>) tensor(-2.2271, grad_fn=<MinBackward1>)\n",
      "Episode 289, Loss: -0.020670732483267784\n",
      "Gradient for fc1.weight: 0.015268644317984581\n",
      "Gradient for fc1.bias: 0.06271810084581375\n",
      "Gradient for fc2.weight: 0.047748465090990067\n",
      "Gradient for fc2.bias: 0.022253647446632385\n",
      "torch.Size([73]) tensor(-0.0030, grad_fn=<MaxBackward1>) tensor(-2.1232, grad_fn=<MinBackward1>)\n",
      "Episode 290, Loss: -0.04063248634338379\n",
      "Gradient for fc1.weight: 0.034070663154125214\n",
      "Gradient for fc1.bias: 0.16681741178035736\n",
      "Gradient for fc2.weight: 0.165532186627388\n",
      "Gradient for fc2.bias: 0.0833979994058609\n",
      "torch.Size([39]) tensor(-0.0036, grad_fn=<MaxBackward1>) tensor(-2.9458, grad_fn=<MinBackward1>)\n",
      "Episode 291, Loss: -0.04980562999844551\n",
      "Gradient for fc1.weight: 0.04730915650725365\n",
      "Gradient for fc1.bias: 0.048963531851768494\n",
      "Gradient for fc2.weight: 0.0544893778860569\n",
      "Gradient for fc2.bias: 0.007556283846497536\n",
      "torch.Size([33]) tensor(-0.0017, grad_fn=<MaxBackward1>) tensor(-4.0416, grad_fn=<MinBackward1>)\n",
      "Episode 292, Loss: -0.016763409599661827\n",
      "Gradient for fc1.weight: 0.014282413758337498\n",
      "Gradient for fc1.bias: 0.08030032366514206\n",
      "Gradient for fc2.weight: 0.06019435077905655\n",
      "Gradient for fc2.bias: 0.030913271009922028\n",
      "torch.Size([78]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-3.9189, grad_fn=<MinBackward1>)\n",
      "Episode 293, Loss: 0.05602791905403137\n",
      "Gradient for fc1.weight: 0.06556398421525955\n",
      "Gradient for fc1.bias: 0.06330864876508713\n",
      "Gradient for fc2.weight: 0.08344781398773193\n",
      "Gradient for fc2.bias: 0.028311608359217644\n",
      "torch.Size([72]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-4.9059, grad_fn=<MinBackward1>)\n",
      "Episode 294, Loss: -0.01703295111656189\n",
      "Gradient for fc1.weight: 0.01787545159459114\n",
      "Gradient for fc1.bias: 0.04855302348732948\n",
      "Gradient for fc2.weight: 0.07159910351037979\n",
      "Gradient for fc2.bias: 0.03276607021689415\n",
      "torch.Size([39]) tensor(-0.0029, grad_fn=<MaxBackward1>) tensor(-2.6904, grad_fn=<MinBackward1>)\n",
      "Episode 295, Loss: -0.09760480374097824\n",
      "Gradient for fc1.weight: 0.08062034845352173\n",
      "Gradient for fc1.bias: 0.22956423461437225\n",
      "Gradient for fc2.weight: 0.2560974657535553\n",
      "Gradient for fc2.bias: 0.12426867336034775\n",
      "torch.Size([43]) tensor(-0.0019, grad_fn=<MaxBackward1>) tensor(-1.6969, grad_fn=<MinBackward1>)\n",
      "Episode 296, Loss: -0.020491357892751694\n",
      "Gradient for fc1.weight: 0.04776587709784508\n",
      "Gradient for fc1.bias: 0.2843223810195923\n",
      "Gradient for fc2.weight: 0.2551667094230652\n",
      "Gradient for fc2.bias: 0.13089536130428314\n",
      "torch.Size([33]) tensor(-0.0028, grad_fn=<MaxBackward1>) tensor(-1.3014, grad_fn=<MinBackward1>)\n",
      "Episode 297, Loss: 0.05863552540540695\n",
      "Gradient for fc1.weight: 0.016340365633368492\n",
      "Gradient for fc1.bias: 0.11169233173131943\n",
      "Gradient for fc2.weight: 0.09583670645952225\n",
      "Gradient for fc2.bias: 0.049137406051158905\n",
      "torch.Size([28]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-2.3389, grad_fn=<MinBackward1>)\n",
      "Episode 298, Loss: -0.0484255775809288\n",
      "Gradient for fc1.weight: 0.08201292902231216\n",
      "Gradient for fc1.bias: 0.14303749799728394\n",
      "Gradient for fc2.weight: 0.15094949305057526\n",
      "Gradient for fc2.bias: 0.06600816547870636\n",
      "torch.Size([38]) tensor(-0.0019, grad_fn=<MaxBackward1>) tensor(-1.4896, grad_fn=<MinBackward1>)\n",
      "Episode 299, Loss: 0.09219688177108765\n",
      "Gradient for fc1.weight: 0.038483407348394394\n",
      "Gradient for fc1.bias: 0.16555769741535187\n",
      "Gradient for fc2.weight: 0.142037034034729\n",
      "Gradient for fc2.bias: 0.07190445810556412\n",
      "torch.Size([56]) tensor(-0.0083, grad_fn=<MaxBackward1>) tensor(-1.8068, grad_fn=<MinBackward1>)\n",
      "Episode 300, Loss: 0.03563639149069786\n",
      "Gradient for fc1.weight: 0.015074723400175571\n",
      "Gradient for fc1.bias: 0.21482808887958527\n",
      "Gradient for fc2.weight: 0.18218381702899933\n",
      "Gradient for fc2.bias: 0.0945495218038559\n",
      "Running reward: 38.771179388736776\n",
      "torch.Size([31]) tensor(-0.0022, grad_fn=<MaxBackward1>) tensor(-2.2641, grad_fn=<MinBackward1>)\n",
      "Episode 301, Loss: 0.0628940686583519\n",
      "Gradient for fc1.weight: 0.01901153475046158\n",
      "Gradient for fc1.bias: 0.2401718646287918\n",
      "Gradient for fc2.weight: 0.22394037246704102\n",
      "Gradient for fc2.bias: 0.11445964872837067\n",
      "torch.Size([53]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-2.0360, grad_fn=<MinBackward1>)\n",
      "Episode 302, Loss: -0.11031536012887955\n",
      "Gradient for fc1.weight: 0.04362891986966133\n",
      "Gradient for fc1.bias: 0.17450152337551117\n",
      "Gradient for fc2.weight: 0.1839006394147873\n",
      "Gradient for fc2.bias: 0.08427519351243973\n",
      "torch.Size([47]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.7460, grad_fn=<MinBackward1>)\n",
      "Episode 303, Loss: 0.032435230910778046\n",
      "Gradient for fc1.weight: 0.04353059455752373\n",
      "Gradient for fc1.bias: 0.04096522927284241\n",
      "Gradient for fc2.weight: 0.04304877668619156\n",
      "Gradient for fc2.bias: 0.00041718926513567567\n",
      "torch.Size([33]) tensor(-0.0134, grad_fn=<MaxBackward1>) tensor(-1.6145, grad_fn=<MinBackward1>)\n",
      "Episode 304, Loss: 0.07295799255371094\n",
      "Gradient for fc1.weight: 0.02522837370634079\n",
      "Gradient for fc1.bias: 0.20611795783042908\n",
      "Gradient for fc2.weight: 0.19376671314239502\n",
      "Gradient for fc2.bias: 0.09269298613071442\n",
      "torch.Size([54]) tensor(-0.0040, grad_fn=<MaxBackward1>) tensor(-1.4020, grad_fn=<MinBackward1>)\n",
      "Episode 305, Loss: -0.0009586020023562014\n",
      "Gradient for fc1.weight: 0.047977298498153687\n",
      "Gradient for fc1.bias: 0.13335557281970978\n",
      "Gradient for fc2.weight: 0.14606833457946777\n",
      "Gradient for fc2.bias: 0.06522189825773239\n",
      "torch.Size([32]) tensor(-0.0026, grad_fn=<MaxBackward1>) tensor(-2.5870, grad_fn=<MinBackward1>)\n",
      "Episode 306, Loss: -0.09327631443738937\n",
      "Gradient for fc1.weight: 0.07828467339277267\n",
      "Gradient for fc1.bias: 0.1904975175857544\n",
      "Gradient for fc2.weight: 0.19289454817771912\n",
      "Gradient for fc2.bias: 0.08570192754268646\n",
      "torch.Size([70]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-3.8097, grad_fn=<MinBackward1>)\n",
      "Episode 307, Loss: 0.010394508019089699\n",
      "Gradient for fc1.weight: 0.010285740718245506\n",
      "Gradient for fc1.bias: 0.13854511082172394\n",
      "Gradient for fc2.weight: 0.10770044475793839\n",
      "Gradient for fc2.bias: 0.055746156722307205\n",
      "torch.Size([46]) tensor(-0.0015, grad_fn=<MaxBackward1>) tensor(-3.4154, grad_fn=<MinBackward1>)\n",
      "Episode 308, Loss: 0.18233294785022736\n",
      "Gradient for fc1.weight: 0.08262477815151215\n",
      "Gradient for fc1.bias: 0.12656615674495697\n",
      "Gradient for fc2.weight: 0.1131204217672348\n",
      "Gradient for fc2.bias: 0.02712002769112587\n",
      "torch.Size([36]) tensor(-7.6058e-05, grad_fn=<MaxBackward1>) tensor(-5.3960, grad_fn=<MinBackward1>)\n",
      "Episode 309, Loss: 0.09946617484092712\n",
      "Gradient for fc1.weight: 0.13666756451129913\n",
      "Gradient for fc1.bias: 0.31538739800453186\n",
      "Gradient for fc2.weight: 0.3179602324962616\n",
      "Gradient for fc2.bias: 0.14990536868572235\n",
      "torch.Size([54]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-2.9439, grad_fn=<MinBackward1>)\n",
      "Episode 310, Loss: 0.013567917980253696\n",
      "Gradient for fc1.weight: 0.03593110293149948\n",
      "Gradient for fc1.bias: 0.12478474527597427\n",
      "Gradient for fc2.weight: 0.13404344022274017\n",
      "Gradient for fc2.bias: 0.0596780925989151\n",
      "torch.Size([55]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-2.9508, grad_fn=<MinBackward1>)\n",
      "Episode 311, Loss: -0.020231422036886215\n",
      "Gradient for fc1.weight: 0.018857551738619804\n",
      "Gradient for fc1.bias: 0.03461048752069473\n",
      "Gradient for fc2.weight: 0.026891784742474556\n",
      "Gradient for fc2.bias: 0.0008235388668254018\n",
      "torch.Size([57]) tensor(-0.0038, grad_fn=<MaxBackward1>) tensor(-1.0895, grad_fn=<MinBackward1>)\n",
      "Episode 312, Loss: -0.037789516150951385\n",
      "Gradient for fc1.weight: 0.039172135293483734\n",
      "Gradient for fc1.bias: 0.2688477635383606\n",
      "Gradient for fc2.weight: 0.23674039542675018\n",
      "Gradient for fc2.bias: 0.11887885630130768\n",
      "torch.Size([32]) tensor(-0.0053, grad_fn=<MaxBackward1>) tensor(-3.1698, grad_fn=<MinBackward1>)\n",
      "Episode 313, Loss: 0.14993225038051605\n",
      "Gradient for fc1.weight: 0.05593157932162285\n",
      "Gradient for fc1.bias: 0.11082233488559723\n",
      "Gradient for fc2.weight: 0.17410749197006226\n",
      "Gradient for fc2.bias: 0.05447885021567345\n",
      "torch.Size([90]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-3.0583, grad_fn=<MinBackward1>)\n",
      "Episode 314, Loss: -0.03090720623731613\n",
      "Gradient for fc1.weight: 0.012446756474673748\n",
      "Gradient for fc1.bias: 0.03408926725387573\n",
      "Gradient for fc2.weight: 0.021294619888067245\n",
      "Gradient for fc2.bias: 0.00579046132043004\n",
      "torch.Size([28]) tensor(-0.0055, grad_fn=<MaxBackward1>) tensor(-2.6705, grad_fn=<MinBackward1>)\n",
      "Episode 315, Loss: 0.1388990581035614\n",
      "Gradient for fc1.weight: 0.16034343838691711\n",
      "Gradient for fc1.bias: 0.3828103542327881\n",
      "Gradient for fc2.weight: 0.49871814250946045\n",
      "Gradient for fc2.bias: 0.19074060022830963\n",
      "torch.Size([70]) tensor(-0.0022, grad_fn=<MaxBackward1>) tensor(-2.7868, grad_fn=<MinBackward1>)\n",
      "Episode 316, Loss: 0.02532958984375\n",
      "Gradient for fc1.weight: 0.06105425953865051\n",
      "Gradient for fc1.bias: 0.17651768028736115\n",
      "Gradient for fc2.weight: 0.1877993494272232\n",
      "Gradient for fc2.bias: 0.08950567990541458\n",
      "torch.Size([37]) tensor(-0.0034, grad_fn=<MaxBackward1>) tensor(-1.8709, grad_fn=<MinBackward1>)\n",
      "Episode 317, Loss: -0.006174791604280472\n",
      "Gradient for fc1.weight: 0.02280900441110134\n",
      "Gradient for fc1.bias: 0.2583620846271515\n",
      "Gradient for fc2.weight: 0.19983230531215668\n",
      "Gradient for fc2.bias: 0.10478749126195908\n",
      "torch.Size([43]) tensor(-0.0012, grad_fn=<MaxBackward1>) tensor(-2.6024, grad_fn=<MinBackward1>)\n",
      "Episode 318, Loss: 0.03368673101067543\n",
      "Gradient for fc1.weight: 0.057057935744524\n",
      "Gradient for fc1.bias: 0.10745693743228912\n",
      "Gradient for fc2.weight: 0.09315629303455353\n",
      "Gradient for fc2.bias: 0.041581906378269196\n",
      "torch.Size([33]) tensor(-0.0856, grad_fn=<MaxBackward1>) tensor(-1.6179, grad_fn=<MinBackward1>)\n",
      "Episode 319, Loss: -0.13565672934055328\n",
      "Gradient for fc1.weight: 0.17696446180343628\n",
      "Gradient for fc1.bias: 0.2206851691007614\n",
      "Gradient for fc2.weight: 0.39540836215019226\n",
      "Gradient for fc2.bias: 0.10631510615348816\n",
      "torch.Size([51]) tensor(-0.0060, grad_fn=<MaxBackward1>) tensor(-2.2784, grad_fn=<MinBackward1>)\n",
      "Episode 320, Loss: -0.09670261293649673\n",
      "Gradient for fc1.weight: 0.08733592182397842\n",
      "Gradient for fc1.bias: 0.11531113088130951\n",
      "Gradient for fc2.weight: 0.13251566886901855\n",
      "Gradient for fc2.bias: 0.0025606686249375343\n",
      "torch.Size([24]) tensor(-0.0262, grad_fn=<MaxBackward1>) tensor(-2.3184, grad_fn=<MinBackward1>)\n",
      "Episode 321, Loss: -0.09093949943780899\n",
      "Gradient for fc1.weight: 0.09225030988454819\n",
      "Gradient for fc1.bias: 0.13602277636528015\n",
      "Gradient for fc2.weight: 0.2120060920715332\n",
      "Gradient for fc2.bias: 0.06227020546793938\n",
      "torch.Size([44]) tensor(-0.0024, grad_fn=<MaxBackward1>) tensor(-2.2937, grad_fn=<MinBackward1>)\n",
      "Episode 322, Loss: -0.023456184193491936\n",
      "Gradient for fc1.weight: 0.0862332433462143\n",
      "Gradient for fc1.bias: 0.14380422234535217\n",
      "Gradient for fc2.weight: 0.14476275444030762\n",
      "Gradient for fc2.bias: 0.06504245847463608\n",
      "torch.Size([19]) tensor(-0.0149, grad_fn=<MaxBackward1>) tensor(-2.6563, grad_fn=<MinBackward1>)\n",
      "Episode 323, Loss: 0.018047496676445007\n",
      "Gradient for fc1.weight: 0.10548091679811478\n",
      "Gradient for fc1.bias: 0.13888239860534668\n",
      "Gradient for fc2.weight: 0.2286701649427414\n",
      "Gradient for fc2.bias: 0.06647222489118576\n",
      "torch.Size([82]) tensor(-0.0016, grad_fn=<MaxBackward1>) tensor(-2.3990, grad_fn=<MinBackward1>)\n",
      "Episode 324, Loss: 0.00802007969468832\n",
      "Gradient for fc1.weight: 0.023832695558667183\n",
      "Gradient for fc1.bias: 0.11383186280727386\n",
      "Gradient for fc2.weight: 0.07930716127157211\n",
      "Gradient for fc2.bias: 0.04338623955845833\n",
      "torch.Size([51]) tensor(-0.0056, grad_fn=<MaxBackward1>) tensor(-2.4434, grad_fn=<MinBackward1>)\n",
      "Episode 325, Loss: -0.053661711513996124\n",
      "Gradient for fc1.weight: 0.0838107019662857\n",
      "Gradient for fc1.bias: 0.26214399933815\n",
      "Gradient for fc2.weight: 0.28809526562690735\n",
      "Gradient for fc2.bias: 0.1223139762878418\n",
      "torch.Size([47]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-2.4648, grad_fn=<MinBackward1>)\n",
      "Episode 326, Loss: -0.024457277730107307\n",
      "Gradient for fc1.weight: 0.10849183052778244\n",
      "Gradient for fc1.bias: 0.13968895375728607\n",
      "Gradient for fc2.weight: 0.12686261534690857\n",
      "Gradient for fc2.bias: 0.04296514019370079\n",
      "torch.Size([38]) tensor(-0.0017, grad_fn=<MaxBackward1>) tensor(-2.1853, grad_fn=<MinBackward1>)\n",
      "Episode 327, Loss: -0.10976974666118622\n",
      "Gradient for fc1.weight: 0.11138315498828888\n",
      "Gradient for fc1.bias: 0.1897888481616974\n",
      "Gradient for fc2.weight: 0.26739436388015747\n",
      "Gradient for fc2.bias: 0.08885958045721054\n",
      "torch.Size([29]) tensor(-0.0163, grad_fn=<MaxBackward1>) tensor(-2.6239, grad_fn=<MinBackward1>)\n",
      "Episode 328, Loss: -0.11916933953762054\n",
      "Gradient for fc1.weight: 0.11532267928123474\n",
      "Gradient for fc1.bias: 0.23771147429943085\n",
      "Gradient for fc2.weight: 0.3187731206417084\n",
      "Gradient for fc2.bias: 0.11260513216257095\n",
      "torch.Size([23]) tensor(-0.0029, grad_fn=<MaxBackward1>) tensor(-4.3451, grad_fn=<MinBackward1>)\n",
      "Episode 329, Loss: -0.466503381729126\n",
      "Gradient for fc1.weight: 0.446040540933609\n",
      "Gradient for fc1.bias: 0.4398943781852722\n",
      "Gradient for fc2.weight: 1.0163602828979492\n",
      "Gradient for fc2.bias: 0.23658569157123566\n",
      "torch.Size([51]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-3.8159, grad_fn=<MinBackward1>)\n",
      "Episode 330, Loss: -0.1009291410446167\n",
      "Gradient for fc1.weight: 0.0480911023914814\n",
      "Gradient for fc1.bias: 0.15896813571453094\n",
      "Gradient for fc2.weight: 0.15023712813854218\n",
      "Gradient for fc2.bias: 0.06806467473506927\n",
      "torch.Size([31]) tensor(-0.0038, grad_fn=<MaxBackward1>) tensor(-1.0312, grad_fn=<MinBackward1>)\n",
      "Episode 331, Loss: 0.030548276379704475\n",
      "Gradient for fc1.weight: 0.12343664467334747\n",
      "Gradient for fc1.bias: 0.12672430276870728\n",
      "Gradient for fc2.weight: 0.19777360558509827\n",
      "Gradient for fc2.bias: 0.04995322972536087\n",
      "torch.Size([56]) tensor(-0.0015, grad_fn=<MaxBackward1>) tensor(-3.0744, grad_fn=<MinBackward1>)\n",
      "Episode 332, Loss: 0.0016023088246583939\n",
      "Gradient for fc1.weight: 0.05245154723525047\n",
      "Gradient for fc1.bias: 0.06223250553011894\n",
      "Gradient for fc2.weight: 0.058026786893606186\n",
      "Gradient for fc2.bias: 0.0010022829519584775\n",
      "torch.Size([61]) tensor(-0.0023, grad_fn=<MaxBackward1>) tensor(-1.3633, grad_fn=<MinBackward1>)\n",
      "Episode 333, Loss: 0.0237059835344553\n",
      "Gradient for fc1.weight: 0.02474833093583584\n",
      "Gradient for fc1.bias: 0.053002774715423584\n",
      "Gradient for fc2.weight: 0.06168729439377785\n",
      "Gradient for fc2.bias: 0.023724954575300217\n",
      "torch.Size([66]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-2.5105, grad_fn=<MinBackward1>)\n",
      "Episode 334, Loss: -0.0352354422211647\n",
      "Gradient for fc1.weight: 0.03507442772388458\n",
      "Gradient for fc1.bias: 0.12340277433395386\n",
      "Gradient for fc2.weight: 0.11373738199472427\n",
      "Gradient for fc2.bias: 0.05406845733523369\n",
      "torch.Size([48]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-2.1714, grad_fn=<MinBackward1>)\n",
      "Episode 335, Loss: -0.09509193152189255\n",
      "Gradient for fc1.weight: 0.029266847297549248\n",
      "Gradient for fc1.bias: 0.07389035075902939\n",
      "Gradient for fc2.weight: 0.061746202409267426\n",
      "Gradient for fc2.bias: 0.027092885226011276\n",
      "torch.Size([35]) tensor(-0.0024, grad_fn=<MaxBackward1>) tensor(-3.0485, grad_fn=<MinBackward1>)\n",
      "Episode 336, Loss: 0.0010631777113303542\n",
      "Gradient for fc1.weight: 0.019933044910430908\n",
      "Gradient for fc1.bias: 0.13077370822429657\n",
      "Gradient for fc2.weight: 0.11739441752433777\n",
      "Gradient for fc2.bias: 0.05587419867515564\n",
      "torch.Size([38]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-1.5615, grad_fn=<MinBackward1>)\n",
      "Episode 337, Loss: -0.02029513753950596\n",
      "Gradient for fc1.weight: 0.06472037732601166\n",
      "Gradient for fc1.bias: 0.1823253184556961\n",
      "Gradient for fc2.weight: 0.1759641021490097\n",
      "Gradient for fc2.bias: 0.07848504930734634\n",
      "torch.Size([52]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.1939, grad_fn=<MinBackward1>)\n",
      "Episode 338, Loss: 0.015127713792026043\n",
      "Gradient for fc1.weight: 0.031352147459983826\n",
      "Gradient for fc1.bias: 0.20905116200447083\n",
      "Gradient for fc2.weight: 0.16695469617843628\n",
      "Gradient for fc2.bias: 0.08344236016273499\n",
      "torch.Size([27]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-2.7894, grad_fn=<MinBackward1>)\n",
      "Episode 339, Loss: -0.05347529053688049\n",
      "Gradient for fc1.weight: 0.09316049516201019\n",
      "Gradient for fc1.bias: 0.2682998776435852\n",
      "Gradient for fc2.weight: 0.23405691981315613\n",
      "Gradient for fc2.bias: 0.10621413588523865\n",
      "torch.Size([76]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.2765, grad_fn=<MinBackward1>)\n",
      "Episode 340, Loss: 0.060178905725479126\n",
      "Gradient for fc1.weight: 0.02473856508731842\n",
      "Gradient for fc1.bias: 0.027474068105220795\n",
      "Gradient for fc2.weight: 0.028499165549874306\n",
      "Gradient for fc2.bias: 0.009905901737511158\n",
      "torch.Size([53]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-4.0259, grad_fn=<MinBackward1>)\n",
      "Episode 341, Loss: -0.019502973183989525\n",
      "Gradient for fc1.weight: 0.028224671259522438\n",
      "Gradient for fc1.bias: 0.08328884094953537\n",
      "Gradient for fc2.weight: 0.056116458028554916\n",
      "Gradient for fc2.bias: 0.03236028179526329\n",
      "torch.Size([41]) tensor(-0.0012, grad_fn=<MaxBackward1>) tensor(-2.2767, grad_fn=<MinBackward1>)\n",
      "Episode 342, Loss: -0.06260759383440018\n",
      "Gradient for fc1.weight: 0.024254953488707542\n",
      "Gradient for fc1.bias: 0.057606834918260574\n",
      "Gradient for fc2.weight: 0.06705281138420105\n",
      "Gradient for fc2.bias: 0.02257463149726391\n",
      "torch.Size([56]) tensor(-0.0012, grad_fn=<MaxBackward1>) tensor(-1.9390, grad_fn=<MinBackward1>)\n",
      "Episode 343, Loss: 0.010695131495594978\n",
      "Gradient for fc1.weight: 0.05098653957247734\n",
      "Gradient for fc1.bias: 0.1698189079761505\n",
      "Gradient for fc2.weight: 0.15115255117416382\n",
      "Gradient for fc2.bias: 0.0690731629729271\n",
      "torch.Size([46]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.4411, grad_fn=<MinBackward1>)\n",
      "Episode 344, Loss: 0.020407525822520256\n",
      "Gradient for fc1.weight: 0.014756109565496445\n",
      "Gradient for fc1.bias: 0.08868108689785004\n",
      "Gradient for fc2.weight: 0.06596122682094574\n",
      "Gradient for fc2.bias: 0.03342604264616966\n",
      "torch.Size([52]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.0436, grad_fn=<MinBackward1>)\n",
      "Episode 345, Loss: -0.011649670079350471\n",
      "Gradient for fc1.weight: 0.024273427203297615\n",
      "Gradient for fc1.bias: 0.1601981222629547\n",
      "Gradient for fc2.weight: 0.1301393359899521\n",
      "Gradient for fc2.bias: 0.06359823048114777\n",
      "torch.Size([55]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.2723, grad_fn=<MinBackward1>)\n",
      "Episode 346, Loss: 0.03851059451699257\n",
      "Gradient for fc1.weight: 0.04770330339670181\n",
      "Gradient for fc1.bias: 0.11344302445650101\n",
      "Gradient for fc2.weight: 0.09878631681203842\n",
      "Gradient for fc2.bias: 0.04614232853055\n",
      "torch.Size([41]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-1.2627, grad_fn=<MinBackward1>)\n",
      "Episode 347, Loss: 0.06447286158800125\n",
      "Gradient for fc1.weight: 0.03412562981247902\n",
      "Gradient for fc1.bias: 0.2081095427274704\n",
      "Gradient for fc2.weight: 0.17374198138713837\n",
      "Gradient for fc2.bias: 0.08334062993526459\n",
      "torch.Size([47]) tensor(-0.0017, grad_fn=<MaxBackward1>) tensor(-1.8160, grad_fn=<MinBackward1>)\n",
      "Episode 348, Loss: 0.07280515879392624\n",
      "Gradient for fc1.weight: 0.08125850558280945\n",
      "Gradient for fc1.bias: 0.25850585103034973\n",
      "Gradient for fc2.weight: 0.23408010601997375\n",
      "Gradient for fc2.bias: 0.10476303845643997\n",
      "torch.Size([38]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.8672, grad_fn=<MinBackward1>)\n",
      "Episode 349, Loss: -0.04203159734606743\n",
      "Gradient for fc1.weight: 0.023357482627034187\n",
      "Gradient for fc1.bias: 0.031767766922712326\n",
      "Gradient for fc2.weight: 0.027624474838376045\n",
      "Gradient for fc2.bias: 0.006073839962482452\n",
      "torch.Size([50]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-1.2041, grad_fn=<MinBackward1>)\n",
      "Episode 350, Loss: 0.0626460611820221\n",
      "Gradient for fc1.weight: 0.026321114972233772\n",
      "Gradient for fc1.bias: 0.14163698256015778\n",
      "Gradient for fc2.weight: 0.12130524963140488\n",
      "Gradient for fc2.bias: 0.05699196457862854\n",
      "torch.Size([61]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.3803, grad_fn=<MinBackward1>)\n",
      "Episode 351, Loss: 0.014202171005308628\n",
      "Gradient for fc1.weight: 0.02675621584057808\n",
      "Gradient for fc1.bias: 0.1733594536781311\n",
      "Gradient for fc2.weight: 0.12878048419952393\n",
      "Gradient for fc2.bias: 0.06469236314296722\n",
      "torch.Size([37]) tensor(-0.0012, grad_fn=<MaxBackward1>) tensor(-1.7249, grad_fn=<MinBackward1>)\n",
      "Episode 352, Loss: 0.0807819738984108\n",
      "Gradient for fc1.weight: 0.022722259163856506\n",
      "Gradient for fc1.bias: 0.1663305014371872\n",
      "Gradient for fc2.weight: 0.10810021311044693\n",
      "Gradient for fc2.bias: 0.05649074912071228\n",
      "torch.Size([25]) tensor(-0.0014, grad_fn=<MaxBackward1>) tensor(-1.1990, grad_fn=<MinBackward1>)\n",
      "Episode 353, Loss: 0.050949886441230774\n",
      "Gradient for fc1.weight: 0.05298486724495888\n",
      "Gradient for fc1.bias: 0.19504694640636444\n",
      "Gradient for fc2.weight: 0.16851858794689178\n",
      "Gradient for fc2.bias: 0.07760598510503769\n",
      "torch.Size([38]) tensor(-9.3345e-05, grad_fn=<MaxBackward1>) tensor(-3.4209, grad_fn=<MinBackward1>)\n",
      "Episode 354, Loss: -0.030802592635154724\n",
      "Gradient for fc1.weight: 0.1445407271385193\n",
      "Gradient for fc1.bias: 0.28647279739379883\n",
      "Gradient for fc2.weight: 0.24501949548721313\n",
      "Gradient for fc2.bias: 0.10935208201408386\n",
      "torch.Size([28]) tensor(-0.0024, grad_fn=<MaxBackward1>) tensor(-1.3231, grad_fn=<MinBackward1>)\n",
      "Episode 355, Loss: -0.11737168580293655\n",
      "Gradient for fc1.weight: 0.028345072641968727\n",
      "Gradient for fc1.bias: 0.10930780321359634\n",
      "Gradient for fc2.weight: 0.05859185755252838\n",
      "Gradient for fc2.bias: 0.03114381432533264\n",
      "torch.Size([39]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-0.9657, grad_fn=<MinBackward1>)\n",
      "Episode 356, Loss: 0.09063435345888138\n",
      "Gradient for fc1.weight: 0.05505641549825668\n",
      "Gradient for fc1.bias: 0.12089676409959793\n",
      "Gradient for fc2.weight: 0.11261066049337387\n",
      "Gradient for fc2.bias: 0.04983009025454521\n",
      "torch.Size([30]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.9037, grad_fn=<MinBackward1>)\n",
      "Episode 357, Loss: -0.03150146082043648\n",
      "Gradient for fc1.weight: 0.07411892712116241\n",
      "Gradient for fc1.bias: 0.07564252614974976\n",
      "Gradient for fc2.weight: 0.10500728338956833\n",
      "Gradient for fc2.bias: 0.030494147911667824\n",
      "torch.Size([60]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.7597, grad_fn=<MinBackward1>)\n",
      "Episode 358, Loss: 0.0019412507535889745\n",
      "Gradient for fc1.weight: 0.09221652150154114\n",
      "Gradient for fc1.bias: 0.2344035506248474\n",
      "Gradient for fc2.weight: 0.19728320837020874\n",
      "Gradient for fc2.bias: 0.09088058769702911\n",
      "torch.Size([32]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.9862, grad_fn=<MinBackward1>)\n",
      "Episode 359, Loss: -0.10103391110897064\n",
      "Gradient for fc1.weight: 0.05233611539006233\n",
      "Gradient for fc1.bias: 0.06488154828548431\n",
      "Gradient for fc2.weight: 0.08054530620574951\n",
      "Gradient for fc2.bias: 0.025579312816262245\n",
      "torch.Size([40]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-3.3949, grad_fn=<MinBackward1>)\n",
      "Episode 360, Loss: -0.04216330498456955\n",
      "Gradient for fc1.weight: 0.029455142095685005\n",
      "Gradient for fc1.bias: 0.10554486513137817\n",
      "Gradient for fc2.weight: 0.09246571362018585\n",
      "Gradient for fc2.bias: 0.04140881448984146\n",
      "torch.Size([26]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-1.4447, grad_fn=<MinBackward1>)\n",
      "Episode 361, Loss: -0.007446626666933298\n",
      "Gradient for fc1.weight: 0.04429233446717262\n",
      "Gradient for fc1.bias: 0.14401912689208984\n",
      "Gradient for fc2.weight: 0.11404933035373688\n",
      "Gradient for fc2.bias: 0.053655192255973816\n",
      "torch.Size([57]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-1.7022, grad_fn=<MinBackward1>)\n",
      "Episode 362, Loss: -0.05850110203027725\n",
      "Gradient for fc1.weight: 0.0222326572984457\n",
      "Gradient for fc1.bias: 0.040627364069223404\n",
      "Gradient for fc2.weight: 0.029344700276851654\n",
      "Gradient for fc2.bias: 0.009875916875898838\n",
      "torch.Size([45]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.2144, grad_fn=<MinBackward1>)\n",
      "Episode 363, Loss: -0.061441559344530106\n",
      "Gradient for fc1.weight: 0.01755012944340706\n",
      "Gradient for fc1.bias: 0.07537326961755753\n",
      "Gradient for fc2.weight: 0.05409467592835426\n",
      "Gradient for fc2.bias: 0.02574840746819973\n",
      "torch.Size([46]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-3.1943, grad_fn=<MinBackward1>)\n",
      "Episode 364, Loss: -0.03427073732018471\n",
      "Gradient for fc1.weight: 0.0249372236430645\n",
      "Gradient for fc1.bias: 0.04455256089568138\n",
      "Gradient for fc2.weight: 0.03850534185767174\n",
      "Gradient for fc2.bias: 0.015865355730056763\n",
      "torch.Size([38]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.2733, grad_fn=<MinBackward1>)\n",
      "Episode 365, Loss: -0.05776824429631233\n",
      "Gradient for fc1.weight: 0.02473605051636696\n",
      "Gradient for fc1.bias: 0.06757330894470215\n",
      "Gradient for fc2.weight: 0.027412470430135727\n",
      "Gradient for fc2.bias: 0.015061271376907825\n",
      "torch.Size([43]) tensor(-0.0014, grad_fn=<MaxBackward1>) tensor(-1.6373, grad_fn=<MinBackward1>)\n",
      "Episode 366, Loss: -0.138885036110878\n",
      "Gradient for fc1.weight: 0.08225837349891663\n",
      "Gradient for fc1.bias: 0.12318418174982071\n",
      "Gradient for fc2.weight: 0.13845372200012207\n",
      "Gradient for fc2.bias: 0.04918596148490906\n",
      "torch.Size([35]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-1.7283, grad_fn=<MinBackward1>)\n",
      "Episode 367, Loss: 0.034904565662145615\n",
      "Gradient for fc1.weight: 0.011344583705067635\n",
      "Gradient for fc1.bias: 0.024989934638142586\n",
      "Gradient for fc2.weight: 0.012348629534244537\n",
      "Gradient for fc2.bias: 0.005975228734314442\n",
      "torch.Size([110]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.1813, grad_fn=<MinBackward1>)\n",
      "Episode 368, Loss: -0.020192021504044533\n",
      "Gradient for fc1.weight: 0.016569407656788826\n",
      "Gradient for fc1.bias: 0.030080441385507584\n",
      "Gradient for fc2.weight: 0.01629510708153248\n",
      "Gradient for fc2.bias: 0.005972438026219606\n",
      "torch.Size([50]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.0696, grad_fn=<MinBackward1>)\n",
      "Episode 369, Loss: 0.037649769335985184\n",
      "Gradient for fc1.weight: 0.03514571860432625\n",
      "Gradient for fc1.bias: 0.09160745143890381\n",
      "Gradient for fc2.weight: 0.0720650926232338\n",
      "Gradient for fc2.bias: 0.033497195690870285\n",
      "torch.Size([46]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-1.7192, grad_fn=<MinBackward1>)\n",
      "Episode 370, Loss: 0.06779088079929352\n",
      "Gradient for fc1.weight: 0.08290912210941315\n",
      "Gradient for fc1.bias: 0.07139439135789871\n",
      "Gradient for fc2.weight: 0.0813298299908638\n",
      "Gradient for fc2.bias: 0.023440834134817123\n",
      "torch.Size([30]) tensor(-0.0021, grad_fn=<MaxBackward1>) tensor(-1.4468, grad_fn=<MinBackward1>)\n",
      "Episode 371, Loss: -0.06009690836071968\n",
      "Gradient for fc1.weight: 0.012439370155334473\n",
      "Gradient for fc1.bias: 0.0727531760931015\n",
      "Gradient for fc2.weight: 0.05099266394972801\n",
      "Gradient for fc2.bias: 0.024175062775611877\n",
      "torch.Size([64]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.1749, grad_fn=<MinBackward1>)\n",
      "Episode 372, Loss: -0.062049705535173416\n",
      "Gradient for fc1.weight: 0.016615496948361397\n",
      "Gradient for fc1.bias: 0.07438299804925919\n",
      "Gradient for fc2.weight: 0.061695028096437454\n",
      "Gradient for fc2.bias: 0.02895953133702278\n",
      "torch.Size([43]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.9268, grad_fn=<MinBackward1>)\n",
      "Episode 373, Loss: -0.140011265873909\n",
      "Gradient for fc1.weight: 0.03712509199976921\n",
      "Gradient for fc1.bias: 0.0924779623746872\n",
      "Gradient for fc2.weight: 0.04721841216087341\n",
      "Gradient for fc2.bias: 0.017436513677239418\n",
      "torch.Size([42]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.0147, grad_fn=<MinBackward1>)\n",
      "Episode 374, Loss: 0.14924804866313934\n",
      "Gradient for fc1.weight: 0.024501407518982887\n",
      "Gradient for fc1.bias: 0.137304425239563\n",
      "Gradient for fc2.weight: 0.08955924212932587\n",
      "Gradient for fc2.bias: 0.040973782539367676\n",
      "torch.Size([67]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.5650, grad_fn=<MinBackward1>)\n",
      "Episode 375, Loss: 0.04857487231492996\n",
      "Gradient for fc1.weight: 0.0076535786502063274\n",
      "Gradient for fc1.bias: 0.0435352697968483\n",
      "Gradient for fc2.weight: 0.02434891276061535\n",
      "Gradient for fc2.bias: 0.01215664204210043\n",
      "torch.Size([37]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.3445, grad_fn=<MinBackward1>)\n",
      "Episode 376, Loss: -0.09996522217988968\n",
      "Gradient for fc1.weight: 0.09065090119838715\n",
      "Gradient for fc1.bias: 0.15517139434814453\n",
      "Gradient for fc2.weight: 0.15613310039043427\n",
      "Gradient for fc2.bias: 0.061471402645111084\n",
      "torch.Size([51]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.6818, grad_fn=<MinBackward1>)\n",
      "Episode 377, Loss: 0.026929059997200966\n",
      "Gradient for fc1.weight: 0.03816697746515274\n",
      "Gradient for fc1.bias: 0.2619743347167969\n",
      "Gradient for fc2.weight: 0.19304345548152924\n",
      "Gradient for fc2.bias: 0.09493845701217651\n",
      "torch.Size([48]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-2.3308, grad_fn=<MinBackward1>)\n",
      "Episode 378, Loss: -0.16908018290996552\n",
      "Gradient for fc1.weight: 0.05119260773062706\n",
      "Gradient for fc1.bias: 0.0908668264746666\n",
      "Gradient for fc2.weight: 0.11051822453737259\n",
      "Gradient for fc2.bias: 0.03863215819001198\n",
      "torch.Size([33]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-3.4460, grad_fn=<MinBackward1>)\n",
      "Episode 379, Loss: 0.11795400083065033\n",
      "Gradient for fc1.weight: 0.06505677103996277\n",
      "Gradient for fc1.bias: 0.20325714349746704\n",
      "Gradient for fc2.weight: 0.18183104693889618\n",
      "Gradient for fc2.bias: 0.07451663911342621\n",
      "torch.Size([23]) tensor(-0.0058, grad_fn=<MaxBackward1>) tensor(-0.8761, grad_fn=<MinBackward1>)\n",
      "Episode 380, Loss: -0.10744733363389969\n",
      "Gradient for fc1.weight: 0.08151561766862869\n",
      "Gradient for fc1.bias: 0.17219756543636322\n",
      "Gradient for fc2.weight: 0.14584219455718994\n",
      "Gradient for fc2.bias: 0.061937592923641205\n",
      "torch.Size([30]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-1.9852, grad_fn=<MinBackward1>)\n",
      "Episode 381, Loss: -0.06530741602182388\n",
      "Gradient for fc1.weight: 0.12857213616371155\n",
      "Gradient for fc1.bias: 0.21959161758422852\n",
      "Gradient for fc2.weight: 0.22156792879104614\n",
      "Gradient for fc2.bias: 0.08679134398698807\n",
      "torch.Size([47]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-4.4450, grad_fn=<MinBackward1>)\n",
      "Episode 382, Loss: 0.1039503887295723\n",
      "Gradient for fc1.weight: 0.06590816378593445\n",
      "Gradient for fc1.bias: 0.06191495805978775\n",
      "Gradient for fc2.weight: 0.0607699379324913\n",
      "Gradient for fc2.bias: 0.0025631983298808336\n",
      "torch.Size([34]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-3.9457, grad_fn=<MinBackward1>)\n",
      "Episode 383, Loss: -0.18483813107013702\n",
      "Gradient for fc1.weight: 0.09423854202032089\n",
      "Gradient for fc1.bias: 0.16844776272773743\n",
      "Gradient for fc2.weight: 0.16172024607658386\n",
      "Gradient for fc2.bias: 0.06385831534862518\n",
      "torch.Size([47]) tensor(-5.1142e-05, grad_fn=<MaxBackward1>) tensor(-3.6210, grad_fn=<MinBackward1>)\n",
      "Episode 384, Loss: 0.02301044575870037\n",
      "Gradient for fc1.weight: 0.026327740401029587\n",
      "Gradient for fc1.bias: 0.09224896878004074\n",
      "Gradient for fc2.weight: 0.06787626445293427\n",
      "Gradient for fc2.bias: 0.03266453742980957\n",
      "torch.Size([45]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.1550, grad_fn=<MinBackward1>)\n",
      "Episode 385, Loss: -0.10161622613668442\n",
      "Gradient for fc1.weight: 0.13212783634662628\n",
      "Gradient for fc1.bias: 0.19089367985725403\n",
      "Gradient for fc2.weight: 0.22662119567394257\n",
      "Gradient for fc2.bias: 0.07906123995780945\n",
      "torch.Size([42]) tensor(-0.0020, grad_fn=<MaxBackward1>) tensor(-1.5611, grad_fn=<MinBackward1>)\n",
      "Episode 386, Loss: -0.10992394387722015\n",
      "Gradient for fc1.weight: 0.04113940894603729\n",
      "Gradient for fc1.bias: 0.053915347903966904\n",
      "Gradient for fc2.weight: 0.055666159838438034\n",
      "Gradient for fc2.bias: 0.012708857655525208\n",
      "torch.Size([37]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-2.9901, grad_fn=<MinBackward1>)\n",
      "Episode 387, Loss: -0.14046768844127655\n",
      "Gradient for fc1.weight: 0.022834427654743195\n",
      "Gradient for fc1.bias: 0.05045175924897194\n",
      "Gradient for fc2.weight: 0.02826508693397045\n",
      "Gradient for fc2.bias: 0.005399813409894705\n",
      "torch.Size([44]) tensor(-6.0203e-05, grad_fn=<MaxBackward1>) tensor(-3.4848, grad_fn=<MinBackward1>)\n",
      "Episode 388, Loss: -0.03543310984969139\n",
      "Gradient for fc1.weight: 0.02707396075129509\n",
      "Gradient for fc1.bias: 0.10023709386587143\n",
      "Gradient for fc2.weight: 0.07454990595579147\n",
      "Gradient for fc2.bias: 0.03593790531158447\n",
      "torch.Size([23]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-3.5472, grad_fn=<MinBackward1>)\n",
      "Episode 389, Loss: -0.2091604471206665\n",
      "Gradient for fc1.weight: 0.05668314918875694\n",
      "Gradient for fc1.bias: 0.10666699707508087\n",
      "Gradient for fc2.weight: 0.12989471852779388\n",
      "Gradient for fc2.bias: 0.042207974940538406\n",
      "torch.Size([42]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.5558, grad_fn=<MinBackward1>)\n",
      "Episode 390, Loss: -0.1449236124753952\n",
      "Gradient for fc1.weight: 0.16088636219501495\n",
      "Gradient for fc1.bias: 0.3197971284389496\n",
      "Gradient for fc2.weight: 0.2925971746444702\n",
      "Gradient for fc2.bias: 0.1210208609700203\n",
      "torch.Size([55]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.7741, grad_fn=<MinBackward1>)\n",
      "Episode 391, Loss: -0.11655154824256897\n",
      "Gradient for fc1.weight: 0.09781411290168762\n",
      "Gradient for fc1.bias: 0.20119744539260864\n",
      "Gradient for fc2.weight: 0.179161936044693\n",
      "Gradient for fc2.bias: 0.07394607365131378\n",
      "torch.Size([26]) tensor(-0.0028, grad_fn=<MaxBackward1>) tensor(-0.6005, grad_fn=<MinBackward1>)\n",
      "Episode 392, Loss: -0.057905908674001694\n",
      "Gradient for fc1.weight: 0.11771665513515472\n",
      "Gradient for fc1.bias: 0.23648200929164886\n",
      "Gradient for fc2.weight: 0.22314633429050446\n",
      "Gradient for fc2.bias: 0.09105779975652695\n",
      "torch.Size([29]) tensor(-0.0019, grad_fn=<MaxBackward1>) tensor(-2.1423, grad_fn=<MinBackward1>)\n",
      "Episode 393, Loss: 0.09134368598461151\n",
      "Gradient for fc1.weight: 0.05044933035969734\n",
      "Gradient for fc1.bias: 0.15611349046230316\n",
      "Gradient for fc2.weight: 0.13042451441287994\n",
      "Gradient for fc2.bias: 0.05364931374788284\n",
      "torch.Size([43]) tensor(-6.8548e-05, grad_fn=<MaxBackward1>) tensor(-3.3495, grad_fn=<MinBackward1>)\n",
      "Episode 394, Loss: 0.01750168390572071\n",
      "Gradient for fc1.weight: 0.07047019898891449\n",
      "Gradient for fc1.bias: 0.07731020450592041\n",
      "Gradient for fc2.weight: 0.07068183273077011\n",
      "Gradient for fc2.bias: 0.007798250764608383\n",
      "torch.Size([39]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-2.2300, grad_fn=<MinBackward1>)\n",
      "Episode 395, Loss: 0.040819864720106125\n",
      "Gradient for fc1.weight: 0.0905359536409378\n",
      "Gradient for fc1.bias: 0.22171902656555176\n",
      "Gradient for fc2.weight: 0.2064671665430069\n",
      "Gradient for fc2.bias: 0.08461354672908783\n",
      "torch.Size([23]) tensor(-0.0020, grad_fn=<MaxBackward1>) tensor(-2.9581, grad_fn=<MinBackward1>)\n",
      "Episode 396, Loss: -0.18285273015499115\n",
      "Gradient for fc1.weight: 0.13747911155223846\n",
      "Gradient for fc1.bias: 0.17443345487117767\n",
      "Gradient for fc2.weight: 0.1785683035850525\n",
      "Gradient for fc2.bias: 0.05922121927142143\n",
      "torch.Size([56]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.1810, grad_fn=<MinBackward1>)\n",
      "Episode 397, Loss: -0.037713322788476944\n",
      "Gradient for fc1.weight: 0.013534151948988438\n",
      "Gradient for fc1.bias: 0.03133736178278923\n",
      "Gradient for fc2.weight: 0.019432885572314262\n",
      "Gradient for fc2.bias: 0.007812642492353916\n",
      "torch.Size([55]) tensor(-3.1591e-05, grad_fn=<MaxBackward1>) tensor(-4.5304, grad_fn=<MinBackward1>)\n",
      "Episode 398, Loss: -0.036156199872493744\n",
      "Gradient for fc1.weight: 0.10332522541284561\n",
      "Gradient for fc1.bias: 0.1937963217496872\n",
      "Gradient for fc2.weight: 0.2044493854045868\n",
      "Gradient for fc2.bias: 0.07533657550811768\n",
      "torch.Size([34]) tensor(-0.0030, grad_fn=<MaxBackward1>) tensor(-2.2989, grad_fn=<MinBackward1>)\n",
      "Episode 399, Loss: 0.004243835806846619\n",
      "Gradient for fc1.weight: 0.03249133378267288\n",
      "Gradient for fc1.bias: 0.09881375730037689\n",
      "Gradient for fc2.weight: 0.07039244472980499\n",
      "Gradient for fc2.bias: 0.033078256994485855\n",
      "torch.Size([26]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-3.2302, grad_fn=<MinBackward1>)\n",
      "Episode 400, Loss: -0.07387831062078476\n",
      "Gradient for fc1.weight: 0.0431058295071125\n",
      "Gradient for fc1.bias: 0.05808104947209358\n",
      "Gradient for fc2.weight: 0.03522089496254921\n",
      "Gradient for fc2.bias: 0.00427949707955122\n",
      "Running reward: 32.86284079759431\n",
      "torch.Size([18]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-1.0412, grad_fn=<MinBackward1>)\n",
      "Episode 401, Loss: -0.04015776887536049\n",
      "Gradient for fc1.weight: 0.08569467812776566\n",
      "Gradient for fc1.bias: 0.06589295715093613\n",
      "Gradient for fc2.weight: 0.08341123163700104\n",
      "Gradient for fc2.bias: 0.015951523557305336\n",
      "torch.Size([19]) tensor(-0.0028, grad_fn=<MaxBackward1>) tensor(-0.7872, grad_fn=<MinBackward1>)\n",
      "Episode 402, Loss: 0.0018261419609189034\n",
      "Gradient for fc1.weight: 0.08051761239767075\n",
      "Gradient for fc1.bias: 0.13894985616207123\n",
      "Gradient for fc2.weight: 0.13758990168571472\n",
      "Gradient for fc2.bias: 0.049598466604948044\n",
      "torch.Size([16]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.5763, grad_fn=<MinBackward1>)\n",
      "Episode 403, Loss: -0.02273964136838913\n",
      "Gradient for fc1.weight: 0.1018819808959961\n",
      "Gradient for fc1.bias: 0.11391264200210571\n",
      "Gradient for fc2.weight: 0.1257140040397644\n",
      "Gradient for fc2.bias: 0.03754396736621857\n",
      "torch.Size([46]) tensor(-0.0026, grad_fn=<MaxBackward1>) tensor(-1.2910, grad_fn=<MinBackward1>)\n",
      "Episode 404, Loss: 0.03717435896396637\n",
      "Gradient for fc1.weight: 0.09076551347970963\n",
      "Gradient for fc1.bias: 0.1377229243516922\n",
      "Gradient for fc2.weight: 0.15233692526817322\n",
      "Gradient for fc2.bias: 0.05132859945297241\n",
      "torch.Size([20]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.6913, grad_fn=<MinBackward1>)\n",
      "Episode 405, Loss: 0.05385243892669678\n",
      "Gradient for fc1.weight: 0.16697674989700317\n",
      "Gradient for fc1.bias: 0.13604609668254852\n",
      "Gradient for fc2.weight: 0.1749875843524933\n",
      "Gradient for fc2.bias: 0.026169266551733017\n",
      "torch.Size([18]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-1.4589, grad_fn=<MinBackward1>)\n",
      "Episode 406, Loss: 0.07229587435722351\n",
      "Gradient for fc1.weight: 0.1479368507862091\n",
      "Gradient for fc1.bias: 0.2905682325363159\n",
      "Gradient for fc2.weight: 0.2622455954551697\n",
      "Gradient for fc2.bias: 0.1025860384106636\n",
      "torch.Size([138]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.0039, grad_fn=<MinBackward1>)\n",
      "Episode 407, Loss: -0.025906050577759743\n",
      "Gradient for fc1.weight: 0.02872789092361927\n",
      "Gradient for fc1.bias: 0.10546282678842545\n",
      "Gradient for fc2.weight: 0.10668762773275375\n",
      "Gradient for fc2.bias: 0.04633348435163498\n",
      "torch.Size([28]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-2.8862, grad_fn=<MinBackward1>)\n",
      "Episode 408, Loss: 0.04190519452095032\n",
      "Gradient for fc1.weight: 0.17248883843421936\n",
      "Gradient for fc1.bias: 0.24633266031742096\n",
      "Gradient for fc2.weight: 0.2912532687187195\n",
      "Gradient for fc2.bias: 0.0948471873998642\n",
      "torch.Size([22]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-3.9054, grad_fn=<MinBackward1>)\n",
      "Episode 409, Loss: 0.1421842724084854\n",
      "Gradient for fc1.weight: 0.12300628423690796\n",
      "Gradient for fc1.bias: 0.13802863657474518\n",
      "Gradient for fc2.weight: 0.12492381036281586\n",
      "Gradient for fc2.bias: 0.03402159363031387\n",
      "torch.Size([17]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-2.2748, grad_fn=<MinBackward1>)\n",
      "Episode 410, Loss: -0.33619651198387146\n",
      "Gradient for fc1.weight: 0.5567699670791626\n",
      "Gradient for fc1.bias: 0.5113787651062012\n",
      "Gradient for fc2.weight: 0.7615674734115601\n",
      "Gradient for fc2.bias: 0.18955613672733307\n",
      "torch.Size([18]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.4200, grad_fn=<MinBackward1>)\n",
      "Episode 411, Loss: 0.03898436576128006\n",
      "Gradient for fc1.weight: 0.062150269746780396\n",
      "Gradient for fc1.bias: 0.09161514043807983\n",
      "Gradient for fc2.weight: 0.10780471563339233\n",
      "Gradient for fc2.bias: 0.03181593492627144\n",
      "torch.Size([32]) tensor(-0.0063, grad_fn=<MaxBackward1>) tensor(-1.4142, grad_fn=<MinBackward1>)\n",
      "Episode 412, Loss: 0.04233819991350174\n",
      "Gradient for fc1.weight: 0.16570785641670227\n",
      "Gradient for fc1.bias: 0.21733824908733368\n",
      "Gradient for fc2.weight: 0.25952276587486267\n",
      "Gradient for fc2.bias: 0.08197527378797531\n",
      "torch.Size([21]) tensor(-0.0036, grad_fn=<MaxBackward1>) tensor(-0.8663, grad_fn=<MinBackward1>)\n",
      "Episode 413, Loss: -0.032062139362096786\n",
      "Gradient for fc1.weight: 0.26340538263320923\n",
      "Gradient for fc1.bias: 0.44211921095848083\n",
      "Gradient for fc2.weight: 0.4503026306629181\n",
      "Gradient for fc2.bias: 0.16259101033210754\n",
      "torch.Size([26]) tensor(-0.0021, grad_fn=<MaxBackward1>) tensor(-0.8213, grad_fn=<MinBackward1>)\n",
      "Episode 414, Loss: 0.0008100683917291462\n",
      "Gradient for fc1.weight: 0.20956844091415405\n",
      "Gradient for fc1.bias: 0.44894152879714966\n",
      "Gradient for fc2.weight: 0.41155487298965454\n",
      "Gradient for fc2.bias: 0.1627451628446579\n",
      "torch.Size([18]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-3.4153, grad_fn=<MinBackward1>)\n",
      "Episode 415, Loss: 0.1635555624961853\n",
      "Gradient for fc1.weight: 0.4335693120956421\n",
      "Gradient for fc1.bias: 0.6334894299507141\n",
      "Gradient for fc2.weight: 0.773639976978302\n",
      "Gradient for fc2.bias: 0.2598154544830322\n",
      "torch.Size([22]) tensor(-0.0020, grad_fn=<MaxBackward1>) tensor(-1.0301, grad_fn=<MinBackward1>)\n",
      "Episode 416, Loss: -0.020952651277184486\n",
      "Gradient for fc1.weight: 0.08066911995410919\n",
      "Gradient for fc1.bias: 0.27188840508461\n",
      "Gradient for fc2.weight: 0.20724879205226898\n",
      "Gradient for fc2.bias: 0.09521480649709702\n",
      "torch.Size([21]) tensor(-0.0027, grad_fn=<MaxBackward1>) tensor(-0.9552, grad_fn=<MinBackward1>)\n",
      "Episode 417, Loss: -0.0003566103405319154\n",
      "Gradient for fc1.weight: 0.04866277053952217\n",
      "Gradient for fc1.bias: 0.1215134933590889\n",
      "Gradient for fc2.weight: 0.06412621587514877\n",
      "Gradient for fc2.bias: 0.035480063408613205\n",
      "torch.Size([26]) tensor(-0.0034, grad_fn=<MaxBackward1>) tensor(-2.3930, grad_fn=<MinBackward1>)\n",
      "Episode 418, Loss: -0.08462183177471161\n",
      "Gradient for fc1.weight: 0.08782150596380234\n",
      "Gradient for fc1.bias: 0.18024198710918427\n",
      "Gradient for fc2.weight: 0.17666314542293549\n",
      "Gradient for fc2.bias: 0.06856079399585724\n",
      "torch.Size([30]) tensor(-3.1472e-05, grad_fn=<MaxBackward1>) tensor(-4.5278, grad_fn=<MinBackward1>)\n",
      "Episode 419, Loss: 0.12834607064723969\n",
      "Gradient for fc1.weight: 0.047701358795166016\n",
      "Gradient for fc1.bias: 0.29975175857543945\n",
      "Gradient for fc2.weight: 0.2204369157552719\n",
      "Gradient for fc2.bias: 0.10807497054338455\n",
      "torch.Size([34]) tensor(-0.0020, grad_fn=<MaxBackward1>) tensor(-1.3409, grad_fn=<MinBackward1>)\n",
      "Episode 420, Loss: -0.01840604655444622\n",
      "Gradient for fc1.weight: 0.10987425595521927\n",
      "Gradient for fc1.bias: 0.089565210044384\n",
      "Gradient for fc2.weight: 0.08189951628446579\n",
      "Gradient for fc2.bias: 0.0035612047649919987\n",
      "torch.Size([59]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.3818, grad_fn=<MinBackward1>)\n",
      "Episode 421, Loss: -0.14497198164463043\n",
      "Gradient for fc1.weight: 0.06555145233869553\n",
      "Gradient for fc1.bias: 0.12210861593484879\n",
      "Gradient for fc2.weight: 0.13371630012989044\n",
      "Gradient for fc2.bias: 0.048901475965976715\n",
      "torch.Size([29]) tensor(-0.0028, grad_fn=<MaxBackward1>) tensor(-2.4106, grad_fn=<MinBackward1>)\n",
      "Episode 422, Loss: -0.07781211286783218\n",
      "Gradient for fc1.weight: 0.12594008445739746\n",
      "Gradient for fc1.bias: 0.20742197334766388\n",
      "Gradient for fc2.weight: 0.22114646434783936\n",
      "Gradient for fc2.bias: 0.08133883029222488\n",
      "torch.Size([41]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.3763, grad_fn=<MinBackward1>)\n",
      "Episode 423, Loss: -0.10013424605131149\n",
      "Gradient for fc1.weight: 0.015169301070272923\n",
      "Gradient for fc1.bias: 0.026892976835370064\n",
      "Gradient for fc2.weight: 0.018217630684375763\n",
      "Gradient for fc2.bias: 0.006214427761733532\n",
      "torch.Size([46]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.0056, grad_fn=<MinBackward1>)\n",
      "Episode 424, Loss: -0.11796273291110992\n",
      "Gradient for fc1.weight: 0.02706136740744114\n",
      "Gradient for fc1.bias: 0.10657310485839844\n",
      "Gradient for fc2.weight: 0.04955006018280983\n",
      "Gradient for fc2.bias: 0.028246203437447548\n",
      "torch.Size([41]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-3.0025, grad_fn=<MinBackward1>)\n",
      "Episode 425, Loss: -0.02818753942847252\n",
      "Gradient for fc1.weight: 0.07245670258998871\n",
      "Gradient for fc1.bias: 0.08245596289634705\n",
      "Gradient for fc2.weight: 0.07874814420938492\n",
      "Gradient for fc2.bias: 0.02608078345656395\n",
      "torch.Size([51]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-2.4412, grad_fn=<MinBackward1>)\n",
      "Episode 426, Loss: -0.019810333847999573\n",
      "Gradient for fc1.weight: 0.04387669637799263\n",
      "Gradient for fc1.bias: 0.09719862788915634\n",
      "Gradient for fc2.weight: 0.08040156215429306\n",
      "Gradient for fc2.bias: 0.034597571939229965\n",
      "torch.Size([37]) tensor(-8.5715e-05, grad_fn=<MaxBackward1>) tensor(-3.0531, grad_fn=<MinBackward1>)\n",
      "Episode 427, Loss: -0.04045102745294571\n",
      "Gradient for fc1.weight: 0.04534561187028885\n",
      "Gradient for fc1.bias: 0.1572452038526535\n",
      "Gradient for fc2.weight: 0.09873516112565994\n",
      "Gradient for fc2.bias: 0.049939077347517014\n",
      "torch.Size([28]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.4781, grad_fn=<MinBackward1>)\n",
      "Episode 428, Loss: -0.11923802644014359\n",
      "Gradient for fc1.weight: 0.13462784886360168\n",
      "Gradient for fc1.bias: 0.11840179562568665\n",
      "Gradient for fc2.weight: 0.13696783781051636\n",
      "Gradient for fc2.bias: 0.027350541204214096\n",
      "torch.Size([30]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.2173, grad_fn=<MinBackward1>)\n",
      "Episode 429, Loss: -0.18446186184883118\n",
      "Gradient for fc1.weight: 0.06463897228240967\n",
      "Gradient for fc1.bias: 0.13516491651535034\n",
      "Gradient for fc2.weight: 0.09755294770002365\n",
      "Gradient for fc2.bias: 0.03947886824607849\n",
      "torch.Size([45]) tensor(-9.0603e-05, grad_fn=<MaxBackward1>) tensor(-2.8182, grad_fn=<MinBackward1>)\n",
      "Episode 430, Loss: -0.011285020038485527\n",
      "Gradient for fc1.weight: 0.08650774508714676\n",
      "Gradient for fc1.bias: 0.060890160501003265\n",
      "Gradient for fc2.weight: 0.08504584431648254\n",
      "Gradient for fc2.bias: 0.016811760142445564\n",
      "torch.Size([52]) tensor(-7.9277e-05, grad_fn=<MaxBackward1>) tensor(-2.8954, grad_fn=<MinBackward1>)\n",
      "Episode 431, Loss: -0.04827909171581268\n",
      "Gradient for fc1.weight: 0.10561305284500122\n",
      "Gradient for fc1.bias: 0.2818478047847748\n",
      "Gradient for fc2.weight: 0.2174297273159027\n",
      "Gradient for fc2.bias: 0.10102953761816025\n",
      "torch.Size([27]) tensor(-0.0023, grad_fn=<MaxBackward1>) tensor(-1.3167, grad_fn=<MinBackward1>)\n",
      "Episode 432, Loss: -0.06362562626600266\n",
      "Gradient for fc1.weight: 0.02969180978834629\n",
      "Gradient for fc1.bias: 0.05542360246181488\n",
      "Gradient for fc2.weight: 0.03428108990192413\n",
      "Gradient for fc2.bias: 0.011830869130790234\n",
      "torch.Size([46]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-1.2347, grad_fn=<MinBackward1>)\n",
      "Episode 433, Loss: -0.07167184352874756\n",
      "Gradient for fc1.weight: 0.0359245203435421\n",
      "Gradient for fc1.bias: 0.12402033805847168\n",
      "Gradient for fc2.weight: 0.08555928617715836\n",
      "Gradient for fc2.bias: 0.042462367564439774\n",
      "torch.Size([27]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.0829, grad_fn=<MinBackward1>)\n",
      "Episode 434, Loss: -0.22221209108829498\n",
      "Gradient for fc1.weight: 0.19865037500858307\n",
      "Gradient for fc1.bias: 0.37788939476013184\n",
      "Gradient for fc2.weight: 0.29926058650016785\n",
      "Gradient for fc2.bias: 0.12865053117275238\n",
      "torch.Size([40]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-1.6303, grad_fn=<MinBackward1>)\n",
      "Episode 435, Loss: 0.168670192360878\n",
      "Gradient for fc1.weight: 0.01751929707825184\n",
      "Gradient for fc1.bias: 0.23921321332454681\n",
      "Gradient for fc2.weight: 0.15046443045139313\n",
      "Gradient for fc2.bias: 0.07768678665161133\n",
      "torch.Size([79]) tensor(-8.9769e-05, grad_fn=<MaxBackward1>) tensor(-2.3973, grad_fn=<MinBackward1>)\n",
      "Episode 436, Loss: 0.08799231052398682\n",
      "Gradient for fc1.weight: 0.02895466238260269\n",
      "Gradient for fc1.bias: 0.17099566757678986\n",
      "Gradient for fc2.weight: 0.10281180590391159\n",
      "Gradient for fc2.bias: 0.05292835086584091\n",
      "torch.Size([41]) tensor(-6.1693e-05, grad_fn=<MaxBackward1>) tensor(-2.8676, grad_fn=<MinBackward1>)\n",
      "Episode 437, Loss: 0.05913135036826134\n",
      "Gradient for fc1.weight: 0.04754071682691574\n",
      "Gradient for fc1.bias: 0.12551568448543549\n",
      "Gradient for fc2.weight: 0.09713311493396759\n",
      "Gradient for fc2.bias: 0.04657776653766632\n",
      "torch.Size([30]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.6657, grad_fn=<MinBackward1>)\n",
      "Episode 438, Loss: -0.019487563520669937\n",
      "Gradient for fc1.weight: 0.013931211084127426\n",
      "Gradient for fc1.bias: 0.08147134631872177\n",
      "Gradient for fc2.weight: 0.050637390464544296\n",
      "Gradient for fc2.bias: 0.026003727689385414\n",
      "torch.Size([41]) tensor(-5.4838e-05, grad_fn=<MaxBackward1>) tensor(-3.2956, grad_fn=<MinBackward1>)\n",
      "Episode 439, Loss: -0.001966737909242511\n",
      "Gradient for fc1.weight: 0.06591489166021347\n",
      "Gradient for fc1.bias: 0.0470040887594223\n",
      "Gradient for fc2.weight: 0.05782691761851311\n",
      "Gradient for fc2.bias: 0.002929084934294224\n",
      "torch.Size([39]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-3.7701, grad_fn=<MinBackward1>)\n",
      "Episode 440, Loss: -0.022648891434073448\n",
      "Gradient for fc1.weight: 0.0934041291475296\n",
      "Gradient for fc1.bias: 0.13244791328907013\n",
      "Gradient for fc2.weight: 0.12376939505338669\n",
      "Gradient for fc2.bias: 0.05010237172245979\n",
      "torch.Size([58]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-2.2107, grad_fn=<MinBackward1>)\n",
      "Episode 441, Loss: -0.005396092310547829\n",
      "Gradient for fc1.weight: 0.03361571952700615\n",
      "Gradient for fc1.bias: 0.07129009068012238\n",
      "Gradient for fc2.weight: 0.060775190591812134\n",
      "Gradient for fc2.bias: 0.027713531628251076\n",
      "torch.Size([78]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-1.7744, grad_fn=<MinBackward1>)\n",
      "Episode 442, Loss: 0.04720539599657059\n",
      "Gradient for fc1.weight: 0.04814492538571358\n",
      "Gradient for fc1.bias: 0.16564588248729706\n",
      "Gradient for fc2.weight: 0.1195848360657692\n",
      "Gradient for fc2.bias: 0.05746939033269882\n",
      "torch.Size([39]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-1.4579, grad_fn=<MinBackward1>)\n",
      "Episode 443, Loss: -0.06735537201166153\n",
      "Gradient for fc1.weight: 0.05084531381726265\n",
      "Gradient for fc1.bias: 0.13461975753307343\n",
      "Gradient for fc2.weight: 0.10114363580942154\n",
      "Gradient for fc2.bias: 0.04706297442317009\n",
      "torch.Size([52]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.9093, grad_fn=<MinBackward1>)\n",
      "Episode 444, Loss: 0.0697072446346283\n",
      "Gradient for fc1.weight: 0.05637040734291077\n",
      "Gradient for fc1.bias: 0.2904708981513977\n",
      "Gradient for fc2.weight: 0.19547408819198608\n",
      "Gradient for fc2.bias: 0.09954196214675903\n",
      "torch.Size([40]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-1.2471, grad_fn=<MinBackward1>)\n",
      "Episode 445, Loss: 0.051895905286073685\n",
      "Gradient for fc1.weight: 0.053245943039655685\n",
      "Gradient for fc1.bias: 0.14799712598323822\n",
      "Gradient for fc2.weight: 0.11507965624332428\n",
      "Gradient for fc2.bias: 0.05423738807439804\n",
      "torch.Size([62]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.6459, grad_fn=<MinBackward1>)\n",
      "Episode 446, Loss: 0.04887641221284866\n",
      "Gradient for fc1.weight: 0.0865154042840004\n",
      "Gradient for fc1.bias: 0.4143042266368866\n",
      "Gradient for fc2.weight: 0.2995675802230835\n",
      "Gradient for fc2.bias: 0.1469331532716751\n",
      "torch.Size([55]) tensor(-8.5059e-05, grad_fn=<MaxBackward1>) tensor(-4.3598, grad_fn=<MinBackward1>)\n",
      "Episode 447, Loss: 0.011374853551387787\n",
      "Gradient for fc1.weight: 0.08090849965810776\n",
      "Gradient for fc1.bias: 0.22927933931350708\n",
      "Gradient for fc2.weight: 0.1808922290802002\n",
      "Gradient for fc2.bias: 0.08704738318920135\n",
      "torch.Size([28]) tensor(-7.1647e-05, grad_fn=<MaxBackward1>) tensor(-3.1359, grad_fn=<MinBackward1>)\n",
      "Episode 448, Loss: -0.08908738940954208\n",
      "Gradient for fc1.weight: 0.014709370210766792\n",
      "Gradient for fc1.bias: 0.02281760238111019\n",
      "Gradient for fc2.weight: 0.01707252860069275\n",
      "Gradient for fc2.bias: 0.005983361974358559\n",
      "torch.Size([49]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-1.5095, grad_fn=<MinBackward1>)\n",
      "Episode 449, Loss: 0.1548180878162384\n",
      "Gradient for fc1.weight: 0.018030401319265366\n",
      "Gradient for fc1.bias: 0.31434300541877747\n",
      "Gradient for fc2.weight: 0.19718752801418304\n",
      "Gradient for fc2.bias: 0.10329130291938782\n",
      "torch.Size([47]) tensor(-6.1991e-05, grad_fn=<MaxBackward1>) tensor(-5.9279, grad_fn=<MinBackward1>)\n",
      "Episode 450, Loss: 0.24136480689048767\n",
      "Gradient for fc1.weight: 0.027986828237771988\n",
      "Gradient for fc1.bias: 0.06357759237289429\n",
      "Gradient for fc2.weight: 0.08738944679498672\n",
      "Gradient for fc2.bias: 0.02683357708156109\n",
      "torch.Size([33]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.1082, grad_fn=<MinBackward1>)\n",
      "Episode 451, Loss: 0.018436752259731293\n",
      "Gradient for fc1.weight: 0.06426785886287689\n",
      "Gradient for fc1.bias: 0.18313974142074585\n",
      "Gradient for fc2.weight: 0.1272730976343155\n",
      "Gradient for fc2.bias: 0.0603962168097496\n",
      "torch.Size([61]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-1.5278, grad_fn=<MinBackward1>)\n",
      "Episode 452, Loss: 0.06042201444506645\n",
      "Gradient for fc1.weight: 0.01999192126095295\n",
      "Gradient for fc1.bias: 0.10995504260063171\n",
      "Gradient for fc2.weight: 0.07466811686754227\n",
      "Gradient for fc2.bias: 0.03821874037384987\n",
      "torch.Size([103]) tensor(-2.9207e-05, grad_fn=<MaxBackward1>) tensor(-6.6735, grad_fn=<MinBackward1>)\n",
      "Episode 453, Loss: 0.09908383339643478\n",
      "Gradient for fc1.weight: 0.012808251194655895\n",
      "Gradient for fc1.bias: 0.034220822155475616\n",
      "Gradient for fc2.weight: 0.02125592529773712\n",
      "Gradient for fc2.bias: 0.009063351899385452\n",
      "torch.Size([37]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-1.3991, grad_fn=<MinBackward1>)\n",
      "Episode 454, Loss: -0.017207561060786247\n",
      "Gradient for fc1.weight: 0.0497131384909153\n",
      "Gradient for fc1.bias: 0.12455695122480392\n",
      "Gradient for fc2.weight: 0.08630089461803436\n",
      "Gradient for fc2.bias: 0.04055272415280342\n",
      "torch.Size([69]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-1.2777, grad_fn=<MinBackward1>)\n",
      "Episode 455, Loss: 0.11577576398849487\n",
      "Gradient for fc1.weight: 0.022547665983438492\n",
      "Gradient for fc1.bias: 0.09653766453266144\n",
      "Gradient for fc2.weight: 0.06971711665391922\n",
      "Gradient for fc2.bias: 0.033237606287002563\n",
      "torch.Size([42]) tensor(-1.2398e-05, grad_fn=<MaxBackward1>) tensor(-6.1593, grad_fn=<MinBackward1>)\n",
      "Episode 456, Loss: 0.16964244842529297\n",
      "Gradient for fc1.weight: 0.060548700392246246\n",
      "Gradient for fc1.bias: 0.2236911952495575\n",
      "Gradient for fc2.weight: 0.15960977971553802\n",
      "Gradient for fc2.bias: 0.07555346190929413\n",
      "torch.Size([25]) tensor(-0.0018, grad_fn=<MaxBackward1>) tensor(-0.7887, grad_fn=<MinBackward1>)\n",
      "Episode 457, Loss: -0.0019184105331078172\n",
      "Gradient for fc1.weight: 0.12900708615779877\n",
      "Gradient for fc1.bias: 0.3826075792312622\n",
      "Gradient for fc2.weight: 0.2713983654975891\n",
      "Gradient for fc2.bias: 0.12961238622665405\n",
      "torch.Size([83]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-0.8818, grad_fn=<MinBackward1>)\n",
      "Episode 458, Loss: 0.11862220615148544\n",
      "Gradient for fc1.weight: 0.01529221422970295\n",
      "Gradient for fc1.bias: 0.08395801484584808\n",
      "Gradient for fc2.weight: 0.06116796284914017\n",
      "Gradient for fc2.bias: 0.02990167774260044\n",
      "torch.Size([33]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-1.4461, grad_fn=<MinBackward1>)\n",
      "Episode 459, Loss: 0.1475900411605835\n",
      "Gradient for fc1.weight: 0.019229242578148842\n",
      "Gradient for fc1.bias: 0.28776344656944275\n",
      "Gradient for fc2.weight: 0.19366133213043213\n",
      "Gradient for fc2.bias: 0.09907179325819016\n",
      "torch.Size([62]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.3546, grad_fn=<MinBackward1>)\n",
      "Episode 460, Loss: 0.07383274286985397\n",
      "Gradient for fc1.weight: 0.02300339937210083\n",
      "Gradient for fc1.bias: 0.03513626009225845\n",
      "Gradient for fc2.weight: 0.030436009168624878\n",
      "Gradient for fc2.bias: 0.013117171823978424\n",
      "torch.Size([32]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-1.6498, grad_fn=<MinBackward1>)\n",
      "Episode 461, Loss: -0.031722068786621094\n",
      "Gradient for fc1.weight: 0.09912163764238358\n",
      "Gradient for fc1.bias: 0.1568644642829895\n",
      "Gradient for fc2.weight: 0.1317884922027588\n",
      "Gradient for fc2.bias: 0.05494298040866852\n",
      "torch.Size([57]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.0150, grad_fn=<MinBackward1>)\n",
      "Episode 462, Loss: -0.0018256229814141989\n",
      "Gradient for fc1.weight: 0.03507786616683006\n",
      "Gradient for fc1.bias: 0.03235265612602234\n",
      "Gradient for fc2.weight: 0.02954191528260708\n",
      "Gradient for fc2.bias: 0.0033586316276341677\n",
      "torch.Size([35]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.9210, grad_fn=<MinBackward1>)\n",
      "Episode 463, Loss: 0.11344092339277267\n",
      "Gradient for fc1.weight: 0.016773685812950134\n",
      "Gradient for fc1.bias: 0.04510055482387543\n",
      "Gradient for fc2.weight: 0.027662908658385277\n",
      "Gradient for fc2.bias: 0.012999407947063446\n",
      "torch.Size([37]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.2812, grad_fn=<MinBackward1>)\n",
      "Episode 464, Loss: 0.024852104485034943\n",
      "Gradient for fc1.weight: 0.1169358491897583\n",
      "Gradient for fc1.bias: 0.200958251953125\n",
      "Gradient for fc2.weight: 0.1609749048948288\n",
      "Gradient for fc2.bias: 0.06910175085067749\n",
      "torch.Size([24]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-0.9495, grad_fn=<MinBackward1>)\n",
      "Episode 465, Loss: -0.09196022152900696\n",
      "Gradient for fc1.weight: 0.04004191234707832\n",
      "Gradient for fc1.bias: 0.026374533772468567\n",
      "Gradient for fc2.weight: 0.03513235226273537\n",
      "Gradient for fc2.bias: 0.007369904778897762\n",
      "torch.Size([61]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-4.0203, grad_fn=<MinBackward1>)\n",
      "Episode 466, Loss: -0.054332442581653595\n",
      "Gradient for fc1.weight: 0.0658961683511734\n",
      "Gradient for fc1.bias: 0.05630768463015556\n",
      "Gradient for fc2.weight: 0.0609571598470211\n",
      "Gradient for fc2.bias: 0.018580880016088486\n",
      "torch.Size([24]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-1.1694, grad_fn=<MinBackward1>)\n",
      "Episode 467, Loss: -0.0919264629483223\n",
      "Gradient for fc1.weight: 0.11774295568466187\n",
      "Gradient for fc1.bias: 0.10176631808280945\n",
      "Gradient for fc2.weight: 0.10931127518415451\n",
      "Gradient for fc2.bias: 0.032556574791669846\n",
      "torch.Size([51]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-1.2360, grad_fn=<MinBackward1>)\n",
      "Episode 468, Loss: 0.09365183115005493\n",
      "Gradient for fc1.weight: 0.0537283755838871\n",
      "Gradient for fc1.bias: 0.13135184347629547\n",
      "Gradient for fc2.weight: 0.10149093717336655\n",
      "Gradient for fc2.bias: 0.04627056047320366\n",
      "torch.Size([48]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.0810, grad_fn=<MinBackward1>)\n",
      "Episode 469, Loss: 0.07508998364210129\n",
      "Gradient for fc1.weight: 0.029990484938025475\n",
      "Gradient for fc1.bias: 0.16795048117637634\n",
      "Gradient for fc2.weight: 0.10725221782922745\n",
      "Gradient for fc2.bias: 0.05687089264392853\n",
      "torch.Size([55]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-1.7299, grad_fn=<MinBackward1>)\n",
      "Episode 470, Loss: -0.03149149566888809\n",
      "Gradient for fc1.weight: 0.04996335506439209\n",
      "Gradient for fc1.bias: 0.1335238218307495\n",
      "Gradient for fc2.weight: 0.09549044072628021\n",
      "Gradient for fc2.bias: 0.045660097151994705\n",
      "torch.Size([41]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-5.2997, grad_fn=<MinBackward1>)\n",
      "Episode 471, Loss: 0.26953616738319397\n",
      "Gradient for fc1.weight: 0.03247247263789177\n",
      "Gradient for fc1.bias: 0.08974871784448624\n",
      "Gradient for fc2.weight: 0.08174099028110504\n",
      "Gradient for fc2.bias: 0.0042298948392271996\n",
      "torch.Size([36]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.6703, grad_fn=<MinBackward1>)\n",
      "Episode 472, Loss: -0.07954862713813782\n",
      "Gradient for fc1.weight: 0.12559010088443756\n",
      "Gradient for fc1.bias: 0.17382380366325378\n",
      "Gradient for fc2.weight: 0.1524408906698227\n",
      "Gradient for fc2.bias: 0.06015155836939812\n",
      "torch.Size([30]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-1.9645, grad_fn=<MinBackward1>)\n",
      "Episode 473, Loss: -0.07903534919023514\n",
      "Gradient for fc1.weight: 0.1463548094034195\n",
      "Gradient for fc1.bias: 0.18360833823680878\n",
      "Gradient for fc2.weight: 0.16165123879909515\n",
      "Gradient for fc2.bias: 0.061958879232406616\n",
      "torch.Size([63]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.8414, grad_fn=<MinBackward1>)\n",
      "Episode 474, Loss: -0.06779836863279343\n",
      "Gradient for fc1.weight: 0.06302390992641449\n",
      "Gradient for fc1.bias: 0.05913452059030533\n",
      "Gradient for fc2.weight: 0.06097611039876938\n",
      "Gradient for fc2.bias: 0.009857455268502235\n",
      "torch.Size([27]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-1.8619, grad_fn=<MinBackward1>)\n",
      "Episode 475, Loss: -0.08848399668931961\n",
      "Gradient for fc1.weight: 0.05686023831367493\n",
      "Gradient for fc1.bias: 0.06640834361314774\n",
      "Gradient for fc2.weight: 0.06230843812227249\n",
      "Gradient for fc2.bias: 0.02311549335718155\n",
      "torch.Size([49]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-1.2848, grad_fn=<MinBackward1>)\n",
      "Episode 476, Loss: 0.09245602786540985\n",
      "Gradient for fc1.weight: 0.059268638491630554\n",
      "Gradient for fc1.bias: 0.2000267654657364\n",
      "Gradient for fc2.weight: 0.14478735625743866\n",
      "Gradient for fc2.bias: 0.07050878554582596\n",
      "torch.Size([28]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-1.8648, grad_fn=<MinBackward1>)\n",
      "Episode 477, Loss: 0.10295349359512329\n",
      "Gradient for fc1.weight: 0.0066213770769536495\n",
      "Gradient for fc1.bias: 0.06938458979129791\n",
      "Gradient for fc2.weight: 0.04098213091492653\n",
      "Gradient for fc2.bias: 0.0218820720911026\n",
      "torch.Size([32]) tensor(-0.0031, grad_fn=<MaxBackward1>) tensor(-0.7089, grad_fn=<MinBackward1>)\n",
      "Episode 478, Loss: -0.07568707317113876\n",
      "Gradient for fc1.weight: 0.05191568657755852\n",
      "Gradient for fc1.bias: 0.08789052814245224\n",
      "Gradient for fc2.weight: 0.06651026010513306\n",
      "Gradient for fc2.bias: 0.028144478797912598\n",
      "torch.Size([22]) tensor(-0.0019, grad_fn=<MaxBackward1>) tensor(-0.8199, grad_fn=<MinBackward1>)\n",
      "Episode 479, Loss: -0.07805220782756805\n",
      "Gradient for fc1.weight: 0.03211457282304764\n",
      "Gradient for fc1.bias: 0.061472564935684204\n",
      "Gradient for fc2.weight: 0.046466149389743805\n",
      "Gradient for fc2.bias: 0.020526504144072533\n",
      "torch.Size([45]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.4663, grad_fn=<MinBackward1>)\n",
      "Episode 480, Loss: 0.13531962037086487\n",
      "Gradient for fc1.weight: 0.020791415125131607\n",
      "Gradient for fc1.bias: 0.16884513199329376\n",
      "Gradient for fc2.weight: 0.11812116950750351\n",
      "Gradient for fc2.bias: 0.05620086193084717\n",
      "torch.Size([78]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-4.2800, grad_fn=<MinBackward1>)\n",
      "Episode 481, Loss: 0.054041024297475815\n",
      "Gradient for fc1.weight: 0.013378182426095009\n",
      "Gradient for fc1.bias: 0.016120094805955887\n",
      "Gradient for fc2.weight: 0.021252911537885666\n",
      "Gradient for fc2.bias: 0.002329119946807623\n",
      "torch.Size([50]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.1772, grad_fn=<MinBackward1>)\n",
      "Episode 482, Loss: 0.1597166508436203\n",
      "Gradient for fc1.weight: 0.03589918836951256\n",
      "Gradient for fc1.bias: 0.3906398117542267\n",
      "Gradient for fc2.weight: 0.2638133466243744\n",
      "Gradient for fc2.bias: 0.136657252907753\n",
      "torch.Size([48]) tensor(-0.0018, grad_fn=<MaxBackward1>) tensor(-0.7413, grad_fn=<MinBackward1>)\n",
      "Episode 483, Loss: 0.030101580545306206\n",
      "Gradient for fc1.weight: 0.08887182176113129\n",
      "Gradient for fc1.bias: 0.44487860798835754\n",
      "Gradient for fc2.weight: 0.3010154366493225\n",
      "Gradient for fc2.bias: 0.15540002286434174\n",
      "torch.Size([52]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.9039, grad_fn=<MinBackward1>)\n",
      "Episode 484, Loss: 0.0985688790678978\n",
      "Gradient for fc1.weight: 0.011963686905801296\n",
      "Gradient for fc1.bias: 0.11529853194952011\n",
      "Gradient for fc2.weight: 0.07204798609018326\n",
      "Gradient for fc2.bias: 0.0385606549680233\n",
      "torch.Size([36]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.6248, grad_fn=<MinBackward1>)\n",
      "Episode 485, Loss: 0.11673232167959213\n",
      "Gradient for fc1.weight: 0.011845743283629417\n",
      "Gradient for fc1.bias: 0.028400707989931107\n",
      "Gradient for fc2.weight: 0.027448710054159164\n",
      "Gradient for fc2.bias: 0.010907778516411781\n",
      "torch.Size([42]) tensor(-0.0016, grad_fn=<MaxBackward1>) tensor(-1.1995, grad_fn=<MinBackward1>)\n",
      "Episode 486, Loss: 0.07047148048877716\n",
      "Gradient for fc1.weight: 0.01261477917432785\n",
      "Gradient for fc1.bias: 0.01913793943822384\n",
      "Gradient for fc2.weight: 0.012964721769094467\n",
      "Gradient for fc2.bias: 0.0034070550464093685\n",
      "torch.Size([54]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.0395, grad_fn=<MinBackward1>)\n",
      "Episode 487, Loss: 0.004115146119147539\n",
      "Gradient for fc1.weight: 0.03601491451263428\n",
      "Gradient for fc1.bias: 0.030342841520905495\n",
      "Gradient for fc2.weight: 0.03303438797593117\n",
      "Gradient for fc2.bias: 0.01107868179678917\n",
      "torch.Size([31]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-3.3189, grad_fn=<MinBackward1>)\n",
      "Episode 488, Loss: 0.04075166583061218\n",
      "Gradient for fc1.weight: 0.14489737153053284\n",
      "Gradient for fc1.bias: 0.2757503390312195\n",
      "Gradient for fc2.weight: 0.23335742950439453\n",
      "Gradient for fc2.bias: 0.10135543346405029\n",
      "torch.Size([33]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-4.3336, grad_fn=<MinBackward1>)\n",
      "Episode 489, Loss: 0.0809965431690216\n",
      "Gradient for fc1.weight: 0.12435364723205566\n",
      "Gradient for fc1.bias: 0.25149810314178467\n",
      "Gradient for fc2.weight: 0.2312498688697815\n",
      "Gradient for fc2.bias: 0.1004941314458847\n",
      "torch.Size([31]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-3.8537, grad_fn=<MinBackward1>)\n",
      "Episode 490, Loss: 0.06329544633626938\n",
      "Gradient for fc1.weight: 0.11394940316677094\n",
      "Gradient for fc1.bias: 0.25677862763404846\n",
      "Gradient for fc2.weight: 0.23427219688892365\n",
      "Gradient for fc2.bias: 0.10363829880952835\n",
      "torch.Size([58]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.7110, grad_fn=<MinBackward1>)\n",
      "Episode 491, Loss: 0.011955627240240574\n",
      "Gradient for fc1.weight: 0.019027620553970337\n",
      "Gradient for fc1.bias: 0.11514656245708466\n",
      "Gradient for fc2.weight: 0.08631645888090134\n",
      "Gradient for fc2.bias: 0.04478718340396881\n",
      "torch.Size([32]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-1.5065, grad_fn=<MinBackward1>)\n",
      "Episode 492, Loss: 0.040307458490133286\n",
      "Gradient for fc1.weight: 0.030445769429206848\n",
      "Gradient for fc1.bias: 0.11169213056564331\n",
      "Gradient for fc2.weight: 0.0834672600030899\n",
      "Gradient for fc2.bias: 0.04115983098745346\n",
      "torch.Size([27]) tensor(-0.0015, grad_fn=<MaxBackward1>) tensor(-2.8151, grad_fn=<MinBackward1>)\n",
      "Episode 493, Loss: -0.07755206525325775\n",
      "Gradient for fc1.weight: 0.1372625082731247\n",
      "Gradient for fc1.bias: 0.2548576295375824\n",
      "Gradient for fc2.weight: 0.19467687606811523\n",
      "Gradient for fc2.bias: 0.08882280439138412\n",
      "torch.Size([28]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.2509, grad_fn=<MinBackward1>)\n",
      "Episode 494, Loss: -0.02591840736567974\n",
      "Gradient for fc1.weight: 0.14999891817569733\n",
      "Gradient for fc1.bias: 0.13748225569725037\n",
      "Gradient for fc2.weight: 0.1433880478143692\n",
      "Gradient for fc2.bias: 0.04636223241686821\n",
      "torch.Size([34]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-1.5547, grad_fn=<MinBackward1>)\n",
      "Episode 495, Loss: -0.1330636739730835\n",
      "Gradient for fc1.weight: 0.14559581875801086\n",
      "Gradient for fc1.bias: 0.2810852825641632\n",
      "Gradient for fc2.weight: 0.21176622807979584\n",
      "Gradient for fc2.bias: 0.09930560737848282\n",
      "torch.Size([39]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.7193, grad_fn=<MinBackward1>)\n",
      "Episode 496, Loss: -0.01520319003611803\n",
      "Gradient for fc1.weight: 0.14820687472820282\n",
      "Gradient for fc1.bias: 0.39187365770339966\n",
      "Gradient for fc2.weight: 0.2849943935871124\n",
      "Gradient for fc2.bias: 0.14139752089977264\n",
      "torch.Size([43]) tensor(-0.0033, grad_fn=<MaxBackward1>) tensor(-2.3083, grad_fn=<MinBackward1>)\n",
      "Episode 497, Loss: 0.04082107916474342\n",
      "Gradient for fc1.weight: 0.021880222484469414\n",
      "Gradient for fc1.bias: 0.02813992090523243\n",
      "Gradient for fc2.weight: 0.032700065523386\n",
      "Gradient for fc2.bias: 0.007042774464935064\n",
      "torch.Size([57]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-3.0437, grad_fn=<MinBackward1>)\n",
      "Episode 498, Loss: 0.08196989446878433\n",
      "Gradient for fc1.weight: 0.03792034834623337\n",
      "Gradient for fc1.bias: 0.15675145387649536\n",
      "Gradient for fc2.weight: 0.13609854876995087\n",
      "Gradient for fc2.bias: 0.06309375166893005\n",
      "torch.Size([35]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.3842, grad_fn=<MinBackward1>)\n",
      "Episode 499, Loss: 0.1605006903409958\n",
      "Gradient for fc1.weight: 0.04584766924381256\n",
      "Gradient for fc1.bias: 0.21810321509838104\n",
      "Gradient for fc2.weight: 0.13898195326328278\n",
      "Gradient for fc2.bias: 0.07081826776266098\n",
      "torch.Size([49]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.4745, grad_fn=<MinBackward1>)\n",
      "Episode 500, Loss: 0.028029704466462135\n",
      "Gradient for fc1.weight: 0.04063570499420166\n",
      "Gradient for fc1.bias: 0.15238307416439056\n",
      "Gradient for fc2.weight: 0.13156205415725708\n",
      "Gradient for fc2.bias: 0.06540656834840775\n",
      "Running reward: 33.52047833755974\n",
      "torch.Size([24]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-4.0726, grad_fn=<MinBackward1>)\n",
      "Episode 501, Loss: 0.07345228642225266\n",
      "Gradient for fc1.weight: 0.04081713408231735\n",
      "Gradient for fc1.bias: 0.21083290874958038\n",
      "Gradient for fc2.weight: 0.1402508020401001\n",
      "Gradient for fc2.bias: 0.07581362128257751\n",
      "torch.Size([24]) tensor(-0.0044, grad_fn=<MaxBackward1>) tensor(-0.9583, grad_fn=<MinBackward1>)\n",
      "Episode 502, Loss: -0.049011170864105225\n",
      "Gradient for fc1.weight: 0.19829736649990082\n",
      "Gradient for fc1.bias: 0.18361277878284454\n",
      "Gradient for fc2.weight: 0.20158766210079193\n",
      "Gradient for fc2.bias: 0.06488610059022903\n",
      "torch.Size([35]) tensor(-0.0081, grad_fn=<MaxBackward1>) tensor(-1.7267, grad_fn=<MinBackward1>)\n",
      "Episode 503, Loss: 0.01920916885137558\n",
      "Gradient for fc1.weight: 0.060076553374528885\n",
      "Gradient for fc1.bias: 0.059244342148303986\n",
      "Gradient for fc2.weight: 0.061354488134384155\n",
      "Gradient for fc2.bias: 0.016650419682264328\n",
      "torch.Size([23]) tensor(-0.0051, grad_fn=<MaxBackward1>) tensor(-2.4298, grad_fn=<MinBackward1>)\n",
      "Episode 504, Loss: -0.04645765200257301\n",
      "Gradient for fc1.weight: 0.05149044468998909\n",
      "Gradient for fc1.bias: 0.08730180561542511\n",
      "Gradient for fc2.weight: 0.07211800664663315\n",
      "Gradient for fc2.bias: 0.03309042751789093\n",
      "torch.Size([26]) tensor(-0.0075, grad_fn=<MaxBackward1>) tensor(-1.9428, grad_fn=<MinBackward1>)\n",
      "Episode 505, Loss: 0.06610853224992752\n",
      "Gradient for fc1.weight: 0.1000826507806778\n",
      "Gradient for fc1.bias: 0.1667177677154541\n",
      "Gradient for fc2.weight: 0.18428291380405426\n",
      "Gradient for fc2.bias: 0.07513485103845596\n",
      "torch.Size([22]) tensor(-0.0047, grad_fn=<MaxBackward1>) tensor(-0.8271, grad_fn=<MinBackward1>)\n",
      "Episode 506, Loss: -0.049651481211185455\n",
      "Gradient for fc1.weight: 0.28163471817970276\n",
      "Gradient for fc1.bias: 0.4309230446815491\n",
      "Gradient for fc2.weight: 0.42046836018562317\n",
      "Gradient for fc2.bias: 0.1791956126689911\n",
      "torch.Size([34]) tensor(-0.0084, grad_fn=<MaxBackward1>) tensor(-1.4543, grad_fn=<MinBackward1>)\n",
      "Episode 507, Loss: 0.01172703504562378\n",
      "Gradient for fc1.weight: 0.03867382928729057\n",
      "Gradient for fc1.bias: 0.03005409985780716\n",
      "Gradient for fc2.weight: 0.027538171038031578\n",
      "Gradient for fc2.bias: 0.0013257827376946807\n",
      "torch.Size([17]) tensor(-0.0087, grad_fn=<MaxBackward1>) tensor(-1.7853, grad_fn=<MinBackward1>)\n",
      "Episode 508, Loss: -0.07646839320659637\n",
      "Gradient for fc1.weight: 0.1667490303516388\n",
      "Gradient for fc1.bias: 0.18012960255146027\n",
      "Gradient for fc2.weight: 0.20517727732658386\n",
      "Gradient for fc2.bias: 0.07102619856595993\n",
      "torch.Size([39]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-3.3353, grad_fn=<MinBackward1>)\n",
      "Episode 509, Loss: -0.10247733443975449\n",
      "Gradient for fc1.weight: 0.11686383187770844\n",
      "Gradient for fc1.bias: 0.29777732491493225\n",
      "Gradient for fc2.weight: 0.27886828780174255\n",
      "Gradient for fc2.bias: 0.1313253939151764\n",
      "torch.Size([35]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.9832, grad_fn=<MinBackward1>)\n",
      "Episode 510, Loss: 0.04034043848514557\n",
      "Gradient for fc1.weight: 0.04186013713479042\n",
      "Gradient for fc1.bias: 0.09564413130283356\n",
      "Gradient for fc2.weight: 0.05486870929598808\n",
      "Gradient for fc2.bias: 0.023074530065059662\n",
      "torch.Size([76]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-3.9053, grad_fn=<MinBackward1>)\n",
      "Episode 511, Loss: -0.04075261950492859\n",
      "Gradient for fc1.weight: 0.03405521437525749\n",
      "Gradient for fc1.bias: 0.19785282015800476\n",
      "Gradient for fc2.weight: 0.1575244814157486\n",
      "Gradient for fc2.bias: 0.08262110501527786\n",
      "torch.Size([28]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-3.0318, grad_fn=<MinBackward1>)\n",
      "Episode 512, Loss: 0.009682191535830498\n",
      "Gradient for fc1.weight: 0.08472249656915665\n",
      "Gradient for fc1.bias: 0.11431726813316345\n",
      "Gradient for fc2.weight: 0.1138906329870224\n",
      "Gradient for fc2.bias: 0.04712941125035286\n",
      "torch.Size([28]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-3.3916, grad_fn=<MinBackward1>)\n",
      "Episode 513, Loss: -0.014460314996540546\n",
      "Gradient for fc1.weight: 0.08845900744199753\n",
      "Gradient for fc1.bias: 0.23559673130512238\n",
      "Gradient for fc2.weight: 0.18843883275985718\n",
      "Gradient for fc2.bias: 0.09445537626743317\n",
      "torch.Size([56]) tensor(-8.4761e-05, grad_fn=<MaxBackward1>) tensor(-4.4825, grad_fn=<MinBackward1>)\n",
      "Episode 514, Loss: 0.18297694623470306\n",
      "Gradient for fc1.weight: 0.04274282231926918\n",
      "Gradient for fc1.bias: 0.12275633215904236\n",
      "Gradient for fc2.weight: 0.11561842262744904\n",
      "Gradient for fc2.bias: 0.04371301084756851\n",
      "torch.Size([25]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.6503, grad_fn=<MinBackward1>)\n",
      "Episode 515, Loss: -0.1859266608953476\n",
      "Gradient for fc1.weight: 0.1177341565489769\n",
      "Gradient for fc1.bias: 0.17626862227916718\n",
      "Gradient for fc2.weight: 0.1474931687116623\n",
      "Gradient for fc2.bias: 0.06097870320081711\n",
      "torch.Size([28]) tensor(-0.0035, grad_fn=<MaxBackward1>) tensor(-2.1604, grad_fn=<MinBackward1>)\n",
      "Episode 516, Loss: 0.02313845418393612\n",
      "Gradient for fc1.weight: 0.02372320368885994\n",
      "Gradient for fc1.bias: 0.06904733926057816\n",
      "Gradient for fc2.weight: 0.03458886966109276\n",
      "Gradient for fc2.bias: 0.02032463066279888\n",
      "torch.Size([56]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-1.5363, grad_fn=<MinBackward1>)\n",
      "Episode 517, Loss: 0.02850799635052681\n",
      "Gradient for fc1.weight: 0.017396211624145508\n",
      "Gradient for fc1.bias: 0.12712621688842773\n",
      "Gradient for fc2.weight: 0.09191694110631943\n",
      "Gradient for fc2.bias: 0.05070347338914871\n",
      "torch.Size([40]) tensor(-0.0044, grad_fn=<MaxBackward1>) tensor(-1.7799, grad_fn=<MinBackward1>)\n",
      "Episode 518, Loss: 0.04238297790288925\n",
      "Gradient for fc1.weight: 0.0202031210064888\n",
      "Gradient for fc1.bias: 0.029779627919197083\n",
      "Gradient for fc2.weight: 0.019480865448713303\n",
      "Gradient for fc2.bias: 0.0028310806956142187\n",
      "torch.Size([24]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-2.8430, grad_fn=<MinBackward1>)\n",
      "Episode 519, Loss: -0.08126585930585861\n",
      "Gradient for fc1.weight: 0.02360079251229763\n",
      "Gradient for fc1.bias: 0.060509491711854935\n",
      "Gradient for fc2.weight: 0.04214447736740112\n",
      "Gradient for fc2.bias: 0.01952204667031765\n",
      "torch.Size([59]) tensor(-0.0027, grad_fn=<MaxBackward1>) tensor(-2.1685, grad_fn=<MinBackward1>)\n",
      "Episode 520, Loss: 0.054998625069856644\n",
      "Gradient for fc1.weight: 0.03049449622631073\n",
      "Gradient for fc1.bias: 0.19292794167995453\n",
      "Gradient for fc2.weight: 0.15601873397827148\n",
      "Gradient for fc2.bias: 0.08069302141666412\n",
      "torch.Size([49]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-3.6712, grad_fn=<MinBackward1>)\n",
      "Episode 521, Loss: 0.04147519916296005\n",
      "Gradient for fc1.weight: 0.03122558631002903\n",
      "Gradient for fc1.bias: 0.05878961458802223\n",
      "Gradient for fc2.weight: 0.0598948635160923\n",
      "Gradient for fc2.bias: 0.027505533769726753\n",
      "torch.Size([23]) tensor(-0.0033, grad_fn=<MaxBackward1>) tensor(-2.2117, grad_fn=<MinBackward1>)\n",
      "Episode 522, Loss: 0.11134359985589981\n",
      "Gradient for fc1.weight: 0.0543803833425045\n",
      "Gradient for fc1.bias: 0.09942243248224258\n",
      "Gradient for fc2.weight: 0.09898514300584793\n",
      "Gradient for fc2.bias: 0.034082747995853424\n",
      "torch.Size([24]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-1.6589, grad_fn=<MinBackward1>)\n",
      "Episode 523, Loss: -0.04951382055878639\n",
      "Gradient for fc1.weight: 0.10685732215642929\n",
      "Gradient for fc1.bias: 0.13667722046375275\n",
      "Gradient for fc2.weight: 0.11081568896770477\n",
      "Gradient for fc2.bias: 0.0479569286108017\n",
      "torch.Size([53]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.4003, grad_fn=<MinBackward1>)\n",
      "Episode 524, Loss: 0.12439753115177155\n",
      "Gradient for fc1.weight: 0.060107044875621796\n",
      "Gradient for fc1.bias: 0.24424515664577484\n",
      "Gradient for fc2.weight: 0.22100774943828583\n",
      "Gradient for fc2.bias: 0.1065525934100151\n",
      "torch.Size([27]) tensor(-0.0027, grad_fn=<MaxBackward1>) tensor(-2.8079, grad_fn=<MinBackward1>)\n",
      "Episode 525, Loss: -0.1299433559179306\n",
      "Gradient for fc1.weight: 0.13523612916469574\n",
      "Gradient for fc1.bias: 0.20743171870708466\n",
      "Gradient for fc2.weight: 0.21706262230873108\n",
      "Gradient for fc2.bias: 0.0929214134812355\n",
      "torch.Size([32]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-3.9907, grad_fn=<MinBackward1>)\n",
      "Episode 526, Loss: 0.03991905227303505\n",
      "Gradient for fc1.weight: 0.037686120718717575\n",
      "Gradient for fc1.bias: 0.15112920105457306\n",
      "Gradient for fc2.weight: 0.09315202385187149\n",
      "Gradient for fc2.bias: 0.05426744371652603\n",
      "torch.Size([26]) tensor(-0.0356, grad_fn=<MaxBackward1>) tensor(-0.2957, grad_fn=<MinBackward1>)\n",
      "Episode 527, Loss: -0.02620413899421692\n",
      "Gradient for fc1.weight: 0.0734943225979805\n",
      "Gradient for fc1.bias: 0.13359545171260834\n",
      "Gradient for fc2.weight: 0.09596741944551468\n",
      "Gradient for fc2.bias: 0.04869859293103218\n",
      "torch.Size([30]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-1.7280, grad_fn=<MinBackward1>)\n",
      "Episode 528, Loss: -0.07428999245166779\n",
      "Gradient for fc1.weight: 0.03647398576140404\n",
      "Gradient for fc1.bias: 0.033134642988443375\n",
      "Gradient for fc2.weight: 0.033237311989068985\n",
      "Gradient for fc2.bias: 0.007453948259353638\n",
      "torch.Size([63]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-4.0531, grad_fn=<MinBackward1>)\n",
      "Episode 529, Loss: -0.1738884299993515\n",
      "Gradient for fc1.weight: 0.04040713608264923\n",
      "Gradient for fc1.bias: 0.04136401042342186\n",
      "Gradient for fc2.weight: 0.07206912338733673\n",
      "Gradient for fc2.bias: 0.014109056442975998\n",
      "torch.Size([35]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-1.6183, grad_fn=<MinBackward1>)\n",
      "Episode 530, Loss: -0.15340037643909454\n",
      "Gradient for fc1.weight: 0.0784192830324173\n",
      "Gradient for fc1.bias: 0.05369723215699196\n",
      "Gradient for fc2.weight: 0.07099438458681107\n",
      "Gradient for fc2.bias: 0.0064363861456513405\n",
      "torch.Size([34]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-1.8504, grad_fn=<MinBackward1>)\n",
      "Episode 531, Loss: -0.08789747953414917\n",
      "Gradient for fc1.weight: 0.0541386641561985\n",
      "Gradient for fc1.bias: 0.04631548374891281\n",
      "Gradient for fc2.weight: 0.051224999129772186\n",
      "Gradient for fc2.bias: 0.015027565881609917\n",
      "torch.Size([24]) tensor(-0.0032, grad_fn=<MaxBackward1>) tensor(-2.3897, grad_fn=<MinBackward1>)\n",
      "Episode 532, Loss: 0.04620601236820221\n",
      "Gradient for fc1.weight: 0.024070940911769867\n",
      "Gradient for fc1.bias: 0.05503179877996445\n",
      "Gradient for fc2.weight: 0.025449244305491447\n",
      "Gradient for fc2.bias: 0.01269565336406231\n",
      "torch.Size([28]) tensor(-0.0020, grad_fn=<MaxBackward1>) tensor(-3.0112, grad_fn=<MinBackward1>)\n",
      "Episode 533, Loss: -0.07364615052938461\n",
      "Gradient for fc1.weight: 0.027821216732263565\n",
      "Gradient for fc1.bias: 0.06459023058414459\n",
      "Gradient for fc2.weight: 0.050696004182100296\n",
      "Gradient for fc2.bias: 0.02499142661690712\n",
      "torch.Size([43]) tensor(-0.0012, grad_fn=<MaxBackward1>) tensor(-2.7081, grad_fn=<MinBackward1>)\n",
      "Episode 534, Loss: -0.029437653720378876\n",
      "Gradient for fc1.weight: 0.05937371402978897\n",
      "Gradient for fc1.bias: 0.12249206751585007\n",
      "Gradient for fc2.weight: 0.10615571588277817\n",
      "Gradient for fc2.bias: 0.051938459277153015\n",
      "torch.Size([50]) tensor(-0.0026, grad_fn=<MaxBackward1>) tensor(-1.5658, grad_fn=<MinBackward1>)\n",
      "Episode 535, Loss: -0.007495385594666004\n",
      "Gradient for fc1.weight: 0.06141549348831177\n",
      "Gradient for fc1.bias: 0.09912031143903732\n",
      "Gradient for fc2.weight: 0.08909642696380615\n",
      "Gradient for fc2.bias: 0.039805326610803604\n",
      "torch.Size([42]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-1.6316, grad_fn=<MinBackward1>)\n",
      "Episode 536, Loss: -0.10323279350996017\n",
      "Gradient for fc1.weight: 0.09107813984155655\n",
      "Gradient for fc1.bias: 0.08387705683708191\n",
      "Gradient for fc2.weight: 0.07747034728527069\n",
      "Gradient for fc2.bias: 0.007475544698536396\n",
      "torch.Size([25]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-1.3224, grad_fn=<MinBackward1>)\n",
      "Episode 537, Loss: -0.11937913298606873\n",
      "Gradient for fc1.weight: 0.08834999799728394\n",
      "Gradient for fc1.bias: 0.11699552088975906\n",
      "Gradient for fc2.weight: 0.09287018328905106\n",
      "Gradient for fc2.bias: 0.03871876746416092\n",
      "torch.Size([30]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-2.2980, grad_fn=<MinBackward1>)\n",
      "Episode 538, Loss: 0.005101641174405813\n",
      "Gradient for fc1.weight: 0.07630053162574768\n",
      "Gradient for fc1.bias: 0.09896238893270493\n",
      "Gradient for fc2.weight: 0.11073587089776993\n",
      "Gradient for fc2.bias: 0.04729495197534561\n",
      "torch.Size([36]) tensor(-0.0037, grad_fn=<MaxBackward1>) tensor(-0.7164, grad_fn=<MinBackward1>)\n",
      "Episode 539, Loss: 0.0347861647605896\n",
      "Gradient for fc1.weight: 0.04636036977171898\n",
      "Gradient for fc1.bias: 0.2683759033679962\n",
      "Gradient for fc2.weight: 0.18802258372306824\n",
      "Gradient for fc2.bias: 0.10533088445663452\n",
      "torch.Size([22]) tensor(-0.0094, grad_fn=<MaxBackward1>) tensor(-0.7407, grad_fn=<MinBackward1>)\n",
      "Episode 540, Loss: -0.08103791624307632\n",
      "Gradient for fc1.weight: 0.14218616485595703\n",
      "Gradient for fc1.bias: 0.2535967528820038\n",
      "Gradient for fc2.weight: 0.19017736613750458\n",
      "Gradient for fc2.bias: 0.09490606188774109\n",
      "torch.Size([36]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-2.2154, grad_fn=<MinBackward1>)\n",
      "Episode 541, Loss: -0.019863011315464973\n",
      "Gradient for fc1.weight: 0.0441121943295002\n",
      "Gradient for fc1.bias: 0.0556475855410099\n",
      "Gradient for fc2.weight: 0.04863059148192406\n",
      "Gradient for fc2.bias: 0.019750447943806648\n",
      "torch.Size([90]) tensor(-7.9218e-05, grad_fn=<MaxBackward1>) tensor(-4.2095, grad_fn=<MinBackward1>)\n",
      "Episode 542, Loss: 0.011247647926211357\n",
      "Gradient for fc1.weight: 0.053576208651065826\n",
      "Gradient for fc1.bias: 0.1138603687286377\n",
      "Gradient for fc2.weight: 0.10053549706935883\n",
      "Gradient for fc2.bias: 0.048180028796195984\n",
      "torch.Size([38]) tensor(-0.0025, grad_fn=<MaxBackward1>) tensor(-1.2643, grad_fn=<MinBackward1>)\n",
      "Episode 543, Loss: -0.08726868033409119\n",
      "Gradient for fc1.weight: 0.06395619362592697\n",
      "Gradient for fc1.bias: 0.07051995396614075\n",
      "Gradient for fc2.weight: 0.0830487459897995\n",
      "Gradient for fc2.bias: 0.030285030603408813\n",
      "torch.Size([44]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.1910, grad_fn=<MinBackward1>)\n",
      "Episode 544, Loss: 0.056538913398981094\n",
      "Gradient for fc1.weight: 0.05482367053627968\n",
      "Gradient for fc1.bias: 0.2761167585849762\n",
      "Gradient for fc2.weight: 0.214532732963562\n",
      "Gradient for fc2.bias: 0.11203303188085556\n",
      "torch.Size([49]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.2201, grad_fn=<MinBackward1>)\n",
      "Episode 545, Loss: 0.03008764423429966\n",
      "Gradient for fc1.weight: 0.09593317657709122\n",
      "Gradient for fc1.bias: 0.10524208098649979\n",
      "Gradient for fc2.weight: 0.10008709877729416\n",
      "Gradient for fc2.bias: 0.03846285119652748\n",
      "torch.Size([53]) tensor(-0.0018, grad_fn=<MaxBackward1>) tensor(-0.7768, grad_fn=<MinBackward1>)\n",
      "Episode 546, Loss: -0.022018305957317352\n",
      "Gradient for fc1.weight: 0.0534716472029686\n",
      "Gradient for fc1.bias: 0.22101561725139618\n",
      "Gradient for fc2.weight: 0.1525775045156479\n",
      "Gradient for fc2.bias: 0.08447976410388947\n",
      "torch.Size([33]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.6999, grad_fn=<MinBackward1>)\n",
      "Episode 547, Loss: 0.0893949419260025\n",
      "Gradient for fc1.weight: 0.11589659005403519\n",
      "Gradient for fc1.bias: 0.06813238561153412\n",
      "Gradient for fc2.weight: 0.10235044360160828\n",
      "Gradient for fc2.bias: 0.012491367757320404\n",
      "torch.Size([77]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.3648, grad_fn=<MinBackward1>)\n",
      "Episode 548, Loss: -0.07063610851764679\n",
      "Gradient for fc1.weight: 0.021580945700407028\n",
      "Gradient for fc1.bias: 0.08182276040315628\n",
      "Gradient for fc2.weight: 0.05616971105337143\n",
      "Gradient for fc2.bias: 0.030963484197854996\n",
      "torch.Size([34]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.5081, grad_fn=<MinBackward1>)\n",
      "Episode 549, Loss: 0.11426868289709091\n",
      "Gradient for fc1.weight: 0.021413136273622513\n",
      "Gradient for fc1.bias: 0.15644536912441254\n",
      "Gradient for fc2.weight: 0.09939111769199371\n",
      "Gradient for fc2.bias: 0.05636134371161461\n",
      "torch.Size([79]) tensor(-0.0020, grad_fn=<MaxBackward1>) tensor(-1.1254, grad_fn=<MinBackward1>)\n",
      "Episode 550, Loss: -0.034330930560827255\n",
      "Gradient for fc1.weight: 0.007940638810396194\n",
      "Gradient for fc1.bias: 0.023034481331706047\n",
      "Gradient for fc2.weight: 0.01809598132967949\n",
      "Gradient for fc2.bias: 0.008790316991508007\n",
      "torch.Size([46]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-4.8413, grad_fn=<MinBackward1>)\n",
      "Episode 551, Loss: -0.03507448732852936\n",
      "Gradient for fc1.weight: 0.03210972622036934\n",
      "Gradient for fc1.bias: 0.15397703647613525\n",
      "Gradient for fc2.weight: 0.13838687539100647\n",
      "Gradient for fc2.bias: 0.07145955413579941\n",
      "torch.Size([65]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.3412, grad_fn=<MinBackward1>)\n",
      "Episode 552, Loss: -0.05526261031627655\n",
      "Gradient for fc1.weight: 0.007060195319354534\n",
      "Gradient for fc1.bias: 0.01847224310040474\n",
      "Gradient for fc2.weight: 0.011213746853172779\n",
      "Gradient for fc2.bias: 0.004905810579657555\n",
      "torch.Size([35]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-3.3590, grad_fn=<MinBackward1>)\n",
      "Episode 553, Loss: 0.061153676360845566\n",
      "Gradient for fc1.weight: 0.10230369865894318\n",
      "Gradient for fc1.bias: 0.06337372958660126\n",
      "Gradient for fc2.weight: 0.09037336707115173\n",
      "Gradient for fc2.bias: 0.0009485126938670874\n",
      "torch.Size([34]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-3.7468, grad_fn=<MinBackward1>)\n",
      "Episode 554, Loss: 0.02091817557811737\n",
      "Gradient for fc1.weight: 0.18760116398334503\n",
      "Gradient for fc1.bias: 0.3711198568344116\n",
      "Gradient for fc2.weight: 0.2496577799320221\n",
      "Gradient for fc2.bias: 0.1294901967048645\n",
      "torch.Size([47]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-3.0360, grad_fn=<MinBackward1>)\n",
      "Episode 555, Loss: -0.052355386316776276\n",
      "Gradient for fc1.weight: 0.014953754842281342\n",
      "Gradient for fc1.bias: 0.01715683564543724\n",
      "Gradient for fc2.weight: 0.02093306928873062\n",
      "Gradient for fc2.bias: 0.0050215767696499825\n",
      "torch.Size([34]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.8018, grad_fn=<MinBackward1>)\n",
      "Episode 556, Loss: 0.11298125982284546\n",
      "Gradient for fc1.weight: 0.1258099377155304\n",
      "Gradient for fc1.bias: 0.07369817793369293\n",
      "Gradient for fc2.weight: 0.10837189108133316\n",
      "Gradient for fc2.bias: 0.004561685025691986\n",
      "torch.Size([37]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-1.1736, grad_fn=<MinBackward1>)\n",
      "Episode 557, Loss: -0.10230842232704163\n",
      "Gradient for fc1.weight: 0.03328477218747139\n",
      "Gradient for fc1.bias: 0.05579095706343651\n",
      "Gradient for fc2.weight: 0.04051084816455841\n",
      "Gradient for fc2.bias: 0.01953325793147087\n",
      "torch.Size([39]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.4016, grad_fn=<MinBackward1>)\n",
      "Episode 558, Loss: 0.06566847860813141\n",
      "Gradient for fc1.weight: 0.07959838211536407\n",
      "Gradient for fc1.bias: 0.30514368414878845\n",
      "Gradient for fc2.weight: 0.2396136224269867\n",
      "Gradient for fc2.bias: 0.12734158337116241\n",
      "torch.Size([54]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-1.5129, grad_fn=<MinBackward1>)\n",
      "Episode 559, Loss: 0.08733358234167099\n",
      "Gradient for fc1.weight: 0.036098457872867584\n",
      "Gradient for fc1.bias: 0.1078655794262886\n",
      "Gradient for fc2.weight: 0.0856921598315239\n",
      "Gradient for fc2.bias: 0.04288734123110771\n",
      "torch.Size([33]) tensor(-0.0019, grad_fn=<MaxBackward1>) tensor(-2.1689, grad_fn=<MinBackward1>)\n",
      "Episode 560, Loss: 0.031334035098552704\n",
      "Gradient for fc1.weight: 0.019203249365091324\n",
      "Gradient for fc1.bias: 0.22015272080898285\n",
      "Gradient for fc2.weight: 0.1510564684867859\n",
      "Gradient for fc2.bias: 0.0830468088388443\n",
      "torch.Size([44]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.9247, grad_fn=<MinBackward1>)\n",
      "Episode 561, Loss: -0.050797220319509506\n",
      "Gradient for fc1.weight: 0.057148903608322144\n",
      "Gradient for fc1.bias: 0.04852680861949921\n",
      "Gradient for fc2.weight: 0.041851408779621124\n",
      "Gradient for fc2.bias: 0.01140825729817152\n",
      "torch.Size([50]) tensor(-0.0021, grad_fn=<MaxBackward1>) tensor(-1.4470, grad_fn=<MinBackward1>)\n",
      "Episode 562, Loss: 0.08398586511611938\n",
      "Gradient for fc1.weight: 0.014897434040904045\n",
      "Gradient for fc1.bias: 0.17517729103565216\n",
      "Gradient for fc2.weight: 0.12160894274711609\n",
      "Gradient for fc2.bias: 0.067536361515522\n",
      "torch.Size([44]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-1.6151, grad_fn=<MinBackward1>)\n",
      "Episode 563, Loss: 0.03426510840654373\n",
      "Gradient for fc1.weight: 0.05667702481150627\n",
      "Gradient for fc1.bias: 0.156881645321846\n",
      "Gradient for fc2.weight: 0.11710511147975922\n",
      "Gradient for fc2.bias: 0.06183771789073944\n",
      "torch.Size([44]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.5744, grad_fn=<MinBackward1>)\n",
      "Episode 564, Loss: -0.057191070169210434\n",
      "Gradient for fc1.weight: 0.04672306030988693\n",
      "Gradient for fc1.bias: 0.2305954545736313\n",
      "Gradient for fc2.weight: 0.18031832575798035\n",
      "Gradient for fc2.bias: 0.09626596421003342\n",
      "torch.Size([41]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-3.5911, grad_fn=<MinBackward1>)\n",
      "Episode 565, Loss: 0.09809771925210953\n",
      "Gradient for fc1.weight: 0.021839667111635208\n",
      "Gradient for fc1.bias: 0.3261792063713074\n",
      "Gradient for fc2.weight: 0.22691550850868225\n",
      "Gradient for fc2.bias: 0.1268940567970276\n",
      "torch.Size([41]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-1.6364, grad_fn=<MinBackward1>)\n",
      "Episode 566, Loss: -0.003496629185974598\n",
      "Gradient for fc1.weight: 0.12695258855819702\n",
      "Gradient for fc1.bias: 0.2631496489048004\n",
      "Gradient for fc2.weight: 0.2000948041677475\n",
      "Gradient for fc2.bias: 0.10335085541009903\n",
      "torch.Size([31]) tensor(-0.0023, grad_fn=<MaxBackward1>) tensor(-1.3235, grad_fn=<MinBackward1>)\n",
      "Episode 567, Loss: -0.05774346739053726\n",
      "Gradient for fc1.weight: 0.038292281329631805\n",
      "Gradient for fc1.bias: 0.1573266237974167\n",
      "Gradient for fc2.weight: 0.1147647500038147\n",
      "Gradient for fc2.bias: 0.06256762892007828\n",
      "torch.Size([64]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.9255, grad_fn=<MinBackward1>)\n",
      "Episode 568, Loss: 0.03864389657974243\n",
      "Gradient for fc1.weight: 0.03279629349708557\n",
      "Gradient for fc1.bias: 0.09405557066202164\n",
      "Gradient for fc2.weight: 0.08571839332580566\n",
      "Gradient for fc2.bias: 0.04215460270643234\n",
      "torch.Size([61]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.2955, grad_fn=<MinBackward1>)\n",
      "Episode 569, Loss: -0.0784752294421196\n",
      "Gradient for fc1.weight: 0.05902586132287979\n",
      "Gradient for fc1.bias: 0.11503414809703827\n",
      "Gradient for fc2.weight: 0.10058022290468216\n",
      "Gradient for fc2.bias: 0.040869686752557755\n",
      "torch.Size([41]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-2.7370, grad_fn=<MinBackward1>)\n",
      "Episode 570, Loss: -0.04486672580242157\n",
      "Gradient for fc1.weight: 0.10521931946277618\n",
      "Gradient for fc1.bias: 0.2437211275100708\n",
      "Gradient for fc2.weight: 0.19260352849960327\n",
      "Gradient for fc2.bias: 0.10062555968761444\n",
      "torch.Size([39]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.5438, grad_fn=<MinBackward1>)\n",
      "Episode 571, Loss: -0.11534808576107025\n",
      "Gradient for fc1.weight: 0.16324986517429352\n",
      "Gradient for fc1.bias: 0.2671092450618744\n",
      "Gradient for fc2.weight: 0.2266286462545395\n",
      "Gradient for fc2.bias: 0.10924813896417618\n",
      "torch.Size([30]) tensor(-0.0020, grad_fn=<MaxBackward1>) tensor(-1.8429, grad_fn=<MinBackward1>)\n",
      "Episode 572, Loss: -0.05236644297838211\n",
      "Gradient for fc1.weight: 0.048836659640073776\n",
      "Gradient for fc1.bias: 0.14101603627204895\n",
      "Gradient for fc2.weight: 0.07841260731220245\n",
      "Gradient for fc2.bias: 0.041848644614219666\n",
      "torch.Size([33]) tensor(-0.0012, grad_fn=<MaxBackward1>) tensor(-1.3918, grad_fn=<MinBackward1>)\n",
      "Episode 573, Loss: 0.11772935092449188\n",
      "Gradient for fc1.weight: 0.025355909019708633\n",
      "Gradient for fc1.bias: 0.027341453358530998\n",
      "Gradient for fc2.weight: 0.03440771624445915\n",
      "Gradient for fc2.bias: 0.006289121229201555\n",
      "torch.Size([29]) tensor(-0.0059, grad_fn=<MaxBackward1>) tensor(-0.6064, grad_fn=<MinBackward1>)\n",
      "Episode 574, Loss: -0.08342239260673523\n",
      "Gradient for fc1.weight: 0.038526859134435654\n",
      "Gradient for fc1.bias: 0.03380291908979416\n",
      "Gradient for fc2.weight: 0.02896558679640293\n",
      "Gradient for fc2.bias: 0.007319080643355846\n",
      "torch.Size([67]) tensor(-0.0018, grad_fn=<MaxBackward1>) tensor(-1.9598, grad_fn=<MinBackward1>)\n",
      "Episode 575, Loss: 0.07648833096027374\n",
      "Gradient for fc1.weight: 0.007732224650681019\n",
      "Gradient for fc1.bias: 0.02761240489780903\n",
      "Gradient for fc2.weight: 0.017829246819019318\n",
      "Gradient for fc2.bias: 0.008844774216413498\n",
      "torch.Size([54]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.7096, grad_fn=<MinBackward1>)\n",
      "Episode 576, Loss: 0.08230467140674591\n",
      "Gradient for fc1.weight: 0.02919430285692215\n",
      "Gradient for fc1.bias: 0.08758366852998734\n",
      "Gradient for fc2.weight: 0.05649237334728241\n",
      "Gradient for fc2.bias: 0.019784681499004364\n",
      "torch.Size([37]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-3.4804, grad_fn=<MinBackward1>)\n",
      "Episode 577, Loss: 0.04809856787323952\n",
      "Gradient for fc1.weight: 0.05905120074748993\n",
      "Gradient for fc1.bias: 0.18474215269088745\n",
      "Gradient for fc2.weight: 0.16327674686908722\n",
      "Gradient for fc2.bias: 0.08192407339811325\n",
      "torch.Size([32]) tensor(-0.0012, grad_fn=<MaxBackward1>) tensor(-1.4215, grad_fn=<MinBackward1>)\n",
      "Episode 578, Loss: 0.09929093718528748\n",
      "Gradient for fc1.weight: 0.03657241538167\n",
      "Gradient for fc1.bias: 0.08260290324687958\n",
      "Gradient for fc2.weight: 0.06775303184986115\n",
      "Gradient for fc2.bias: 0.03234507516026497\n",
      "torch.Size([88]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-3.9750, grad_fn=<MinBackward1>)\n",
      "Episode 579, Loss: -0.09045367687940598\n",
      "Gradient for fc1.weight: 0.033814266324043274\n",
      "Gradient for fc1.bias: 0.04768766462802887\n",
      "Gradient for fc2.weight: 0.04009826108813286\n",
      "Gradient for fc2.bias: 0.01446948479861021\n",
      "torch.Size([43]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.9216, grad_fn=<MinBackward1>)\n",
      "Episode 580, Loss: 0.09037001430988312\n",
      "Gradient for fc1.weight: 0.02952335961163044\n",
      "Gradient for fc1.bias: 0.10962506383657455\n",
      "Gradient for fc2.weight: 0.08460449427366257\n",
      "Gradient for fc2.bias: 0.042896568775177\n",
      "torch.Size([25]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-1.6399, grad_fn=<MinBackward1>)\n",
      "Episode 581, Loss: -0.1390235275030136\n",
      "Gradient for fc1.weight: 0.14630857110023499\n",
      "Gradient for fc1.bias: 0.21047112345695496\n",
      "Gradient for fc2.weight: 0.1799149066209793\n",
      "Gradient for fc2.bias: 0.08239026367664337\n",
      "torch.Size([50]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-4.6304, grad_fn=<MinBackward1>)\n",
      "Episode 582, Loss: -0.04586443305015564\n",
      "Gradient for fc1.weight: 0.027446333318948746\n",
      "Gradient for fc1.bias: 0.06985761970281601\n",
      "Gradient for fc2.weight: 0.05205117538571358\n",
      "Gradient for fc2.bias: 0.02493942715227604\n",
      "torch.Size([33]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-1.7262, grad_fn=<MinBackward1>)\n",
      "Episode 583, Loss: -0.08557828515768051\n",
      "Gradient for fc1.weight: 0.029827028512954712\n",
      "Gradient for fc1.bias: 0.024805808439850807\n",
      "Gradient for fc2.weight: 0.026985514909029007\n",
      "Gradient for fc2.bias: 0.001378929242491722\n",
      "torch.Size([57]) tensor(-0.0012, grad_fn=<MaxBackward1>) tensor(-1.4119, grad_fn=<MinBackward1>)\n",
      "Episode 584, Loss: -0.029941337183117867\n",
      "Gradient for fc1.weight: 0.013043981976807117\n",
      "Gradient for fc1.bias: 0.06493032723665237\n",
      "Gradient for fc2.weight: 0.04803962633013725\n",
      "Gradient for fc2.bias: 0.02576068975031376\n",
      "torch.Size([35]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-1.7648, grad_fn=<MinBackward1>)\n",
      "Episode 585, Loss: 0.10141587257385254\n",
      "Gradient for fc1.weight: 0.041304413229227066\n",
      "Gradient for fc1.bias: 0.05295582115650177\n",
      "Gradient for fc2.weight: 0.0572650209069252\n",
      "Gradient for fc2.bias: 0.02113797701895237\n",
      "torch.Size([41]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.4983, grad_fn=<MinBackward1>)\n",
      "Episode 586, Loss: 0.008655091747641563\n",
      "Gradient for fc1.weight: 0.06446564942598343\n",
      "Gradient for fc1.bias: 0.05043888837099075\n",
      "Gradient for fc2.weight: 0.05922282859683037\n",
      "Gradient for fc2.bias: 0.007370807230472565\n",
      "torch.Size([23]) tensor(-0.0040, grad_fn=<MaxBackward1>) tensor(-1.1218, grad_fn=<MinBackward1>)\n",
      "Episode 587, Loss: -0.15739178657531738\n",
      "Gradient for fc1.weight: 0.1725989729166031\n",
      "Gradient for fc1.bias: 0.22293700277805328\n",
      "Gradient for fc2.weight: 0.19811664521694183\n",
      "Gradient for fc2.bias: 0.0868629515171051\n",
      "torch.Size([61]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-4.1556, grad_fn=<MinBackward1>)\n",
      "Episode 588, Loss: -0.09843135625123978\n",
      "Gradient for fc1.weight: 0.041904009878635406\n",
      "Gradient for fc1.bias: 0.11233507096767426\n",
      "Gradient for fc2.weight: 0.09518161416053772\n",
      "Gradient for fc2.bias: 0.04905612766742706\n",
      "torch.Size([35]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-1.4379, grad_fn=<MinBackward1>)\n",
      "Episode 589, Loss: -0.1396794319152832\n",
      "Gradient for fc1.weight: 0.05310361459851265\n",
      "Gradient for fc1.bias: 0.03605595976114273\n",
      "Gradient for fc2.weight: 0.050697628408670425\n",
      "Gradient for fc2.bias: 0.0022798425052314997\n",
      "torch.Size([91]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-1.5424, grad_fn=<MinBackward1>)\n",
      "Episode 590, Loss: 0.04244086891412735\n",
      "Gradient for fc1.weight: 0.0185307078063488\n",
      "Gradient for fc1.bias: 0.0748959481716156\n",
      "Gradient for fc2.weight: 0.06049186363816261\n",
      "Gradient for fc2.bias: 0.031656477600336075\n",
      "torch.Size([66]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-4.5190, grad_fn=<MinBackward1>)\n",
      "Episode 591, Loss: 0.045354414731264114\n",
      "Gradient for fc1.weight: 0.04011659696698189\n",
      "Gradient for fc1.bias: 0.17321191728115082\n",
      "Gradient for fc2.weight: 0.13225506246089935\n",
      "Gradient for fc2.bias: 0.07309099286794662\n",
      "torch.Size([32]) tensor(-0.0072, grad_fn=<MaxBackward1>) tensor(-0.6461, grad_fn=<MinBackward1>)\n",
      "Episode 592, Loss: -0.06499486416578293\n",
      "Gradient for fc1.weight: 0.04167783260345459\n",
      "Gradient for fc1.bias: 0.10733141005039215\n",
      "Gradient for fc2.weight: 0.07558956742286682\n",
      "Gradient for fc2.bias: 0.041216179728507996\n",
      "torch.Size([54]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.3836, grad_fn=<MinBackward1>)\n",
      "Episode 593, Loss: -0.14255039393901825\n",
      "Gradient for fc1.weight: 0.02053024247288704\n",
      "Gradient for fc1.bias: 0.06910455971956253\n",
      "Gradient for fc2.weight: 0.05195460096001625\n",
      "Gradient for fc2.bias: 0.022464631125330925\n",
      "torch.Size([47]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-5.0805, grad_fn=<MinBackward1>)\n",
      "Episode 594, Loss: -0.060210876166820526\n",
      "Gradient for fc1.weight: 0.04605017229914665\n",
      "Gradient for fc1.bias: 0.04323142394423485\n",
      "Gradient for fc2.weight: 0.05183732882142067\n",
      "Gradient for fc2.bias: 0.007794019300490618\n",
      "torch.Size([36]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-1.3471, grad_fn=<MinBackward1>)\n",
      "Episode 595, Loss: -0.07732976227998734\n",
      "Gradient for fc1.weight: 0.10315477102994919\n",
      "Gradient for fc1.bias: 0.22785359621047974\n",
      "Gradient for fc2.weight: 0.17067793011665344\n",
      "Gradient for fc2.bias: 0.08972989767789841\n",
      "torch.Size([42]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.0826, grad_fn=<MinBackward1>)\n",
      "Episode 596, Loss: -0.07370812445878983\n",
      "Gradient for fc1.weight: 0.015879733487963676\n",
      "Gradient for fc1.bias: 0.09274186939001083\n",
      "Gradient for fc2.weight: 0.06529747694730759\n",
      "Gradient for fc2.bias: 0.03444710373878479\n",
      "torch.Size([39]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.1811, grad_fn=<MinBackward1>)\n",
      "Episode 597, Loss: -0.042178865522146225\n",
      "Gradient for fc1.weight: 0.042080365121364594\n",
      "Gradient for fc1.bias: 0.09213371574878693\n",
      "Gradient for fc2.weight: 0.07750868052244186\n",
      "Gradient for fc2.bias: 0.03977212682366371\n",
      "torch.Size([34]) tensor(-0.0020, grad_fn=<MaxBackward1>) tensor(-1.3812, grad_fn=<MinBackward1>)\n",
      "Episode 598, Loss: 0.0418514683842659\n",
      "Gradient for fc1.weight: 0.020356547087430954\n",
      "Gradient for fc1.bias: 0.07643575221300125\n",
      "Gradient for fc2.weight: 0.05692499503493309\n",
      "Gradient for fc2.bias: 0.02894599735736847\n",
      "torch.Size([38]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-2.5272, grad_fn=<MinBackward1>)\n",
      "Episode 599, Loss: -0.2408997118473053\n",
      "Gradient for fc1.weight: 0.18985357880592346\n",
      "Gradient for fc1.bias: 0.28096744418144226\n",
      "Gradient for fc2.weight: 0.2575279474258423\n",
      "Gradient for fc2.bias: 0.1136617660522461\n",
      "torch.Size([35]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-3.6044, grad_fn=<MinBackward1>)\n",
      "Episode 600, Loss: 0.11433188617229462\n",
      "Gradient for fc1.weight: 0.021056825295090675\n",
      "Gradient for fc1.bias: 0.20460446178913116\n",
      "Gradient for fc2.weight: 0.15419898927211761\n",
      "Gradient for fc2.bias: 0.07847956568002701\n",
      "Running reward: 34.64172685820469\n",
      "torch.Size([98]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.7676, grad_fn=<MinBackward1>)\n",
      "Episode 601, Loss: -0.08943694829940796\n",
      "Gradient for fc1.weight: 0.011969435960054398\n",
      "Gradient for fc1.bias: 0.037002693861722946\n",
      "Gradient for fc2.weight: 0.028394335880875587\n",
      "Gradient for fc2.bias: 0.012782152742147446\n",
      "torch.Size([38]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-1.5693, grad_fn=<MinBackward1>)\n",
      "Episode 602, Loss: 0.050749558955430984\n",
      "Gradient for fc1.weight: 0.03208071365952492\n",
      "Gradient for fc1.bias: 0.1285313069820404\n",
      "Gradient for fc2.weight: 0.08999063074588776\n",
      "Gradient for fc2.bias: 0.05030994117259979\n",
      "torch.Size([49]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-1.6303, grad_fn=<MinBackward1>)\n",
      "Episode 603, Loss: 0.09333699941635132\n",
      "Gradient for fc1.weight: 0.010022463276982307\n",
      "Gradient for fc1.bias: 0.0204610638320446\n",
      "Gradient for fc2.weight: 0.02055368386209011\n",
      "Gradient for fc2.bias: 0.0037072296254336834\n",
      "torch.Size([38]) tensor(-0.0014, grad_fn=<MaxBackward1>) tensor(-1.3351, grad_fn=<MinBackward1>)\n",
      "Episode 604, Loss: -0.004312355071306229\n",
      "Gradient for fc1.weight: 0.016373906284570694\n",
      "Gradient for fc1.bias: 0.07539784163236618\n",
      "Gradient for fc2.weight: 0.05334414914250374\n",
      "Gradient for fc2.bias: 0.027225254103541374\n",
      "torch.Size([31]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.8807, grad_fn=<MinBackward1>)\n",
      "Episode 605, Loss: 0.10320276021957397\n",
      "Gradient for fc1.weight: 0.01369393803179264\n",
      "Gradient for fc1.bias: 0.11861493438482285\n",
      "Gradient for fc2.weight: 0.08108459413051605\n",
      "Gradient for fc2.bias: 0.04364016279578209\n",
      "torch.Size([40]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-3.3014, grad_fn=<MinBackward1>)\n",
      "Episode 606, Loss: 0.01122253853827715\n",
      "Gradient for fc1.weight: 0.014537529088556767\n",
      "Gradient for fc1.bias: 0.027845943346619606\n",
      "Gradient for fc2.weight: 0.018007822334766388\n",
      "Gradient for fc2.bias: 0.006793548818677664\n",
      "torch.Size([49]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-2.2442, grad_fn=<MinBackward1>)\n",
      "Episode 607, Loss: -0.08989287167787552\n",
      "Gradient for fc1.weight: 0.06537311524152756\n",
      "Gradient for fc1.bias: 0.16954703629016876\n",
      "Gradient for fc2.weight: 0.12813009321689606\n",
      "Gradient for fc2.bias: 0.06566227227449417\n",
      "torch.Size([39]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.8715, grad_fn=<MinBackward1>)\n",
      "Episode 608, Loss: 0.041013747453689575\n",
      "Gradient for fc1.weight: 0.0501568540930748\n",
      "Gradient for fc1.bias: 0.049638692289590836\n",
      "Gradient for fc2.weight: 0.05088145285844803\n",
      "Gradient for fc2.bias: 0.0105061624199152\n",
      "torch.Size([81]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-4.1742, grad_fn=<MinBackward1>)\n",
      "Episode 609, Loss: 0.03307672217488289\n",
      "Gradient for fc1.weight: 0.015674281865358353\n",
      "Gradient for fc1.bias: 0.08367224782705307\n",
      "Gradient for fc2.weight: 0.05551823973655701\n",
      "Gradient for fc2.bias: 0.0314529687166214\n",
      "torch.Size([59]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.2932, grad_fn=<MinBackward1>)\n",
      "Episode 610, Loss: 0.10935384780168533\n",
      "Gradient for fc1.weight: 0.02684067003428936\n",
      "Gradient for fc1.bias: 0.08592312037944794\n",
      "Gradient for fc2.weight: 0.062298886477947235\n",
      "Gradient for fc2.bias: 0.03290802240371704\n",
      "torch.Size([37]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-2.2373, grad_fn=<MinBackward1>)\n",
      "Episode 611, Loss: -0.08263374119997025\n",
      "Gradient for fc1.weight: 0.031892891973257065\n",
      "Gradient for fc1.bias: 0.036577481776475906\n",
      "Gradient for fc2.weight: 0.03462720289826393\n",
      "Gradient for fc2.bias: 0.0052100117318332195\n",
      "torch.Size([35]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.1747, grad_fn=<MinBackward1>)\n",
      "Episode 612, Loss: 0.21884329617023468\n",
      "Gradient for fc1.weight: 0.019165415316820145\n",
      "Gradient for fc1.bias: 0.3273514211177826\n",
      "Gradient for fc2.weight: 0.2288893163204193\n",
      "Gradient for fc2.bias: 0.12423461675643921\n",
      "torch.Size([29]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-1.7123, grad_fn=<MinBackward1>)\n",
      "Episode 613, Loss: -0.04563837870955467\n",
      "Gradient for fc1.weight: 0.042656250298023224\n",
      "Gradient for fc1.bias: 0.049650028347969055\n",
      "Gradient for fc2.weight: 0.04771364852786064\n",
      "Gradient for fc2.bias: 0.01639377512037754\n",
      "torch.Size([107]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-1.8543, grad_fn=<MinBackward1>)\n",
      "Episode 614, Loss: 0.05644381046295166\n",
      "Gradient for fc1.weight: 0.027723167091608047\n",
      "Gradient for fc1.bias: 0.13994811475276947\n",
      "Gradient for fc2.weight: 0.0962267592549324\n",
      "Gradient for fc2.bias: 0.05426652356982231\n",
      "torch.Size([30]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-2.3277, grad_fn=<MinBackward1>)\n",
      "Episode 615, Loss: 0.018810689449310303\n",
      "Gradient for fc1.weight: 0.06585345417261124\n",
      "Gradient for fc1.bias: 0.06126946955919266\n",
      "Gradient for fc2.weight: 0.06514184176921844\n",
      "Gradient for fc2.bias: 0.019947323948144913\n",
      "torch.Size([33]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-1.1735, grad_fn=<MinBackward1>)\n",
      "Episode 616, Loss: 0.13543002307415009\n",
      "Gradient for fc1.weight: 0.041304249316453934\n",
      "Gradient for fc1.bias: 0.2085142880678177\n",
      "Gradient for fc2.weight: 0.146968811750412\n",
      "Gradient for fc2.bias: 0.08083262294530869\n",
      "torch.Size([103]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.0290, grad_fn=<MinBackward1>)\n",
      "Episode 617, Loss: 0.03620385378599167\n",
      "Gradient for fc1.weight: 0.023652048781514168\n",
      "Gradient for fc1.bias: 0.12396863847970963\n",
      "Gradient for fc2.weight: 0.08556733280420303\n",
      "Gradient for fc2.bias: 0.04703276976943016\n",
      "torch.Size([49]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-2.6485, grad_fn=<MinBackward1>)\n",
      "Episode 618, Loss: 0.0017676332499831915\n",
      "Gradient for fc1.weight: 0.03953162580728531\n",
      "Gradient for fc1.bias: 0.06484945118427277\n",
      "Gradient for fc2.weight: 0.053438037633895874\n",
      "Gradient for fc2.bias: 0.02534196712076664\n",
      "torch.Size([46]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.4879, grad_fn=<MinBackward1>)\n",
      "Episode 619, Loss: -0.013225197792053223\n",
      "Gradient for fc1.weight: 0.11489128321409225\n",
      "Gradient for fc1.bias: 0.12325593829154968\n",
      "Gradient for fc2.weight: 0.14274650812149048\n",
      "Gradient for fc2.bias: 0.048361629247665405\n",
      "torch.Size([36]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.7479, grad_fn=<MinBackward1>)\n",
      "Episode 620, Loss: 0.04572194069623947\n",
      "Gradient for fc1.weight: 0.03204415366053581\n",
      "Gradient for fc1.bias: 0.12108106166124344\n",
      "Gradient for fc2.weight: 0.09416632354259491\n",
      "Gradient for fc2.bias: 0.04743798077106476\n",
      "torch.Size([81]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-4.4657, grad_fn=<MinBackward1>)\n",
      "Episode 621, Loss: -0.039468903094530106\n",
      "Gradient for fc1.weight: 0.017032403498888016\n",
      "Gradient for fc1.bias: 0.08973222225904465\n",
      "Gradient for fc2.weight: 0.0687541514635086\n",
      "Gradient for fc2.bias: 0.03552114591002464\n",
      "torch.Size([65]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-1.9746, grad_fn=<MinBackward1>)\n",
      "Episode 622, Loss: 0.021423280239105225\n",
      "Gradient for fc1.weight: 0.05307651311159134\n",
      "Gradient for fc1.bias: 0.21501699090003967\n",
      "Gradient for fc2.weight: 0.15489377081394196\n",
      "Gradient for fc2.bias: 0.08338269591331482\n",
      "torch.Size([41]) tensor(-3.7134e-05, grad_fn=<MaxBackward1>) tensor(-4.9102, grad_fn=<MinBackward1>)\n",
      "Episode 623, Loss: -0.09618311375379562\n",
      "Gradient for fc1.weight: 0.07578373700380325\n",
      "Gradient for fc1.bias: 0.22822622954845428\n",
      "Gradient for fc2.weight: 0.18370993435382843\n",
      "Gradient for fc2.bias: 0.0870438888669014\n",
      "torch.Size([37]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.8980, grad_fn=<MinBackward1>)\n",
      "Episode 624, Loss: 0.08809138834476471\n",
      "Gradient for fc1.weight: 0.01540177408605814\n",
      "Gradient for fc1.bias: 0.18321314454078674\n",
      "Gradient for fc2.weight: 0.13013868033885956\n",
      "Gradient for fc2.bias: 0.07069512456655502\n",
      "torch.Size([40]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-1.1360, grad_fn=<MinBackward1>)\n",
      "Episode 625, Loss: 0.1150110512971878\n",
      "Gradient for fc1.weight: 0.020873982459306717\n",
      "Gradient for fc1.bias: 0.13851751387119293\n",
      "Gradient for fc2.weight: 0.09998820722103119\n",
      "Gradient for fc2.bias: 0.053168874233961105\n",
      "torch.Size([57]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-1.3922, grad_fn=<MinBackward1>)\n",
      "Episode 626, Loss: 0.05646586790680885\n",
      "Gradient for fc1.weight: 0.01113074366003275\n",
      "Gradient for fc1.bias: 0.18553952872753143\n",
      "Gradient for fc2.weight: 0.12874072790145874\n",
      "Gradient for fc2.bias: 0.07040988653898239\n",
      "torch.Size([90]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-4.6383, grad_fn=<MinBackward1>)\n",
      "Episode 627, Loss: 0.06179377809166908\n",
      "Gradient for fc1.weight: 0.017700612545013428\n",
      "Gradient for fc1.bias: 0.03435283154249191\n",
      "Gradient for fc2.weight: 0.03399944305419922\n",
      "Gradient for fc2.bias: 0.01069655455648899\n",
      "torch.Size([71]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-2.0536, grad_fn=<MinBackward1>)\n",
      "Episode 628, Loss: 0.026523921638727188\n",
      "Gradient for fc1.weight: 0.06952133029699326\n",
      "Gradient for fc1.bias: 0.12280262261629105\n",
      "Gradient for fc2.weight: 0.10518942028284073\n",
      "Gradient for fc2.bias: 0.04882936179637909\n",
      "torch.Size([65]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.1899, grad_fn=<MinBackward1>)\n",
      "Episode 629, Loss: 0.015221337787806988\n",
      "Gradient for fc1.weight: 0.0353013277053833\n",
      "Gradient for fc1.bias: 0.036682937294244766\n",
      "Gradient for fc2.weight: 0.041114870458841324\n",
      "Gradient for fc2.bias: 0.01224448997527361\n",
      "torch.Size([66]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.9030, grad_fn=<MinBackward1>)\n",
      "Episode 630, Loss: 0.04413823038339615\n",
      "Gradient for fc1.weight: 0.038447365164756775\n",
      "Gradient for fc1.bias: 0.1445368528366089\n",
      "Gradient for fc2.weight: 0.10677623748779297\n",
      "Gradient for fc2.bias: 0.05453609302639961\n",
      "torch.Size([44]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.7499, grad_fn=<MinBackward1>)\n",
      "Episode 631, Loss: 0.11756878346204758\n",
      "Gradient for fc1.weight: 0.0761260911822319\n",
      "Gradient for fc1.bias: 0.37809333205223083\n",
      "Gradient for fc2.weight: 0.2861022651195526\n",
      "Gradient for fc2.bias: 0.14903445541858673\n",
      "torch.Size([84]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.7084, grad_fn=<MinBackward1>)\n",
      "Episode 632, Loss: -0.027700558304786682\n",
      "Gradient for fc1.weight: 0.03548404946923256\n",
      "Gradient for fc1.bias: 0.09445250779390335\n",
      "Gradient for fc2.weight: 0.07321878522634506\n",
      "Gradient for fc2.bias: 0.03567807376384735\n",
      "torch.Size([38]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-1.4826, grad_fn=<MinBackward1>)\n",
      "Episode 633, Loss: 0.09161596745252609\n",
      "Gradient for fc1.weight: 0.04999687522649765\n",
      "Gradient for fc1.bias: 0.13418568670749664\n",
      "Gradient for fc2.weight: 0.11708007752895355\n",
      "Gradient for fc2.bias: 0.05524332448840141\n",
      "torch.Size([39]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.6130, grad_fn=<MinBackward1>)\n",
      "Episode 634, Loss: 0.05500498041510582\n",
      "Gradient for fc1.weight: 0.03786545619368553\n",
      "Gradient for fc1.bias: 0.19060246646404266\n",
      "Gradient for fc2.weight: 0.12374557554721832\n",
      "Gradient for fc2.bias: 0.06961429864168167\n",
      "torch.Size([60]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.4141, grad_fn=<MinBackward1>)\n",
      "Episode 635, Loss: 0.0758809745311737\n",
      "Gradient for fc1.weight: 0.04551156610250473\n",
      "Gradient for fc1.bias: 0.08497447520494461\n",
      "Gradient for fc2.weight: 0.06637975573539734\n",
      "Gradient for fc2.bias: 0.03235943987965584\n",
      "torch.Size([44]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.2563, grad_fn=<MinBackward1>)\n",
      "Episode 636, Loss: 0.034128062427043915\n",
      "Gradient for fc1.weight: 0.014739100821316242\n",
      "Gradient for fc1.bias: 0.02263379655778408\n",
      "Gradient for fc2.weight: 0.010463949292898178\n",
      "Gradient for fc2.bias: 0.0028104770462960005\n",
      "torch.Size([36]) tensor(-8.6192e-05, grad_fn=<MaxBackward1>) tensor(-3.3641, grad_fn=<MinBackward1>)\n",
      "Episode 637, Loss: -0.017091214656829834\n",
      "Gradient for fc1.weight: 0.045855507254600525\n",
      "Gradient for fc1.bias: 0.06445016711950302\n",
      "Gradient for fc2.weight: 0.06340643018484116\n",
      "Gradient for fc2.bias: 0.02254183404147625\n",
      "torch.Size([53]) tensor(-1.6391e-05, grad_fn=<MaxBackward1>) tensor(-6.3665, grad_fn=<MinBackward1>)\n",
      "Episode 638, Loss: 0.1751783788204193\n",
      "Gradient for fc1.weight: 0.021959327161312103\n",
      "Gradient for fc1.bias: 0.26145264506340027\n",
      "Gradient for fc2.weight: 0.18333405256271362\n",
      "Gradient for fc2.bias: 0.10048111528158188\n",
      "torch.Size([86]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-3.6511, grad_fn=<MinBackward1>)\n",
      "Episode 639, Loss: 0.024791240692138672\n",
      "Gradient for fc1.weight: 0.03844093531370163\n",
      "Gradient for fc1.bias: 0.0813138484954834\n",
      "Gradient for fc2.weight: 0.06335913389921188\n",
      "Gradient for fc2.bias: 0.032284341752529144\n",
      "torch.Size([77]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.0431, grad_fn=<MinBackward1>)\n",
      "Episode 640, Loss: 0.039516475051641464\n",
      "Gradient for fc1.weight: 0.021808302029967308\n",
      "Gradient for fc1.bias: 0.03913280740380287\n",
      "Gradient for fc2.weight: 0.03156470134854317\n",
      "Gradient for fc2.bias: 0.015323683619499207\n",
      "torch.Size([42]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.5504, grad_fn=<MinBackward1>)\n",
      "Episode 641, Loss: -0.10404857248067856\n",
      "Gradient for fc1.weight: 0.11573786288499832\n",
      "Gradient for fc1.bias: 0.16238003969192505\n",
      "Gradient for fc2.weight: 0.18176166713237762\n",
      "Gradient for fc2.bias: 0.07050181180238724\n",
      "torch.Size([91]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.8193, grad_fn=<MinBackward1>)\n",
      "Episode 642, Loss: 0.05119248852133751\n",
      "Gradient for fc1.weight: 0.017708661034703255\n",
      "Gradient for fc1.bias: 0.07420101016759872\n",
      "Gradient for fc2.weight: 0.0496114157140255\n",
      "Gradient for fc2.bias: 0.02673749066889286\n",
      "torch.Size([39]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.2103, grad_fn=<MinBackward1>)\n",
      "Episode 643, Loss: -0.05882057920098305\n",
      "Gradient for fc1.weight: 0.04170558974146843\n",
      "Gradient for fc1.bias: 0.2007283866405487\n",
      "Gradient for fc2.weight: 0.14273738861083984\n",
      "Gradient for fc2.bias: 0.07925541698932648\n",
      "torch.Size([54]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.6974, grad_fn=<MinBackward1>)\n",
      "Episode 644, Loss: 0.04319591447710991\n",
      "Gradient for fc1.weight: 0.060462042689323425\n",
      "Gradient for fc1.bias: 0.17151129245758057\n",
      "Gradient for fc2.weight: 0.14224553108215332\n",
      "Gradient for fc2.bias: 0.07007352262735367\n",
      "torch.Size([45]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.3307, grad_fn=<MinBackward1>)\n",
      "Episode 645, Loss: 0.047406960278749466\n",
      "Gradient for fc1.weight: 0.07392586022615433\n",
      "Gradient for fc1.bias: 0.07974015921354294\n",
      "Gradient for fc2.weight: 0.08982443064451218\n",
      "Gradient for fc2.bias: 0.03233708068728447\n",
      "torch.Size([57]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.1869, grad_fn=<MinBackward1>)\n",
      "Episode 646, Loss: -0.01657659001648426\n",
      "Gradient for fc1.weight: 0.029286997392773628\n",
      "Gradient for fc1.bias: 0.10212478041648865\n",
      "Gradient for fc2.weight: 0.07701289653778076\n",
      "Gradient for fc2.bias: 0.039481017738580704\n",
      "torch.Size([91]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.2864, grad_fn=<MinBackward1>)\n",
      "Episode 647, Loss: 0.05148795247077942\n",
      "Gradient for fc1.weight: 0.019890714436769485\n",
      "Gradient for fc1.bias: 0.08567675948143005\n",
      "Gradient for fc2.weight: 0.058487631380558014\n",
      "Gradient for fc2.bias: 0.03079116903245449\n",
      "torch.Size([44]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-1.8564, grad_fn=<MinBackward1>)\n",
      "Episode 648, Loss: 0.04435219615697861\n",
      "Gradient for fc1.weight: 0.022813431918621063\n",
      "Gradient for fc1.bias: 0.04452408477663994\n",
      "Gradient for fc2.weight: 0.03359544649720192\n",
      "Gradient for fc2.bias: 0.014942898415029049\n",
      "torch.Size([54]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-1.3197, grad_fn=<MinBackward1>)\n",
      "Episode 649, Loss: -0.019758081063628197\n",
      "Gradient for fc1.weight: 0.019387654960155487\n",
      "Gradient for fc1.bias: 0.09249632805585861\n",
      "Gradient for fc2.weight: 0.04782694950699806\n",
      "Gradient for fc2.bias: 0.028444994240999222\n",
      "torch.Size([56]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.8061, grad_fn=<MinBackward1>)\n",
      "Episode 650, Loss: 0.13257457315921783\n",
      "Gradient for fc1.weight: 0.017699217423796654\n",
      "Gradient for fc1.bias: 0.3872044086456299\n",
      "Gradient for fc2.weight: 0.2571370303630829\n",
      "Gradient for fc2.bias: 0.14439165592193604\n",
      "torch.Size([42]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.0945, grad_fn=<MinBackward1>)\n",
      "Episode 651, Loss: 0.06317050009965897\n",
      "Gradient for fc1.weight: 0.06164143234491348\n",
      "Gradient for fc1.bias: 0.11375352740287781\n",
      "Gradient for fc2.weight: 0.09133343398571014\n",
      "Gradient for fc2.bias: 0.043063290417194366\n",
      "torch.Size([71]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-5.1962, grad_fn=<MinBackward1>)\n",
      "Episode 652, Loss: -0.15723827481269836\n",
      "Gradient for fc1.weight: 0.08737736195325851\n",
      "Gradient for fc1.bias: 0.07140375673770905\n",
      "Gradient for fc2.weight: 0.1107013076543808\n",
      "Gradient for fc2.bias: 0.033564046025276184\n",
      "torch.Size([47]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.6781, grad_fn=<MinBackward1>)\n",
      "Episode 653, Loss: 0.0369168259203434\n",
      "Gradient for fc1.weight: 0.029033876955509186\n",
      "Gradient for fc1.bias: 0.09764296561479568\n",
      "Gradient for fc2.weight: 0.06587529182434082\n",
      "Gradient for fc2.bias: 0.03763693943619728\n",
      "torch.Size([39]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-0.9781, grad_fn=<MinBackward1>)\n",
      "Episode 654, Loss: 0.13402268290519714\n",
      "Gradient for fc1.weight: 0.03259798139333725\n",
      "Gradient for fc1.bias: 0.2211080640554428\n",
      "Gradient for fc2.weight: 0.15742024779319763\n",
      "Gradient for fc2.bias: 0.0865609422326088\n",
      "torch.Size([36]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.6510, grad_fn=<MinBackward1>)\n",
      "Episode 655, Loss: 0.02096417546272278\n",
      "Gradient for fc1.weight: 0.053410790860652924\n",
      "Gradient for fc1.bias: 0.1613943874835968\n",
      "Gradient for fc2.weight: 0.12734255194664001\n",
      "Gradient for fc2.bias: 0.06380981206893921\n",
      "torch.Size([55]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.9494, grad_fn=<MinBackward1>)\n",
      "Episode 656, Loss: 0.06221417337656021\n",
      "Gradient for fc1.weight: 0.03795572742819786\n",
      "Gradient for fc1.bias: 0.056931234896183014\n",
      "Gradient for fc2.weight: 0.052307743579149246\n",
      "Gradient for fc2.bias: 0.02193208411335945\n",
      "torch.Size([33]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.8230, grad_fn=<MinBackward1>)\n",
      "Episode 657, Loss: 0.04601867124438286\n",
      "Gradient for fc1.weight: 0.015990544110536575\n",
      "Gradient for fc1.bias: 0.058111920952796936\n",
      "Gradient for fc2.weight: 0.04241866618394852\n",
      "Gradient for fc2.bias: 0.02350032329559326\n",
      "torch.Size([32]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.8459, grad_fn=<MinBackward1>)\n",
      "Episode 658, Loss: 0.032344263046979904\n",
      "Gradient for fc1.weight: 0.02730574645102024\n",
      "Gradient for fc1.bias: 0.0945303812623024\n",
      "Gradient for fc2.weight: 0.07396222651004791\n",
      "Gradient for fc2.bias: 0.03735307231545448\n",
      "torch.Size([54]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.9181, grad_fn=<MinBackward1>)\n",
      "Episode 659, Loss: 0.05713287368416786\n",
      "Gradient for fc1.weight: 0.023586280643939972\n",
      "Gradient for fc1.bias: 0.05706450343132019\n",
      "Gradient for fc2.weight: 0.055127523839473724\n",
      "Gradient for fc2.bias: 0.02071153186261654\n",
      "torch.Size([67]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-4.4143, grad_fn=<MinBackward1>)\n",
      "Episode 660, Loss: -0.03808540478348732\n",
      "Gradient for fc1.weight: 0.013598362915217876\n",
      "Gradient for fc1.bias: 0.021178480237722397\n",
      "Gradient for fc2.weight: 0.020622320473194122\n",
      "Gradient for fc2.bias: 0.002524509560316801\n",
      "torch.Size([38]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-1.3886, grad_fn=<MinBackward1>)\n",
      "Episode 661, Loss: 0.0832754597067833\n",
      "Gradient for fc1.weight: 0.025274954736232758\n",
      "Gradient for fc1.bias: 0.09982402622699738\n",
      "Gradient for fc2.weight: 0.07505510002374649\n",
      "Gradient for fc2.bias: 0.03790993243455887\n",
      "torch.Size([41]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.7202, grad_fn=<MinBackward1>)\n",
      "Episode 662, Loss: 0.12013314664363861\n",
      "Gradient for fc1.weight: 0.03552380949258804\n",
      "Gradient for fc1.bias: 0.06942915916442871\n",
      "Gradient for fc2.weight: 0.06751858443021774\n",
      "Gradient for fc2.bias: 0.029024576768279076\n",
      "torch.Size([31]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.8268, grad_fn=<MinBackward1>)\n",
      "Episode 663, Loss: 0.00035299721639603376\n",
      "Gradient for fc1.weight: 0.021087361499667168\n",
      "Gradient for fc1.bias: 0.05709438398480415\n",
      "Gradient for fc2.weight: 0.045908086001873016\n",
      "Gradient for fc2.bias: 0.022732265293598175\n",
      "torch.Size([45]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.3831, grad_fn=<MinBackward1>)\n",
      "Episode 664, Loss: -0.06179306283593178\n",
      "Gradient for fc1.weight: 0.02387998066842556\n",
      "Gradient for fc1.bias: 0.10757356882095337\n",
      "Gradient for fc2.weight: 0.09178697317838669\n",
      "Gradient for fc2.bias: 0.04368199035525322\n",
      "torch.Size([27]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.8778, grad_fn=<MinBackward1>)\n",
      "Episode 665, Loss: 0.17977546155452728\n",
      "Gradient for fc1.weight: 0.026129605248570442\n",
      "Gradient for fc1.bias: 0.10900267958641052\n",
      "Gradient for fc2.weight: 0.09664212912321091\n",
      "Gradient for fc2.bias: 0.04180986061692238\n",
      "torch.Size([65]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.5709, grad_fn=<MinBackward1>)\n",
      "Episode 666, Loss: 0.0918051153421402\n",
      "Gradient for fc1.weight: 0.0493491105735302\n",
      "Gradient for fc1.bias: 0.31239792704582214\n",
      "Gradient for fc2.weight: 0.23099660873413086\n",
      "Gradient for fc2.bias: 0.12212072312831879\n",
      "torch.Size([39]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.7948, grad_fn=<MinBackward1>)\n",
      "Episode 667, Loss: 0.06385039538145065\n",
      "Gradient for fc1.weight: 0.037518084049224854\n",
      "Gradient for fc1.bias: 0.02576320804655552\n",
      "Gradient for fc2.weight: 0.03195958212018013\n",
      "Gradient for fc2.bias: 0.0034924866631627083\n",
      "torch.Size([107]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.7713, grad_fn=<MinBackward1>)\n",
      "Episode 668, Loss: -0.009208218194544315\n",
      "Gradient for fc1.weight: 0.010364093817770481\n",
      "Gradient for fc1.bias: 0.03965093567967415\n",
      "Gradient for fc2.weight: 0.03772677481174469\n",
      "Gradient for fc2.bias: 0.014979240484535694\n",
      "torch.Size([36]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.8075, grad_fn=<MinBackward1>)\n",
      "Episode 669, Loss: -0.001611222862266004\n",
      "Gradient for fc1.weight: 0.10346540063619614\n",
      "Gradient for fc1.bias: 0.1566237211227417\n",
      "Gradient for fc2.weight: 0.14329539239406586\n",
      "Gradient for fc2.bias: 0.06116778403520584\n",
      "torch.Size([46]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.8150, grad_fn=<MinBackward1>)\n",
      "Episode 670, Loss: -0.004474261309951544\n",
      "Gradient for fc1.weight: 0.01681564375758171\n",
      "Gradient for fc1.bias: 0.11567776650190353\n",
      "Gradient for fc2.weight: 0.09257517009973526\n",
      "Gradient for fc2.bias: 0.047047074884176254\n",
      "torch.Size([36]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.9888, grad_fn=<MinBackward1>)\n",
      "Episode 671, Loss: 0.17355011403560638\n",
      "Gradient for fc1.weight: 0.060918137431144714\n",
      "Gradient for fc1.bias: 0.305749773979187\n",
      "Gradient for fc2.weight: 0.22714121639728546\n",
      "Gradient for fc2.bias: 0.11995784193277359\n",
      "torch.Size([39]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.9387, grad_fn=<MinBackward1>)\n",
      "Episode 672, Loss: 0.09053074568510056\n",
      "Gradient for fc1.weight: 0.035916756838560104\n",
      "Gradient for fc1.bias: 0.02375001274049282\n",
      "Gradient for fc2.weight: 0.05100669711828232\n",
      "Gradient for fc2.bias: 0.0037929024547338486\n",
      "torch.Size([37]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.5712, grad_fn=<MinBackward1>)\n",
      "Episode 673, Loss: 0.0380961075425148\n",
      "Gradient for fc1.weight: 0.031492751091718674\n",
      "Gradient for fc1.bias: 0.04905599728226662\n",
      "Gradient for fc2.weight: 0.046352729201316833\n",
      "Gradient for fc2.bias: 0.019851764664053917\n",
      "torch.Size([98]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-3.0240, grad_fn=<MinBackward1>)\n",
      "Episode 674, Loss: 0.024364719167351723\n",
      "Gradient for fc1.weight: 0.008523675613105297\n",
      "Gradient for fc1.bias: 0.01011314895004034\n",
      "Gradient for fc2.weight: 0.014050999656319618\n",
      "Gradient for fc2.bias: 0.0015099247684702277\n",
      "torch.Size([28]) tensor(-9.3703e-05, grad_fn=<MaxBackward1>) tensor(-5.2101, grad_fn=<MinBackward1>)\n",
      "Episode 675, Loss: 0.27479711174964905\n",
      "Gradient for fc1.weight: 0.11864359676837921\n",
      "Gradient for fc1.bias: 0.13355688750743866\n",
      "Gradient for fc2.weight: 0.1706855148077011\n",
      "Gradient for fc2.bias: 0.049999963492155075\n",
      "torch.Size([64]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.1951, grad_fn=<MinBackward1>)\n",
      "Episode 676, Loss: 0.0013977903872728348\n",
      "Gradient for fc1.weight: 0.0928497165441513\n",
      "Gradient for fc1.bias: 0.20634473860263824\n",
      "Gradient for fc2.weight: 0.1816094070672989\n",
      "Gradient for fc2.bias: 0.08764953166246414\n",
      "torch.Size([33]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.3074, grad_fn=<MinBackward1>)\n",
      "Episode 677, Loss: 0.07864949107170105\n",
      "Gradient for fc1.weight: 0.033437348902225494\n",
      "Gradient for fc1.bias: 0.1553240269422531\n",
      "Gradient for fc2.weight: 0.12049628049135208\n",
      "Gradient for fc2.bias: 0.06344137340784073\n",
      "torch.Size([61]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-1.6687, grad_fn=<MinBackward1>)\n",
      "Episode 678, Loss: 0.08119965344667435\n",
      "Gradient for fc1.weight: 0.0185101181268692\n",
      "Gradient for fc1.bias: 0.11359015852212906\n",
      "Gradient for fc2.weight: 0.08342350274324417\n",
      "Gradient for fc2.bias: 0.04538658261299133\n",
      "torch.Size([34]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.4302, grad_fn=<MinBackward1>)\n",
      "Episode 679, Loss: -0.04393278434872627\n",
      "Gradient for fc1.weight: 0.028050310909748077\n",
      "Gradient for fc1.bias: 0.11384301632642746\n",
      "Gradient for fc2.weight: 0.08870416134595871\n",
      "Gradient for fc2.bias: 0.044708237051963806\n",
      "torch.Size([61]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-2.1405, grad_fn=<MinBackward1>)\n",
      "Episode 680, Loss: 0.036677852272987366\n",
      "Gradient for fc1.weight: 0.020797397941350937\n",
      "Gradient for fc1.bias: 0.062057413160800934\n",
      "Gradient for fc2.weight: 0.04937291145324707\n",
      "Gradient for fc2.bias: 0.02515234984457493\n",
      "torch.Size([63]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.4765, grad_fn=<MinBackward1>)\n",
      "Episode 681, Loss: -0.1803489625453949\n",
      "Gradient for fc1.weight: 0.1063699722290039\n",
      "Gradient for fc1.bias: 0.2312752604484558\n",
      "Gradient for fc2.weight: 0.20900242030620575\n",
      "Gradient for fc2.bias: 0.1022663488984108\n",
      "torch.Size([54]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-3.8907, grad_fn=<MinBackward1>)\n",
      "Episode 682, Loss: -0.07070731371641159\n",
      "Gradient for fc1.weight: 0.019447583705186844\n",
      "Gradient for fc1.bias: 0.15775664150714874\n",
      "Gradient for fc2.weight: 0.12606054544448853\n",
      "Gradient for fc2.bias: 0.0704115480184555\n",
      "torch.Size([45]) tensor(-0.0010, grad_fn=<MaxBackward1>) tensor(-1.9939, grad_fn=<MinBackward1>)\n",
      "Episode 683, Loss: 0.10651429742574692\n",
      "Gradient for fc1.weight: 0.02570270374417305\n",
      "Gradient for fc1.bias: 0.02965237945318222\n",
      "Gradient for fc2.weight: 0.03711871802806854\n",
      "Gradient for fc2.bias: 0.010080086067318916\n",
      "torch.Size([36]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.7632, grad_fn=<MinBackward1>)\n",
      "Episode 684, Loss: 0.09689489006996155\n",
      "Gradient for fc1.weight: 0.037711337208747864\n",
      "Gradient for fc1.bias: 0.09926661103963852\n",
      "Gradient for fc2.weight: 0.0806783065199852\n",
      "Gradient for fc2.bias: 0.03699159994721413\n",
      "torch.Size([53]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-3.9090, grad_fn=<MinBackward1>)\n",
      "Episode 685, Loss: 0.03054868057370186\n",
      "Gradient for fc1.weight: 0.03153296187520027\n",
      "Gradient for fc1.bias: 0.024959638714790344\n",
      "Gradient for fc2.weight: 0.033443741500377655\n",
      "Gradient for fc2.bias: 0.0019466957310214639\n",
      "torch.Size([33]) tensor(-0.0018, grad_fn=<MaxBackward1>) tensor(-1.1677, grad_fn=<MinBackward1>)\n",
      "Episode 686, Loss: -0.019740361720323563\n",
      "Gradient for fc1.weight: 0.016919463872909546\n",
      "Gradient for fc1.bias: 0.0431903637945652\n",
      "Gradient for fc2.weight: 0.02361929602921009\n",
      "Gradient for fc2.bias: 0.01248429249972105\n",
      "torch.Size([43]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-3.3832, grad_fn=<MinBackward1>)\n",
      "Episode 687, Loss: 0.01554236002266407\n",
      "Gradient for fc1.weight: 0.019268713891506195\n",
      "Gradient for fc1.bias: 0.07540853321552277\n",
      "Gradient for fc2.weight: 0.06844893842935562\n",
      "Gradient for fc2.bias: 0.03522875905036926\n",
      "torch.Size([96]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-3.0407, grad_fn=<MinBackward1>)\n",
      "Episode 688, Loss: -0.029687395319342613\n",
      "Gradient for fc1.weight: 0.023064052686095238\n",
      "Gradient for fc1.bias: 0.16065004467964172\n",
      "Gradient for fc2.weight: 0.12217307090759277\n",
      "Gradient for fc2.bias: 0.06780330836772919\n",
      "torch.Size([76]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.7250, grad_fn=<MinBackward1>)\n",
      "Episode 689, Loss: 0.04457060992717743\n",
      "Gradient for fc1.weight: 0.02803669311106205\n",
      "Gradient for fc1.bias: 0.06823281943798065\n",
      "Gradient for fc2.weight: 0.06087852641940117\n",
      "Gradient for fc2.bias: 0.0295134074985981\n",
      "torch.Size([60]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-3.0619, grad_fn=<MinBackward1>)\n",
      "Episode 690, Loss: 0.0554235503077507\n",
      "Gradient for fc1.weight: 0.014500876888632774\n",
      "Gradient for fc1.bias: 0.08591814339160919\n",
      "Gradient for fc2.weight: 0.07453855872154236\n",
      "Gradient for fc2.bias: 0.03632309287786484\n",
      "torch.Size([55]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.4713, grad_fn=<MinBackward1>)\n",
      "Episode 691, Loss: -0.01949843019247055\n",
      "Gradient for fc1.weight: 0.02965882234275341\n",
      "Gradient for fc1.bias: 0.024115297943353653\n",
      "Gradient for fc2.weight: 0.036526743322610855\n",
      "Gradient for fc2.bias: 0.007295513991266489\n",
      "torch.Size([33]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-4.9481, grad_fn=<MinBackward1>)\n",
      "Episode 692, Loss: 0.16136513650417328\n",
      "Gradient for fc1.weight: 0.08873998373746872\n",
      "Gradient for fc1.bias: 0.2690710127353668\n",
      "Gradient for fc2.weight: 0.2700783908367157\n",
      "Gradient for fc2.bias: 0.13037315011024475\n",
      "torch.Size([35]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-2.6440, grad_fn=<MinBackward1>)\n",
      "Episode 693, Loss: 0.02356969378888607\n",
      "Gradient for fc1.weight: 0.08646722137928009\n",
      "Gradient for fc1.bias: 0.2001103311777115\n",
      "Gradient for fc2.weight: 0.17865709960460663\n",
      "Gradient for fc2.bias: 0.08803946524858475\n",
      "torch.Size([42]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-2.1166, grad_fn=<MinBackward1>)\n",
      "Episode 694, Loss: 0.06604760885238647\n",
      "Gradient for fc1.weight: 0.06870372593402863\n",
      "Gradient for fc1.bias: 0.0699647068977356\n",
      "Gradient for fc2.weight: 0.08651290088891983\n",
      "Gradient for fc2.bias: 0.02997707389295101\n",
      "torch.Size([45]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-3.0383, grad_fn=<MinBackward1>)\n",
      "Episode 695, Loss: -0.12260747700929642\n",
      "Gradient for fc1.weight: 0.015291961841285229\n",
      "Gradient for fc1.bias: 0.025875000283122063\n",
      "Gradient for fc2.weight: 0.0420718751847744\n",
      "Gradient for fc2.bias: 0.00701686879619956\n",
      "torch.Size([42]) tensor(-0.0015, grad_fn=<MaxBackward1>) tensor(-1.9891, grad_fn=<MinBackward1>)\n",
      "Episode 696, Loss: 0.025717686861753464\n",
      "Gradient for fc1.weight: 0.02004140056669712\n",
      "Gradient for fc1.bias: 0.1571958363056183\n",
      "Gradient for fc2.weight: 0.11859322339296341\n",
      "Gradient for fc2.bias: 0.06592017412185669\n",
      "torch.Size([29]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-3.6489, grad_fn=<MinBackward1>)\n",
      "Episode 697, Loss: -0.019015472382307053\n",
      "Gradient for fc1.weight: 0.0923195630311966\n",
      "Gradient for fc1.bias: 0.25940150022506714\n",
      "Gradient for fc2.weight: 0.24355736374855042\n",
      "Gradient for fc2.bias: 0.12092205137014389\n",
      "torch.Size([50]) tensor(-0.0036, grad_fn=<MaxBackward1>) tensor(-1.3394, grad_fn=<MinBackward1>)\n",
      "Episode 698, Loss: -0.04210197553038597\n",
      "Gradient for fc1.weight: 0.052751682698726654\n",
      "Gradient for fc1.bias: 0.05099385604262352\n",
      "Gradient for fc2.weight: 0.05528273433446884\n",
      "Gradient for fc2.bias: 0.0008719405741430819\n",
      "torch.Size([50]) tensor(-0.0038, grad_fn=<MaxBackward1>) tensor(-0.5610, grad_fn=<MinBackward1>)\n",
      "Episode 699, Loss: -0.027636509388685226\n",
      "Gradient for fc1.weight: 0.02244686894118786\n",
      "Gradient for fc1.bias: 0.07641216367483139\n",
      "Gradient for fc2.weight: 0.05304894968867302\n",
      "Gradient for fc2.bias: 0.029474623501300812\n",
      "torch.Size([61]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-3.1390, grad_fn=<MinBackward1>)\n",
      "Episode 700, Loss: -0.1064130887389183\n",
      "Gradient for fc1.weight: 0.09599600732326508\n",
      "Gradient for fc1.bias: 0.1901487112045288\n",
      "Gradient for fc2.weight: 0.2285916656255722\n",
      "Gradient for fc2.bias: 0.0977136567234993\n",
      "Running reward: 38.11537341874153\n",
      "torch.Size([29]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-4.4791, grad_fn=<MinBackward1>)\n",
      "Episode 701, Loss: 0.06864117830991745\n",
      "Gradient for fc1.weight: 0.029817763715982437\n",
      "Gradient for fc1.bias: 0.059451598674058914\n",
      "Gradient for fc2.weight: 0.06515436619520187\n",
      "Gradient for fc2.bias: 0.026094339787960052\n",
      "torch.Size([57]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-3.2671, grad_fn=<MinBackward1>)\n",
      "Episode 702, Loss: 0.02691488526761532\n",
      "Gradient for fc1.weight: 0.051464494317770004\n",
      "Gradient for fc1.bias: 0.049702491611242294\n",
      "Gradient for fc2.weight: 0.06575947254896164\n",
      "Gradient for fc2.bias: 0.01790536940097809\n",
      "torch.Size([36]) tensor(-0.0016, grad_fn=<MaxBackward1>) tensor(-1.0235, grad_fn=<MinBackward1>)\n",
      "Episode 703, Loss: 0.07707863301038742\n",
      "Gradient for fc1.weight: 0.030134249478578568\n",
      "Gradient for fc1.bias: 0.05432417243719101\n",
      "Gradient for fc2.weight: 0.05568157881498337\n",
      "Gradient for fc2.bias: 0.02301764115691185\n",
      "torch.Size([53]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.6862, grad_fn=<MinBackward1>)\n",
      "Episode 704, Loss: 0.010204183869063854\n",
      "Gradient for fc1.weight: 0.03897787258028984\n",
      "Gradient for fc1.bias: 0.06888552755117416\n",
      "Gradient for fc2.weight: 0.07053721696138382\n",
      "Gradient for fc2.bias: 0.03171088919043541\n",
      "torch.Size([45]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-4.8093, grad_fn=<MinBackward1>)\n",
      "Episode 705, Loss: 0.08721913397312164\n",
      "Gradient for fc1.weight: 0.022618670016527176\n",
      "Gradient for fc1.bias: 0.04165443405508995\n",
      "Gradient for fc2.weight: 0.054469309747219086\n",
      "Gradient for fc2.bias: 0.019199829548597336\n",
      "torch.Size([43]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-4.4684, grad_fn=<MinBackward1>)\n",
      "Episode 706, Loss: 0.036916058510541916\n",
      "Gradient for fc1.weight: 0.10685968399047852\n",
      "Gradient for fc1.bias: 0.24377311766147614\n",
      "Gradient for fc2.weight: 0.2182420939207077\n",
      "Gradient for fc2.bias: 0.10785818845033646\n",
      "torch.Size([58]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.7869, grad_fn=<MinBackward1>)\n",
      "Episode 707, Loss: -0.004470188170671463\n",
      "Gradient for fc1.weight: 0.03045191988348961\n",
      "Gradient for fc1.bias: 0.16965597867965698\n",
      "Gradient for fc2.weight: 0.1345270574092865\n",
      "Gradient for fc2.bias: 0.07516954839229584\n",
      "torch.Size([46]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.9962, grad_fn=<MinBackward1>)\n",
      "Episode 708, Loss: -0.026526356115937233\n",
      "Gradient for fc1.weight: 0.014918009750545025\n",
      "Gradient for fc1.bias: 0.09972653537988663\n",
      "Gradient for fc2.weight: 0.07383250445127487\n",
      "Gradient for fc2.bias: 0.04218891263008118\n",
      "torch.Size([46]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.7591, grad_fn=<MinBackward1>)\n",
      "Episode 709, Loss: 0.022253461182117462\n",
      "Gradient for fc1.weight: 0.011477325111627579\n",
      "Gradient for fc1.bias: 0.09175043553113937\n",
      "Gradient for fc2.weight: 0.06673603504896164\n",
      "Gradient for fc2.bias: 0.03721289709210396\n",
      "torch.Size([36]) tensor(-0.0014, grad_fn=<MaxBackward1>) tensor(-1.6162, grad_fn=<MinBackward1>)\n",
      "Episode 710, Loss: 0.03246450424194336\n",
      "Gradient for fc1.weight: 0.05164012312889099\n",
      "Gradient for fc1.bias: 0.034171655774116516\n",
      "Gradient for fc2.weight: 0.05057549104094505\n",
      "Gradient for fc2.bias: 0.004900699947029352\n",
      "torch.Size([44]) tensor(-0.0018, grad_fn=<MaxBackward1>) tensor(-1.3680, grad_fn=<MinBackward1>)\n",
      "Episode 711, Loss: -0.06311396509408951\n",
      "Gradient for fc1.weight: 0.06167111545801163\n",
      "Gradient for fc1.bias: 0.05909471958875656\n",
      "Gradient for fc2.weight: 0.07866831123828888\n",
      "Gradient for fc2.bias: 0.02684430219233036\n",
      "torch.Size([48]) tensor(-0.0020, grad_fn=<MaxBackward1>) tensor(-1.4882, grad_fn=<MinBackward1>)\n",
      "Episode 712, Loss: 0.07957074046134949\n",
      "Gradient for fc1.weight: 0.019789764657616615\n",
      "Gradient for fc1.bias: 0.051544420421123505\n",
      "Gradient for fc2.weight: 0.047772351652383804\n",
      "Gradient for fc2.bias: 0.021850528195500374\n",
      "torch.Size([31]) tensor(-0.0026, grad_fn=<MaxBackward1>) tensor(-1.2715, grad_fn=<MinBackward1>)\n",
      "Episode 713, Loss: -0.05163988471031189\n",
      "Gradient for fc1.weight: 0.09966868162155151\n",
      "Gradient for fc1.bias: 0.25985920429229736\n",
      "Gradient for fc2.weight: 0.22846680879592896\n",
      "Gradient for fc2.bias: 0.11778660863637924\n",
      "torch.Size([37]) tensor(-0.0015, grad_fn=<MaxBackward1>) tensor(-1.3544, grad_fn=<MinBackward1>)\n",
      "Episode 714, Loss: -0.04798473045229912\n",
      "Gradient for fc1.weight: 0.08975283801555634\n",
      "Gradient for fc1.bias: 0.1643720418214798\n",
      "Gradient for fc2.weight: 0.15984469652175903\n",
      "Gradient for fc2.bias: 0.07560773193836212\n",
      "torch.Size([48]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-1.5648, grad_fn=<MinBackward1>)\n",
      "Episode 715, Loss: 0.014253195375204086\n",
      "Gradient for fc1.weight: 0.05916909873485565\n",
      "Gradient for fc1.bias: 0.033138833940029144\n",
      "Gradient for fc2.weight: 0.05730227008461952\n",
      "Gradient for fc2.bias: 0.00815510656684637\n",
      "torch.Size([59]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-2.0650, grad_fn=<MinBackward1>)\n",
      "Episode 716, Loss: -0.01212280709296465\n",
      "Gradient for fc1.weight: 0.04639498144388199\n",
      "Gradient for fc1.bias: 0.13115191459655762\n",
      "Gradient for fc2.weight: 0.11361029744148254\n",
      "Gradient for fc2.bias: 0.0591648668050766\n",
      "torch.Size([40]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-1.9230, grad_fn=<MinBackward1>)\n",
      "Episode 717, Loss: 0.07845034450292587\n",
      "Gradient for fc1.weight: 0.046552933752536774\n",
      "Gradient for fc1.bias: 0.2238011658191681\n",
      "Gradient for fc2.weight: 0.15066654980182648\n",
      "Gradient for fc2.bias: 0.08750896900892258\n",
      "torch.Size([60]) tensor(-0.0022, grad_fn=<MaxBackward1>) tensor(-0.8437, grad_fn=<MinBackward1>)\n",
      "Episode 718, Loss: 0.03202943503856659\n",
      "Gradient for fc1.weight: 0.018496667966246605\n",
      "Gradient for fc1.bias: 0.12504713237285614\n",
      "Gradient for fc2.weight: 0.09064607322216034\n",
      "Gradient for fc2.bias: 0.05235688388347626\n",
      "torch.Size([68]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-1.8445, grad_fn=<MinBackward1>)\n",
      "Episode 719, Loss: 0.01781383715569973\n",
      "Gradient for fc1.weight: 0.04196269437670708\n",
      "Gradient for fc1.bias: 0.2327871024608612\n",
      "Gradient for fc2.weight: 0.18477663397789001\n",
      "Gradient for fc2.bias: 0.1028035581111908\n",
      "torch.Size([45]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-2.0577, grad_fn=<MinBackward1>)\n",
      "Episode 720, Loss: -0.005120431073009968\n",
      "Gradient for fc1.weight: 0.06966418027877808\n",
      "Gradient for fc1.bias: 0.35883986949920654\n",
      "Gradient for fc2.weight: 0.2905259132385254\n",
      "Gradient for fc2.bias: 0.16283869743347168\n",
      "torch.Size([59]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-3.1551, grad_fn=<MinBackward1>)\n",
      "Episode 721, Loss: -0.008330894634127617\n",
      "Gradient for fc1.weight: 0.022783124819397926\n",
      "Gradient for fc1.bias: 0.05402877554297447\n",
      "Gradient for fc2.weight: 0.03596753254532814\n",
      "Gradient for fc2.bias: 0.01975434459745884\n",
      "torch.Size([43]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-1.9679, grad_fn=<MinBackward1>)\n",
      "Episode 722, Loss: 0.009123087860643864\n",
      "Gradient for fc1.weight: 0.03661829233169556\n",
      "Gradient for fc1.bias: 0.02092745713889599\n",
      "Gradient for fc2.weight: 0.037993546575307846\n",
      "Gradient for fc2.bias: 0.008029480464756489\n",
      "torch.Size([82]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-3.0400, grad_fn=<MinBackward1>)\n",
      "Episode 723, Loss: -0.03919953852891922\n",
      "Gradient for fc1.weight: 0.0169525109231472\n",
      "Gradient for fc1.bias: 0.10029274225234985\n",
      "Gradient for fc2.weight: 0.07382732629776001\n",
      "Gradient for fc2.bias: 0.041568342596292496\n",
      "torch.Size([43]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-3.3123, grad_fn=<MinBackward1>)\n",
      "Episode 724, Loss: -0.018874837085604668\n",
      "Gradient for fc1.weight: 0.05339530110359192\n",
      "Gradient for fc1.bias: 0.14635151624679565\n",
      "Gradient for fc2.weight: 0.13616329431533813\n",
      "Gradient for fc2.bias: 0.07022576034069061\n",
      "torch.Size([37]) tensor(-0.0019, grad_fn=<MaxBackward1>) tensor(-0.9864, grad_fn=<MinBackward1>)\n",
      "Episode 725, Loss: 0.025528430938720703\n",
      "Gradient for fc1.weight: 0.039677925407886505\n",
      "Gradient for fc1.bias: 0.030840875580906868\n",
      "Gradient for fc2.weight: 0.03577021136879921\n",
      "Gradient for fc2.bias: 0.0017972331261262298\n",
      "torch.Size([45]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-3.1136, grad_fn=<MinBackward1>)\n",
      "Episode 726, Loss: -0.051568444818258286\n",
      "Gradient for fc1.weight: 0.036004893481731415\n",
      "Gradient for fc1.bias: 0.08219418674707413\n",
      "Gradient for fc2.weight: 0.08817317336797714\n",
      "Gradient for fc2.bias: 0.03853669762611389\n",
      "torch.Size([50]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-3.0233, grad_fn=<MinBackward1>)\n",
      "Episode 727, Loss: -0.052114009857177734\n",
      "Gradient for fc1.weight: 0.04145420342683792\n",
      "Gradient for fc1.bias: 0.032645341008901596\n",
      "Gradient for fc2.weight: 0.04977024346590042\n",
      "Gradient for fc2.bias: 0.000949293258599937\n",
      "torch.Size([57]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.8506, grad_fn=<MinBackward1>)\n",
      "Episode 728, Loss: 0.025905219838023186\n",
      "Gradient for fc1.weight: 0.007691517006605864\n",
      "Gradient for fc1.bias: 0.09324508905410767\n",
      "Gradient for fc2.weight: 0.06641504168510437\n",
      "Gradient for fc2.bias: 0.03854091465473175\n",
      "torch.Size([56]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-2.3898, grad_fn=<MinBackward1>)\n",
      "Episode 729, Loss: -0.0017733733402565122\n",
      "Gradient for fc1.weight: 0.024867098778486252\n",
      "Gradient for fc1.bias: 0.08843686431646347\n",
      "Gradient for fc2.weight: 0.05639011785387993\n",
      "Gradient for fc2.bias: 0.032041504979133606\n",
      "torch.Size([31]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-2.0522, grad_fn=<MinBackward1>)\n",
      "Episode 730, Loss: 0.11059345304965973\n",
      "Gradient for fc1.weight: 0.02980811707675457\n",
      "Gradient for fc1.bias: 0.1823025792837143\n",
      "Gradient for fc2.weight: 0.13987930119037628\n",
      "Gradient for fc2.bias: 0.07835141569375992\n",
      "torch.Size([61]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-3.4445, grad_fn=<MinBackward1>)\n",
      "Episode 731, Loss: -0.08519060164690018\n",
      "Gradient for fc1.weight: 0.07581869512796402\n",
      "Gradient for fc1.bias: 0.09839267283678055\n",
      "Gradient for fc2.weight: 0.10792576521635056\n",
      "Gradient for fc2.bias: 0.04274902120232582\n",
      "torch.Size([27]) tensor(-0.0022, grad_fn=<MaxBackward1>) tensor(-1.2102, grad_fn=<MinBackward1>)\n",
      "Episode 732, Loss: -0.013482541777193546\n",
      "Gradient for fc1.weight: 0.014144808053970337\n",
      "Gradient for fc1.bias: 0.033262062817811966\n",
      "Gradient for fc2.weight: 0.019073963165283203\n",
      "Gradient for fc2.bias: 0.009750740602612495\n",
      "torch.Size([60]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-3.3835, grad_fn=<MinBackward1>)\n",
      "Episode 733, Loss: -0.019011348485946655\n",
      "Gradient for fc1.weight: 0.06412571668624878\n",
      "Gradient for fc1.bias: 0.1388167440891266\n",
      "Gradient for fc2.weight: 0.1388474553823471\n",
      "Gradient for fc2.bias: 0.06749213486909866\n",
      "torch.Size([49]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-2.3986, grad_fn=<MinBackward1>)\n",
      "Episode 734, Loss: 0.007409852463752031\n",
      "Gradient for fc1.weight: 0.021641049534082413\n",
      "Gradient for fc1.bias: 0.1684263050556183\n",
      "Gradient for fc2.weight: 0.10669942945241928\n",
      "Gradient for fc2.bias: 0.061924517154693604\n",
      "torch.Size([61]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-3.5042, grad_fn=<MinBackward1>)\n",
      "Episode 735, Loss: 0.022526107728481293\n",
      "Gradient for fc1.weight: 0.0328114815056324\n",
      "Gradient for fc1.bias: 0.1337437778711319\n",
      "Gradient for fc2.weight: 0.10359513759613037\n",
      "Gradient for fc2.bias: 0.056505654007196426\n",
      "torch.Size([98]) tensor(-0.0012, grad_fn=<MaxBackward1>) tensor(-1.3932, grad_fn=<MinBackward1>)\n",
      "Episode 736, Loss: 0.03923918306827545\n",
      "Gradient for fc1.weight: 0.022738296538591385\n",
      "Gradient for fc1.bias: 0.11875168234109879\n",
      "Gradient for fc2.weight: 0.09049005806446075\n",
      "Gradient for fc2.bias: 0.050828687846660614\n",
      "torch.Size([72]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-2.2900, grad_fn=<MinBackward1>)\n",
      "Episode 737, Loss: 0.07679764926433563\n",
      "Gradient for fc1.weight: 0.019796965643763542\n",
      "Gradient for fc1.bias: 0.07864101231098175\n",
      "Gradient for fc2.weight: 0.0505765825510025\n",
      "Gradient for fc2.bias: 0.02864658646285534\n",
      "torch.Size([59]) tensor(-0.0017, grad_fn=<MaxBackward1>) tensor(-1.1753, grad_fn=<MinBackward1>)\n",
      "Episode 738, Loss: 0.050466910004615784\n",
      "Gradient for fc1.weight: 0.06827157735824585\n",
      "Gradient for fc1.bias: 0.23138350248336792\n",
      "Gradient for fc2.weight: 0.1841966211795807\n",
      "Gradient for fc2.bias: 0.10077637434005737\n",
      "torch.Size([62]) tensor(-0.0012, grad_fn=<MaxBackward1>) tensor(-1.5225, grad_fn=<MinBackward1>)\n",
      "Episode 739, Loss: 0.07165155559778214\n",
      "Gradient for fc1.weight: 0.013678778894245625\n",
      "Gradient for fc1.bias: 0.18718010187149048\n",
      "Gradient for fc2.weight: 0.12645204365253448\n",
      "Gradient for fc2.bias: 0.07657703757286072\n",
      "torch.Size([56]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.5786, grad_fn=<MinBackward1>)\n",
      "Episode 740, Loss: -0.07607845962047577\n",
      "Gradient for fc1.weight: 0.12023260444402695\n",
      "Gradient for fc1.bias: 0.224905863404274\n",
      "Gradient for fc2.weight: 0.21294455230236053\n",
      "Gradient for fc2.bias: 0.10011652857065201\n",
      "torch.Size([37]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-2.5508, grad_fn=<MinBackward1>)\n",
      "Episode 741, Loss: -0.015796814113855362\n",
      "Gradient for fc1.weight: 0.07792355120182037\n",
      "Gradient for fc1.bias: 0.11308716982603073\n",
      "Gradient for fc2.weight: 0.11367758363485336\n",
      "Gradient for fc2.bias: 0.04795294627547264\n",
      "torch.Size([43]) tensor(-9.5908e-05, grad_fn=<MaxBackward1>) tensor(-5.1462, grad_fn=<MinBackward1>)\n",
      "Episode 742, Loss: 0.07457102090120316\n",
      "Gradient for fc1.weight: 0.048618391156196594\n",
      "Gradient for fc1.bias: 0.1880636364221573\n",
      "Gradient for fc2.weight: 0.13988901674747467\n",
      "Gradient for fc2.bias: 0.07943280786275864\n",
      "torch.Size([48]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-1.9291, grad_fn=<MinBackward1>)\n",
      "Episode 743, Loss: 0.003160658059641719\n",
      "Gradient for fc1.weight: 0.019471392035484314\n",
      "Gradient for fc1.bias: 0.03173984959721565\n",
      "Gradient for fc2.weight: 0.024487527087330818\n",
      "Gradient for fc2.bias: 0.007078137714415789\n",
      "torch.Size([58]) tensor(-5.5553e-05, grad_fn=<MaxBackward1>) tensor(-5.5359, grad_fn=<MinBackward1>)\n",
      "Episode 744, Loss: 0.16468416154384613\n",
      "Gradient for fc1.weight: 0.02562526799738407\n",
      "Gradient for fc1.bias: 0.03873290494084358\n",
      "Gradient for fc2.weight: 0.05967160686850548\n",
      "Gradient for fc2.bias: 0.003995520994067192\n",
      "torch.Size([70]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.7680, grad_fn=<MinBackward1>)\n",
      "Episode 745, Loss: -0.004504946060478687\n",
      "Gradient for fc1.weight: 0.02397661656141281\n",
      "Gradient for fc1.bias: 0.08739541471004486\n",
      "Gradient for fc2.weight: 0.06978952884674072\n",
      "Gradient for fc2.bias: 0.038091991096735\n",
      "torch.Size([47]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-1.5814, grad_fn=<MinBackward1>)\n",
      "Episode 746, Loss: 0.0799017921090126\n",
      "Gradient for fc1.weight: 0.011088836006820202\n",
      "Gradient for fc1.bias: 0.0436660535633564\n",
      "Gradient for fc2.weight: 0.035748787224292755\n",
      "Gradient for fc2.bias: 0.01708056405186653\n",
      "torch.Size([64]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-1.6273, grad_fn=<MinBackward1>)\n",
      "Episode 747, Loss: 0.03796938806772232\n",
      "Gradient for fc1.weight: 0.06549671292304993\n",
      "Gradient for fc1.bias: 0.22310321033000946\n",
      "Gradient for fc2.weight: 0.16063791513442993\n",
      "Gradient for fc2.bias: 0.09290704876184464\n",
      "torch.Size([37]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-1.3836, grad_fn=<MinBackward1>)\n",
      "Episode 748, Loss: 0.03352894261479378\n",
      "Gradient for fc1.weight: 0.0959801971912384\n",
      "Gradient for fc1.bias: 0.28538018465042114\n",
      "Gradient for fc2.weight: 0.23679415881633759\n",
      "Gradient for fc2.bias: 0.1254752278327942\n",
      "torch.Size([67]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.5125, grad_fn=<MinBackward1>)\n",
      "Episode 749, Loss: -0.11548019200563431\n",
      "Gradient for fc1.weight: 0.015184704214334488\n",
      "Gradient for fc1.bias: 0.0803651288151741\n",
      "Gradient for fc2.weight: 0.06210383400321007\n",
      "Gradient for fc2.bias: 0.030932707712054253\n",
      "torch.Size([42]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-3.9811, grad_fn=<MinBackward1>)\n",
      "Episode 750, Loss: 0.07810910046100616\n",
      "Gradient for fc1.weight: 0.023199833929538727\n",
      "Gradient for fc1.bias: 0.17410409450531006\n",
      "Gradient for fc2.weight: 0.1125289648771286\n",
      "Gradient for fc2.bias: 0.06431795656681061\n",
      "torch.Size([46]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.9053, grad_fn=<MinBackward1>)\n",
      "Episode 751, Loss: 0.09955986589193344\n",
      "Gradient for fc1.weight: 0.03331206738948822\n",
      "Gradient for fc1.bias: 0.17984327673912048\n",
      "Gradient for fc2.weight: 0.12260784953832626\n",
      "Gradient for fc2.bias: 0.07270275801420212\n",
      "torch.Size([24]) tensor(-0.0023, grad_fn=<MaxBackward1>) tensor(-1.3916, grad_fn=<MinBackward1>)\n",
      "Episode 752, Loss: -0.0846722349524498\n",
      "Gradient for fc1.weight: 0.06126439571380615\n",
      "Gradient for fc1.bias: 0.12200472503900528\n",
      "Gradient for fc2.weight: 0.0933196023106575\n",
      "Gradient for fc2.bias: 0.047941386699676514\n",
      "torch.Size([47]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-2.1231, grad_fn=<MinBackward1>)\n",
      "Episode 753, Loss: 0.06420263648033142\n",
      "Gradient for fc1.weight: 0.05471828207373619\n",
      "Gradient for fc1.bias: 0.13207721710205078\n",
      "Gradient for fc2.weight: 0.09553598612546921\n",
      "Gradient for fc2.bias: 0.05419190227985382\n",
      "torch.Size([41]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-3.5729, grad_fn=<MinBackward1>)\n",
      "Episode 754, Loss: -0.11384274065494537\n",
      "Gradient for fc1.weight: 0.05684765428304672\n",
      "Gradient for fc1.bias: 0.09984378516674042\n",
      "Gradient for fc2.weight: 0.0700404942035675\n",
      "Gradient for fc2.bias: 0.01022449228912592\n",
      "torch.Size([33]) tensor(-0.0022, grad_fn=<MaxBackward1>) tensor(-1.1568, grad_fn=<MinBackward1>)\n",
      "Episode 755, Loss: 0.0025891319382935762\n",
      "Gradient for fc1.weight: 0.07225173711776733\n",
      "Gradient for fc1.bias: 0.26514336466789246\n",
      "Gradient for fc2.weight: 0.1807474046945572\n",
      "Gradient for fc2.bias: 0.1070512905716896\n",
      "torch.Size([35]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-3.6059, grad_fn=<MinBackward1>)\n",
      "Episode 756, Loss: -0.04556003957986832\n",
      "Gradient for fc1.weight: 0.014972643926739693\n",
      "Gradient for fc1.bias: 0.055204007774591446\n",
      "Gradient for fc2.weight: 0.0437362864613533\n",
      "Gradient for fc2.bias: 0.020938411355018616\n",
      "torch.Size([74]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-1.8159, grad_fn=<MinBackward1>)\n",
      "Episode 757, Loss: 0.033260125666856766\n",
      "Gradient for fc1.weight: 0.05252663418650627\n",
      "Gradient for fc1.bias: 0.11887172609567642\n",
      "Gradient for fc2.weight: 0.1072535514831543\n",
      "Gradient for fc2.bias: 0.05265228822827339\n",
      "torch.Size([61]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-2.4442, grad_fn=<MinBackward1>)\n",
      "Episode 758, Loss: -0.0557585135102272\n",
      "Gradient for fc1.weight: 0.025715798139572144\n",
      "Gradient for fc1.bias: 0.04263729602098465\n",
      "Gradient for fc2.weight: 0.02918006107211113\n",
      "Gradient for fc2.bias: 0.008372287265956402\n",
      "torch.Size([42]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-1.8622, grad_fn=<MinBackward1>)\n",
      "Episode 759, Loss: 0.019187308847904205\n",
      "Gradient for fc1.weight: 0.02826783061027527\n",
      "Gradient for fc1.bias: 0.10955985635519028\n",
      "Gradient for fc2.weight: 0.07415331900119781\n",
      "Gradient for fc2.bias: 0.04452195018529892\n",
      "torch.Size([40]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-1.6344, grad_fn=<MinBackward1>)\n",
      "Episode 760, Loss: -0.10188944637775421\n",
      "Gradient for fc1.weight: 0.05466163158416748\n",
      "Gradient for fc1.bias: 0.1170373186469078\n",
      "Gradient for fc2.weight: 0.08749561011791229\n",
      "Gradient for fc2.bias: 0.04663829505443573\n",
      "torch.Size([35]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-3.2723, grad_fn=<MinBackward1>)\n",
      "Episode 761, Loss: -0.10045906901359558\n",
      "Gradient for fc1.weight: 0.0898112952709198\n",
      "Gradient for fc1.bias: 0.0951075330376625\n",
      "Gradient for fc2.weight: 0.11683805286884308\n",
      "Gradient for fc2.bias: 0.041066329926252365\n",
      "torch.Size([43]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.2860, grad_fn=<MinBackward1>)\n",
      "Episode 762, Loss: -0.03465760871767998\n",
      "Gradient for fc1.weight: 0.046154629439115524\n",
      "Gradient for fc1.bias: 0.10571842640638351\n",
      "Gradient for fc2.weight: 0.07691049575805664\n",
      "Gradient for fc2.bias: 0.04195575416088104\n",
      "torch.Size([31]) tensor(-0.0041, grad_fn=<MaxBackward1>) tensor(-0.8541, grad_fn=<MinBackward1>)\n",
      "Episode 763, Loss: -0.033949267119169235\n",
      "Gradient for fc1.weight: 0.012735240161418915\n",
      "Gradient for fc1.bias: 0.0184868685901165\n",
      "Gradient for fc2.weight: 0.009912311099469662\n",
      "Gradient for fc2.bias: 0.0027018594555556774\n",
      "torch.Size([28]) tensor(-0.0009, grad_fn=<MaxBackward1>) tensor(-1.6017, grad_fn=<MinBackward1>)\n",
      "Episode 764, Loss: -0.05074156075716019\n",
      "Gradient for fc1.weight: 0.045909274369478226\n",
      "Gradient for fc1.bias: 0.20764170587062836\n",
      "Gradient for fc2.weight: 0.13166062533855438\n",
      "Gradient for fc2.bias: 0.08071205765008926\n",
      "torch.Size([34]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.6562, grad_fn=<MinBackward1>)\n",
      "Episode 765, Loss: -0.03867758437991142\n",
      "Gradient for fc1.weight: 0.03202734887599945\n",
      "Gradient for fc1.bias: 0.09449384361505508\n",
      "Gradient for fc2.weight: 0.061596453189849854\n",
      "Gradient for fc2.bias: 0.03663836792111397\n",
      "torch.Size([27]) tensor(-4.8162e-05, grad_fn=<MaxBackward1>) tensor(-4.4407, grad_fn=<MinBackward1>)\n",
      "Episode 766, Loss: 0.1768292337656021\n",
      "Gradient for fc1.weight: 0.02341146394610405\n",
      "Gradient for fc1.bias: 0.21642719209194183\n",
      "Gradient for fc2.weight: 0.1455424726009369\n",
      "Gradient for fc2.bias: 0.0784815177321434\n",
      "torch.Size([35]) tensor(-0.0011, grad_fn=<MaxBackward1>) tensor(-3.2882, grad_fn=<MinBackward1>)\n",
      "Episode 767, Loss: 0.13249409198760986\n",
      "Gradient for fc1.weight: 0.0751812681555748\n",
      "Gradient for fc1.bias: 0.22316671907901764\n",
      "Gradient for fc2.weight: 0.18928900361061096\n",
      "Gradient for fc2.bias: 0.09077119827270508\n",
      "torch.Size([65]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-2.9880, grad_fn=<MinBackward1>)\n",
      "Episode 768, Loss: -0.008491149172186852\n",
      "Gradient for fc1.weight: 0.017984721809625626\n",
      "Gradient for fc1.bias: 0.08466915041208267\n",
      "Gradient for fc2.weight: 0.06451818346977234\n",
      "Gradient for fc2.bias: 0.03446458652615547\n",
      "torch.Size([31]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.8311, grad_fn=<MinBackward1>)\n",
      "Episode 769, Loss: -0.09783419966697693\n",
      "Gradient for fc1.weight: 0.03898065537214279\n",
      "Gradient for fc1.bias: 0.0856197252869606\n",
      "Gradient for fc2.weight: 0.05758936330676079\n",
      "Gradient for fc2.bias: 0.030774248763918877\n",
      "torch.Size([29]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-1.3453, grad_fn=<MinBackward1>)\n",
      "Episode 770, Loss: -0.02367238886654377\n",
      "Gradient for fc1.weight: 0.024290380999445915\n",
      "Gradient for fc1.bias: 0.03900418430566788\n",
      "Gradient for fc2.weight: 0.026241451501846313\n",
      "Gradient for fc2.bias: 0.013161578215658665\n",
      "torch.Size([64]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-1.6525, grad_fn=<MinBackward1>)\n",
      "Episode 771, Loss: -0.10321308672428131\n",
      "Gradient for fc1.weight: 0.06436619162559509\n",
      "Gradient for fc1.bias: 0.13972485065460205\n",
      "Gradient for fc2.weight: 0.09202612191438675\n",
      "Gradient for fc2.bias: 0.05216192826628685\n",
      "torch.Size([53]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-3.7791, grad_fn=<MinBackward1>)\n",
      "Episode 772, Loss: -0.05448375269770622\n",
      "Gradient for fc1.weight: 0.015705622732639313\n",
      "Gradient for fc1.bias: 0.04391668736934662\n",
      "Gradient for fc2.weight: 0.03366248309612274\n",
      "Gradient for fc2.bias: 0.01813407614827156\n",
      "torch.Size([27]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-1.3280, grad_fn=<MinBackward1>)\n",
      "Episode 773, Loss: -0.11452043801546097\n",
      "Gradient for fc1.weight: 0.03493022546172142\n",
      "Gradient for fc1.bias: 0.06944220513105392\n",
      "Gradient for fc2.weight: 0.05226132273674011\n",
      "Gradient for fc2.bias: 0.022755417972803116\n",
      "torch.Size([27]) tensor(-0.0027, grad_fn=<MaxBackward1>) tensor(-2.4851, grad_fn=<MinBackward1>)\n",
      "Episode 774, Loss: -0.005299071315675974\n",
      "Gradient for fc1.weight: 0.08103129267692566\n",
      "Gradient for fc1.bias: 0.12366154044866562\n",
      "Gradient for fc2.weight: 0.052808765321969986\n",
      "Gradient for fc2.bias: 0.013717882335186005\n",
      "torch.Size([65]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.7765, grad_fn=<MinBackward1>)\n",
      "Episode 775, Loss: -0.05806412547826767\n",
      "Gradient for fc1.weight: 0.04719678312540054\n",
      "Gradient for fc1.bias: 0.08994344621896744\n",
      "Gradient for fc2.weight: 0.06524311751127243\n",
      "Gradient for fc2.bias: 0.03641011193394661\n",
      "torch.Size([26]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.2822, grad_fn=<MinBackward1>)\n",
      "Episode 776, Loss: -0.16363660991191864\n",
      "Gradient for fc1.weight: 0.08564040064811707\n",
      "Gradient for fc1.bias: 0.11765091866254807\n",
      "Gradient for fc2.weight: 0.13501471281051636\n",
      "Gradient for fc2.bias: 0.05551357939839363\n",
      "torch.Size([25]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-2.4352, grad_fn=<MinBackward1>)\n",
      "Episode 777, Loss: -0.049033746123313904\n",
      "Gradient for fc1.weight: 0.048676494508981705\n",
      "Gradient for fc1.bias: 0.07758814096450806\n",
      "Gradient for fc2.weight: 0.021009914577007294\n",
      "Gradient for fc2.bias: 0.00031440804013982415\n",
      "torch.Size([106]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-1.7676, grad_fn=<MinBackward1>)\n",
      "Episode 778, Loss: -0.03065311908721924\n",
      "Gradient for fc1.weight: 0.010709475725889206\n",
      "Gradient for fc1.bias: 0.02288781851530075\n",
      "Gradient for fc2.weight: 0.016802074387669563\n",
      "Gradient for fc2.bias: 0.007018952164798975\n",
      "torch.Size([65]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-3.6647, grad_fn=<MinBackward1>)\n",
      "Episode 779, Loss: -0.01726202666759491\n",
      "Gradient for fc1.weight: 0.032672200351953506\n",
      "Gradient for fc1.bias: 0.0660046935081482\n",
      "Gradient for fc2.weight: 0.06624523550271988\n",
      "Gradient for fc2.bias: 0.02611025795340538\n",
      "torch.Size([25]) tensor(-0.0031, grad_fn=<MaxBackward1>) tensor(-1.6760, grad_fn=<MinBackward1>)\n",
      "Episode 780, Loss: -0.16645082831382751\n",
      "Gradient for fc1.weight: 0.247857004404068\n",
      "Gradient for fc1.bias: 0.4504031538963318\n",
      "Gradient for fc2.weight: 0.3865434229373932\n",
      "Gradient for fc2.bias: 0.19450613856315613\n",
      "torch.Size([27]) tensor(-0.0020, grad_fn=<MaxBackward1>) tensor(-0.5046, grad_fn=<MinBackward1>)\n",
      "Episode 781, Loss: -0.017431482672691345\n",
      "Gradient for fc1.weight: 0.02608484774827957\n",
      "Gradient for fc1.bias: 0.07011425495147705\n",
      "Gradient for fc2.weight: 0.05142021179199219\n",
      "Gradient for fc2.bias: 0.027809247374534607\n",
      "torch.Size([36]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-4.4531, grad_fn=<MinBackward1>)\n",
      "Episode 782, Loss: -0.07601653784513474\n",
      "Gradient for fc1.weight: 0.06838642805814743\n",
      "Gradient for fc1.bias: 0.128584086894989\n",
      "Gradient for fc2.weight: 0.15201470255851746\n",
      "Gradient for fc2.bias: 0.06847808510065079\n",
      "torch.Size([23]) tensor(-0.0038, grad_fn=<MaxBackward1>) tensor(-1.3010, grad_fn=<MinBackward1>)\n",
      "Episode 783, Loss: -0.027527406811714172\n",
      "Gradient for fc1.weight: 0.01588822714984417\n",
      "Gradient for fc1.bias: 0.017204659059643745\n",
      "Gradient for fc2.weight: 0.011692224070429802\n",
      "Gradient for fc2.bias: 0.0004774878907483071\n",
      "torch.Size([96]) tensor(-4.1665e-05, grad_fn=<MaxBackward1>) tensor(-4.0714, grad_fn=<MinBackward1>)\n",
      "Episode 784, Loss: -0.09124735742807388\n",
      "Gradient for fc1.weight: 0.030844934284687042\n",
      "Gradient for fc1.bias: 0.05201574042439461\n",
      "Gradient for fc2.weight: 0.04372508078813553\n",
      "Gradient for fc2.bias: 0.021351266652345657\n",
      "torch.Size([26]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-2.0658, grad_fn=<MinBackward1>)\n",
      "Episode 785, Loss: -0.0774494856595993\n",
      "Gradient for fc1.weight: 0.04557669162750244\n",
      "Gradient for fc1.bias: 0.10771284997463226\n",
      "Gradient for fc2.weight: 0.0680994838476181\n",
      "Gradient for fc2.bias: 0.03791163116693497\n",
      "torch.Size([37]) tensor(-0.0016, grad_fn=<MaxBackward1>) tensor(-0.5258, grad_fn=<MinBackward1>)\n",
      "Episode 786, Loss: -0.027619268745183945\n",
      "Gradient for fc1.weight: 0.011658145114779472\n",
      "Gradient for fc1.bias: 0.01690986193716526\n",
      "Gradient for fc2.weight: 0.01015122514218092\n",
      "Gradient for fc2.bias: 0.0029612849466502666\n",
      "torch.Size([30]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-2.5963, grad_fn=<MinBackward1>)\n",
      "Episode 787, Loss: -0.1076740100979805\n",
      "Gradient for fc1.weight: 0.10759896785020828\n",
      "Gradient for fc1.bias: 0.13793937861919403\n",
      "Gradient for fc2.weight: 0.09273926913738251\n",
      "Gradient for fc2.bias: 0.03058152087032795\n",
      "torch.Size([64]) tensor(-7.8085e-05, grad_fn=<MaxBackward1>) tensor(-2.9741, grad_fn=<MinBackward1>)\n",
      "Episode 788, Loss: -0.0034359507262706757\n",
      "Gradient for fc1.weight: 0.05324853956699371\n",
      "Gradient for fc1.bias: 0.16694507002830505\n",
      "Gradient for fc2.weight: 0.1059882789850235\n",
      "Gradient for fc2.bias: 0.06272943317890167\n",
      "torch.Size([58]) tensor(-0.0005, grad_fn=<MaxBackward1>) tensor(-3.9916, grad_fn=<MinBackward1>)\n",
      "Episode 789, Loss: -0.19860553741455078\n",
      "Gradient for fc1.weight: 0.08001256734132767\n",
      "Gradient for fc1.bias: 0.13279584050178528\n",
      "Gradient for fc2.weight: 0.1408730149269104\n",
      "Gradient for fc2.bias: 0.06428608298301697\n",
      "torch.Size([25]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.1117, grad_fn=<MinBackward1>)\n",
      "Episode 790, Loss: -0.15863889455795288\n",
      "Gradient for fc1.weight: 0.0982581153512001\n",
      "Gradient for fc1.bias: 0.16793812811374664\n",
      "Gradient for fc2.weight: 0.12799115478992462\n",
      "Gradient for fc2.bias: 0.06061312556266785\n",
      "torch.Size([43]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.1369, grad_fn=<MinBackward1>)\n",
      "Episode 791, Loss: -0.04211823642253876\n",
      "Gradient for fc1.weight: 0.011207706294953823\n",
      "Gradient for fc1.bias: 0.02132902294397354\n",
      "Gradient for fc2.weight: 0.00861333031207323\n",
      "Gradient for fc2.bias: 0.0017117157112807035\n",
      "torch.Size([25]) tensor(-0.0006, grad_fn=<MaxBackward1>) tensor(-1.7754, grad_fn=<MinBackward1>)\n",
      "Episode 792, Loss: -0.04991697147488594\n",
      "Gradient for fc1.weight: 0.0399259477853775\n",
      "Gradient for fc1.bias: 0.06657451391220093\n",
      "Gradient for fc2.weight: 0.04779691621661186\n",
      "Gradient for fc2.bias: 0.023342741653323174\n",
      "torch.Size([46]) tensor(-0.0013, grad_fn=<MaxBackward1>) tensor(-0.6664, grad_fn=<MinBackward1>)\n",
      "Episode 793, Loss: -0.0458078607916832\n",
      "Gradient for fc1.weight: 0.05045788735151291\n",
      "Gradient for fc1.bias: 0.07361290603876114\n",
      "Gradient for fc2.weight: 0.05518428981304169\n",
      "Gradient for fc2.bias: 0.027499746531248093\n",
      "torch.Size([39]) tensor(-0.0008, grad_fn=<MaxBackward1>) tensor(-1.3364, grad_fn=<MinBackward1>)\n",
      "Episode 794, Loss: -0.04400142282247543\n",
      "Gradient for fc1.weight: 0.029707783833146095\n",
      "Gradient for fc1.bias: 0.0577031709253788\n",
      "Gradient for fc2.weight: 0.038566626608371735\n",
      "Gradient for fc2.bias: 0.021008387207984924\n",
      "torch.Size([28]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-2.4973, grad_fn=<MinBackward1>)\n",
      "Episode 795, Loss: -0.0954667255282402\n",
      "Gradient for fc1.weight: 0.07196389883756638\n",
      "Gradient for fc1.bias: 0.13641813397407532\n",
      "Gradient for fc2.weight: 0.09743174910545349\n",
      "Gradient for fc2.bias: 0.05054813623428345\n",
      "torch.Size([76]) tensor(-7.1111e-05, grad_fn=<MaxBackward1>) tensor(-5.0075, grad_fn=<MinBackward1>)\n",
      "Episode 796, Loss: 0.061882294714450836\n",
      "Gradient for fc1.weight: 0.037416182458400726\n",
      "Gradient for fc1.bias: 0.1412457376718521\n",
      "Gradient for fc2.weight: 0.1064637154340744\n",
      "Gradient for fc2.bias: 0.05704723298549652\n",
      "torch.Size([99]) tensor(-5.6507e-05, grad_fn=<MaxBackward1>) tensor(-2.9975, grad_fn=<MinBackward1>)\n",
      "Episode 797, Loss: -0.03425803780555725\n",
      "Gradient for fc1.weight: 0.031902190297842026\n",
      "Gradient for fc1.bias: 0.09114380925893784\n",
      "Gradient for fc2.weight: 0.06926789879798889\n",
      "Gradient for fc2.bias: 0.03436841070652008\n",
      "torch.Size([50]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-0.8838, grad_fn=<MinBackward1>)\n",
      "Episode 798, Loss: -0.049948081374168396\n",
      "Gradient for fc1.weight: 0.030413582921028137\n",
      "Gradient for fc1.bias: 0.03196088224649429\n",
      "Gradient for fc2.weight: 0.025424649938941002\n",
      "Gradient for fc2.bias: 0.00762714259326458\n",
      "torch.Size([34]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-1.3226, grad_fn=<MinBackward1>)\n",
      "Episode 799, Loss: -0.030038626864552498\n",
      "Gradient for fc1.weight: 0.10684684664011002\n",
      "Gradient for fc1.bias: 0.32333633303642273\n",
      "Gradient for fc2.weight: 0.20040573179721832\n",
      "Gradient for fc2.bias: 0.11664006114006042\n",
      "torch.Size([57]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-1.3014, grad_fn=<MinBackward1>)\n",
      "Episode 800, Loss: -0.07866200804710388\n",
      "Gradient for fc1.weight: 0.04746082052588463\n",
      "Gradient for fc1.bias: 0.07193722575902939\n",
      "Gradient for fc2.weight: 0.06952542811632156\n",
      "Gradient for fc2.bias: 0.02708384208381176\n",
      "Running reward: 35.93437995538459\n",
      "torch.Size([42]) tensor(-3.7313e-05, grad_fn=<MaxBackward1>) tensor(-4.0084, grad_fn=<MinBackward1>)\n",
      "Episode 801, Loss: 0.01552634872496128\n",
      "Gradient for fc1.weight: 0.04266620799899101\n",
      "Gradient for fc1.bias: 0.03885499760508537\n",
      "Gradient for fc2.weight: 0.03799502179026604\n",
      "Gradient for fc2.bias: 0.0010103778913617134\n",
      "torch.Size([28]) tensor(-0.0007, grad_fn=<MaxBackward1>) tensor(-0.6156, grad_fn=<MinBackward1>)\n",
      "Episode 802, Loss: -0.02511429786682129\n",
      "Gradient for fc1.weight: 0.054631974548101425\n",
      "Gradient for fc1.bias: 0.14238986372947693\n",
      "Gradient for fc2.weight: 0.08810998499393463\n",
      "Gradient for fc2.bias: 0.050704777240753174\n",
      "torch.Size([62]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-2.0687, grad_fn=<MinBackward1>)\n",
      "Episode 803, Loss: -0.06420274078845978\n",
      "Gradient for fc1.weight: 0.02825135551393032\n",
      "Gradient for fc1.bias: 0.029244154691696167\n",
      "Gradient for fc2.weight: 0.025362545624375343\n",
      "Gradient for fc2.bias: 0.00892233569175005\n",
      "torch.Size([84]) tensor(-9.1792e-06, grad_fn=<MaxBackward1>) tensor(-5.4689, grad_fn=<MinBackward1>)\n",
      "Episode 804, Loss: -0.00523030199110508\n",
      "Gradient for fc1.weight: 0.0268911924213171\n",
      "Gradient for fc1.bias: 0.11491835117340088\n",
      "Gradient for fc2.weight: 0.09194722771644592\n",
      "Gradient for fc2.bias: 0.04443353787064552\n",
      "torch.Size([31]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-1.4716, grad_fn=<MinBackward1>)\n",
      "Episode 805, Loss: -0.03150329738855362\n",
      "Gradient for fc1.weight: 0.06608765572309494\n",
      "Gradient for fc1.bias: 0.13314765691757202\n",
      "Gradient for fc2.weight: 0.08662307262420654\n",
      "Gradient for fc2.bias: 0.046575047075748444\n",
      "torch.Size([144]) tensor(-4.0770e-05, grad_fn=<MaxBackward1>) tensor(-3.6853, grad_fn=<MinBackward1>)\n",
      "Episode 806, Loss: -0.013957909308373928\n",
      "Gradient for fc1.weight: 0.02860930748283863\n",
      "Gradient for fc1.bias: 0.023595722392201424\n",
      "Gradient for fc2.weight: 0.024177633225917816\n",
      "Gradient for fc2.bias: 0.008178644813597202\n",
      "torch.Size([48]) tensor(-9.1974e-05, grad_fn=<MaxBackward1>) tensor(-2.6432, grad_fn=<MinBackward1>)\n",
      "Episode 807, Loss: -0.08332039415836334\n",
      "Gradient for fc1.weight: 0.03396911919116974\n",
      "Gradient for fc1.bias: 0.0684317946434021\n",
      "Gradient for fc2.weight: 0.053809795528650284\n",
      "Gradient for fc2.bias: 0.02144986391067505\n",
      "torch.Size([31]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-2.1025, grad_fn=<MinBackward1>)\n",
      "Episode 808, Loss: 0.0011834983015432954\n",
      "Gradient for fc1.weight: 0.06131013110280037\n",
      "Gradient for fc1.bias: 0.11178616434335709\n",
      "Gradient for fc2.weight: 0.0681932345032692\n",
      "Gradient for fc2.bias: 0.03698351979255676\n",
      "torch.Size([58]) tensor(-4.6135e-05, grad_fn=<MaxBackward1>) tensor(-2.4749, grad_fn=<MinBackward1>)\n",
      "Episode 809, Loss: -0.1605834662914276\n",
      "Gradient for fc1.weight: 0.06072986125946045\n",
      "Gradient for fc1.bias: 0.13593244552612305\n",
      "Gradient for fc2.weight: 0.11524142324924469\n",
      "Gradient for fc2.bias: 0.05089305341243744\n",
      "torch.Size([59]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-2.6086, grad_fn=<MinBackward1>)\n",
      "Episode 810, Loss: -0.07805868238210678\n",
      "Gradient for fc1.weight: 0.07404173165559769\n",
      "Gradient for fc1.bias: 0.19251203536987305\n",
      "Gradient for fc2.weight: 0.16718587279319763\n",
      "Gradient for fc2.bias: 0.07316896319389343\n",
      "torch.Size([32]) tensor(-0.0003, grad_fn=<MaxBackward1>) tensor(-1.8345, grad_fn=<MinBackward1>)\n",
      "Episode 811, Loss: 0.06426475197076797\n",
      "Gradient for fc1.weight: 0.021836664527654648\n",
      "Gradient for fc1.bias: 0.07652562111616135\n",
      "Gradient for fc2.weight: 0.053689658641815186\n",
      "Gradient for fc2.bias: 0.026619253680109978\n",
      "torch.Size([47]) tensor(-4.2201e-05, grad_fn=<MaxBackward1>) tensor(-5.2154, grad_fn=<MinBackward1>)\n",
      "Episode 812, Loss: -0.020087720826268196\n",
      "Gradient for fc1.weight: 0.058192815631628036\n",
      "Gradient for fc1.bias: 0.06712161749601364\n",
      "Gradient for fc2.weight: 0.04297231137752533\n",
      "Gradient for fc2.bias: 0.008345261216163635\n",
      "torch.Size([48]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-1.9981, grad_fn=<MinBackward1>)\n",
      "Episode 813, Loss: -0.06293206661939621\n",
      "Gradient for fc1.weight: 0.04263819381594658\n",
      "Gradient for fc1.bias: 0.13662947714328766\n",
      "Gradient for fc2.weight: 0.08017361164093018\n",
      "Gradient for fc2.bias: 0.0481373555958271\n",
      "torch.Size([49]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-1.8494, grad_fn=<MinBackward1>)\n",
      "Episode 814, Loss: -0.04890904948115349\n",
      "Gradient for fc1.weight: 0.008035351522266865\n",
      "Gradient for fc1.bias: 0.028334876522421837\n",
      "Gradient for fc2.weight: 0.018527520820498466\n",
      "Gradient for fc2.bias: 0.009106874465942383\n",
      "torch.Size([39]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-3.5880, grad_fn=<MinBackward1>)\n",
      "Episode 815, Loss: -0.14346809685230255\n",
      "Gradient for fc1.weight: 0.15511105954647064\n",
      "Gradient for fc1.bias: 0.2943532466888428\n",
      "Gradient for fc2.weight: 0.19768904149532318\n",
      "Gradient for fc2.bias: 0.10477256774902344\n",
      "torch.Size([62]) tensor(-6.1752e-05, grad_fn=<MaxBackward1>) tensor(-2.3048, grad_fn=<MinBackward1>)\n",
      "Episode 816, Loss: 0.05554104223847389\n",
      "Gradient for fc1.weight: 0.0273793563246727\n",
      "Gradient for fc1.bias: 0.03954419493675232\n",
      "Gradient for fc2.weight: 0.026376446709036827\n",
      "Gradient for fc2.bias: 0.012286844663321972\n",
      "torch.Size([114]) tensor(-1.4365e-05, grad_fn=<MaxBackward1>) tensor(-4.4013, grad_fn=<MinBackward1>)\n",
      "Episode 817, Loss: -0.00042195606511086226\n",
      "Gradient for fc1.weight: 0.018103161826729774\n",
      "Gradient for fc1.bias: 0.0715067908167839\n",
      "Gradient for fc2.weight: 0.04140275716781616\n",
      "Gradient for fc2.bias: 0.024086186662316322\n",
      "torch.Size([64]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-3.6197, grad_fn=<MinBackward1>)\n",
      "Episode 818, Loss: -0.15202996134757996\n",
      "Gradient for fc1.weight: 0.05558878183364868\n",
      "Gradient for fc1.bias: 0.10819878429174423\n",
      "Gradient for fc2.weight: 0.08857329934835434\n",
      "Gradient for fc2.bias: 0.04280461370944977\n",
      "torch.Size([35]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-0.8841, grad_fn=<MinBackward1>)\n",
      "Episode 819, Loss: 0.011594787240028381\n",
      "Gradient for fc1.weight: 0.015747936442494392\n",
      "Gradient for fc1.bias: 0.03247887268662453\n",
      "Gradient for fc2.weight: 0.007787427399307489\n",
      "Gradient for fc2.bias: 0.0044050742872059345\n",
      "torch.Size([54]) tensor(-3.3856e-05, grad_fn=<MaxBackward1>) tensor(-4.1966, grad_fn=<MinBackward1>)\n",
      "Episode 820, Loss: -0.01279271487146616\n",
      "Gradient for fc1.weight: 0.02641567587852478\n",
      "Gradient for fc1.bias: 0.19690680503845215\n",
      "Gradient for fc2.weight: 0.11639347672462463\n",
      "Gradient for fc2.bias: 0.06896781921386719\n",
      "torch.Size([52]) tensor(-5.9428e-05, grad_fn=<MaxBackward1>) tensor(-2.2959, grad_fn=<MinBackward1>)\n",
      "Episode 821, Loss: -0.06376934796571732\n",
      "Gradient for fc1.weight: 0.02798190526664257\n",
      "Gradient for fc1.bias: 0.04082346707582474\n",
      "Gradient for fc2.weight: 0.026989474892616272\n",
      "Gradient for fc2.bias: 0.012291355058550835\n",
      "torch.Size([33]) tensor(-7.6655e-05, grad_fn=<MaxBackward1>) tensor(-4.0055, grad_fn=<MinBackward1>)\n",
      "Episode 822, Loss: -0.13197466731071472\n",
      "Gradient for fc1.weight: 0.20199768245220184\n",
      "Gradient for fc1.bias: 0.4766015112400055\n",
      "Gradient for fc2.weight: 0.3037438988685608\n",
      "Gradient for fc2.bias: 0.16970433294773102\n",
      "torch.Size([35]) tensor(-0.0004, grad_fn=<MaxBackward1>) tensor(-0.7794, grad_fn=<MinBackward1>)\n",
      "Episode 823, Loss: 0.0029576539527624846\n",
      "Gradient for fc1.weight: 0.024430816993117332\n",
      "Gradient for fc1.bias: 0.12043876945972443\n",
      "Gradient for fc2.weight: 0.06596844643354416\n",
      "Gradient for fc2.bias: 0.03960644081234932\n",
      "torch.Size([45]) tensor(-5.1142e-05, grad_fn=<MaxBackward1>) tensor(-3.8440, grad_fn=<MinBackward1>)\n",
      "Episode 824, Loss: -0.11361142247915268\n",
      "Gradient for fc1.weight: 0.04955673590302467\n",
      "Gradient for fc1.bias: 0.09629947692155838\n",
      "Gradient for fc2.weight: 0.05871867761015892\n",
      "Gradient for fc2.bias: 0.009338684380054474\n",
      "torch.Size([58]) tensor(-6.9382e-05, grad_fn=<MaxBackward1>) tensor(-3.3728, grad_fn=<MinBackward1>)\n",
      "Episode 825, Loss: -0.0442972332239151\n",
      "Gradient for fc1.weight: 0.012751596979796886\n",
      "Gradient for fc1.bias: 0.02433743141591549\n",
      "Gradient for fc2.weight: 0.02061649225652218\n",
      "Gradient for fc2.bias: 0.006059733685106039\n",
      "torch.Size([40]) tensor(-4.2082e-05, grad_fn=<MaxBackward1>) tensor(-2.7873, grad_fn=<MinBackward1>)\n",
      "Episode 826, Loss: 0.17135266959667206\n",
      "Gradient for fc1.weight: 0.020030399784445763\n",
      "Gradient for fc1.bias: 0.25271075963974\n",
      "Gradient for fc2.weight: 0.14308342337608337\n",
      "Gradient for fc2.bias: 0.0823364332318306\n",
      "torch.Size([59]) tensor(-5.9130e-05, grad_fn=<MaxBackward1>) tensor(-1.5919, grad_fn=<MinBackward1>)\n",
      "Episode 827, Loss: -0.07689455151557922\n",
      "Gradient for fc1.weight: 0.005202572792768478\n",
      "Gradient for fc1.bias: 0.023477867245674133\n",
      "Gradient for fc2.weight: 0.01189512386918068\n",
      "Gradient for fc2.bias: 0.006564606446772814\n",
      "torch.Size([35]) tensor(-9.0067e-05, grad_fn=<MaxBackward1>) tensor(-1.7175, grad_fn=<MinBackward1>)\n",
      "Episode 828, Loss: 0.07838579267263412\n",
      "Gradient for fc1.weight: 0.03194407373666763\n",
      "Gradient for fc1.bias: 0.062215615063905716\n",
      "Gradient for fc2.weight: 0.04287144914269447\n",
      "Gradient for fc2.bias: 0.02067193016409874\n",
      "torch.Size([157]) tensor(-5.9845e-05, grad_fn=<MaxBackward1>) tensor(-1.9306, grad_fn=<MinBackward1>)\n",
      "Episode 829, Loss: -0.012026706710457802\n",
      "Gradient for fc1.weight: 0.02469530887901783\n",
      "Gradient for fc1.bias: 0.059953637421131134\n",
      "Gradient for fc2.weight: 0.048199813812971115\n",
      "Gradient for fc2.bias: 0.02115161530673504\n",
      "torch.Size([33]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-1.0913, grad_fn=<MinBackward1>)\n",
      "Episode 830, Loss: 0.025918902829289436\n",
      "Gradient for fc1.weight: 0.04771893098950386\n",
      "Gradient for fc1.bias: 0.04521317407488823\n",
      "Gradient for fc2.weight: 0.046232715249061584\n",
      "Gradient for fc2.bias: 0.0037792366929352283\n",
      "torch.Size([56]) tensor(-2.3365e-05, grad_fn=<MaxBackward1>) tensor(-4.6067, grad_fn=<MinBackward1>)\n",
      "Episode 831, Loss: -0.14203599095344543\n",
      "Gradient for fc1.weight: 0.04915137216448784\n",
      "Gradient for fc1.bias: 0.12458116561174393\n",
      "Gradient for fc2.weight: 0.08217297494411469\n",
      "Gradient for fc2.bias: 0.02570727840065956\n",
      "torch.Size([97]) tensor(-2.9326e-05, grad_fn=<MaxBackward1>) tensor(-3.1631, grad_fn=<MinBackward1>)\n",
      "Episode 832, Loss: 0.019061202183365822\n",
      "Gradient for fc1.weight: 0.0136638842523098\n",
      "Gradient for fc1.bias: 0.037486732006073\n",
      "Gradient for fc2.weight: 0.028961287811398506\n",
      "Gradient for fc2.bias: 0.012582291848957539\n",
      "torch.Size([54]) tensor(-2.7538e-05, grad_fn=<MaxBackward1>) tensor(-3.7206, grad_fn=<MinBackward1>)\n",
      "Episode 833, Loss: -0.044425126165151596\n",
      "Gradient for fc1.weight: 0.042656201869249344\n",
      "Gradient for fc1.bias: 0.07578284293413162\n",
      "Gradient for fc2.weight: 0.058461736887693405\n",
      "Gradient for fc2.bias: 0.02603844553232193\n",
      "torch.Size([76]) tensor(-2.0862e-05, grad_fn=<MaxBackward1>) tensor(-3.7152, grad_fn=<MinBackward1>)\n",
      "Episode 834, Loss: 0.0021757029462605715\n",
      "Gradient for fc1.weight: 0.04083815962076187\n",
      "Gradient for fc1.bias: 0.17143389582633972\n",
      "Gradient for fc2.weight: 0.10394477844238281\n",
      "Gradient for fc2.bias: 0.05948247015476227\n",
      "torch.Size([95]) tensor(-3.7194e-05, grad_fn=<MaxBackward1>) tensor(-3.0941, grad_fn=<MinBackward1>)\n",
      "Episode 835, Loss: 0.037171658128499985\n",
      "Gradient for fc1.weight: 0.0075354548171162605\n",
      "Gradient for fc1.bias: 0.03425053134560585\n",
      "Gradient for fc2.weight: 0.018834881484508514\n",
      "Gradient for fc2.bias: 0.00975173432379961\n",
      "torch.Size([65]) tensor(-2.8372e-05, grad_fn=<MaxBackward1>) tensor(-2.4001, grad_fn=<MinBackward1>)\n",
      "Episode 836, Loss: -0.10610127449035645\n",
      "Gradient for fc1.weight: 0.0305855143815279\n",
      "Gradient for fc1.bias: 0.11324374377727509\n",
      "Gradient for fc2.weight: 0.08184193074703217\n",
      "Gradient for fc2.bias: 0.03978386148810387\n",
      "torch.Size([45]) tensor(-3.7134e-05, grad_fn=<MaxBackward1>) tensor(-3.5471, grad_fn=<MinBackward1>)\n",
      "Episode 837, Loss: -0.038421254605054855\n",
      "Gradient for fc1.weight: 0.03390328213572502\n",
      "Gradient for fc1.bias: 0.13180045783519745\n",
      "Gradient for fc2.weight: 0.08575524389743805\n",
      "Gradient for fc2.bias: 0.04554435610771179\n",
      "torch.Size([37]) tensor(-3.4273e-05, grad_fn=<MaxBackward1>) tensor(-2.7812, grad_fn=<MinBackward1>)\n",
      "Episode 838, Loss: 0.0748404935002327\n",
      "Gradient for fc1.weight: 0.02550034411251545\n",
      "Gradient for fc1.bias: 0.1401580274105072\n",
      "Gradient for fc2.weight: 0.07896550744771957\n",
      "Gradient for fc2.bias: 0.0442131832242012\n",
      "torch.Size([67]) tensor(-5.0188e-05, grad_fn=<MaxBackward1>) tensor(-2.1645, grad_fn=<MinBackward1>)\n",
      "Episode 839, Loss: 0.06045408546924591\n",
      "Gradient for fc1.weight: 0.030015859752893448\n",
      "Gradient for fc1.bias: 0.17461025714874268\n",
      "Gradient for fc2.weight: 0.10086464136838913\n",
      "Gradient for fc2.bias: 0.056705866008996964\n",
      "torch.Size([69]) tensor(-2.9803e-05, grad_fn=<MaxBackward1>) tensor(-2.8605, grad_fn=<MinBackward1>)\n",
      "Episode 840, Loss: 0.0750705748796463\n",
      "Gradient for fc1.weight: 0.02593584544956684\n",
      "Gradient for fc1.bias: 0.04851328581571579\n",
      "Gradient for fc2.weight: 0.043390847742557526\n",
      "Gradient for fc2.bias: 0.015331895090639591\n",
      "torch.Size([77]) tensor(-6.2050e-05, grad_fn=<MaxBackward1>) tensor(-1.6811, grad_fn=<MinBackward1>)\n",
      "Episode 841, Loss: -0.024902189150452614\n",
      "Gradient for fc1.weight: 0.020787253975868225\n",
      "Gradient for fc1.bias: 0.11806422472000122\n",
      "Gradient for fc2.weight: 0.06425663083791733\n",
      "Gradient for fc2.bias: 0.03652340918779373\n",
      "torch.Size([53]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-0.8476, grad_fn=<MinBackward1>)\n",
      "Episode 842, Loss: -0.0181050356477499\n",
      "Gradient for fc1.weight: 0.04040150344371796\n",
      "Gradient for fc1.bias: 0.02926657907664776\n",
      "Gradient for fc2.weight: 0.04278488829731941\n",
      "Gradient for fc2.bias: 0.0025752363726496696\n",
      "torch.Size([48]) tensor(-6.5565e-06, grad_fn=<MaxBackward1>) tensor(-5.3477, grad_fn=<MinBackward1>)\n",
      "Episode 843, Loss: -0.14085017144680023\n",
      "Gradient for fc1.weight: 0.11285530030727386\n",
      "Gradient for fc1.bias: 0.185343399643898\n",
      "Gradient for fc2.weight: 0.16728994250297546\n",
      "Gradient for fc2.bias: 0.0719994306564331\n",
      "torch.Size([143]) tensor(-2.3485e-05, grad_fn=<MaxBackward1>) tensor(-3.3189, grad_fn=<MinBackward1>)\n",
      "Episode 844, Loss: 0.020151278004050255\n",
      "Gradient for fc1.weight: 0.015885241329669952\n",
      "Gradient for fc1.bias: 0.052057284861803055\n",
      "Gradient for fc2.weight: 0.03374011069536209\n",
      "Gradient for fc2.bias: 0.01470770314335823\n",
      "torch.Size([42]) tensor(-9.0484e-05, grad_fn=<MaxBackward1>) tensor(-0.9490, grad_fn=<MinBackward1>)\n",
      "Episode 845, Loss: 0.021305538713932037\n",
      "Gradient for fc1.weight: 0.012263862416148186\n",
      "Gradient for fc1.bias: 0.07435981184244156\n",
      "Gradient for fc2.weight: 0.0410710833966732\n",
      "Gradient for fc2.bias: 0.022911079227924347\n",
      "torch.Size([95]) tensor(-6.6938e-05, grad_fn=<MaxBackward1>) tensor(-1.6253, grad_fn=<MinBackward1>)\n",
      "Episode 846, Loss: 0.05045168101787567\n",
      "Gradient for fc1.weight: 0.029520152136683464\n",
      "Gradient for fc1.bias: 0.07934841513633728\n",
      "Gradient for fc2.weight: 0.05087690427899361\n",
      "Gradient for fc2.bias: 0.0263865664601326\n",
      "torch.Size([88]) tensor(-5.4240e-06, grad_fn=<MaxBackward1>) tensor(-5.3233, grad_fn=<MinBackward1>)\n",
      "Episode 847, Loss: -0.005334594752639532\n",
      "Gradient for fc1.weight: 0.023320073261857033\n",
      "Gradient for fc1.bias: 0.09826388955116272\n",
      "Gradient for fc2.weight: 0.04625781998038292\n",
      "Gradient for fc2.bias: 0.02805342711508274\n",
      "torch.Size([41]) tensor(-2.9445e-05, grad_fn=<MaxBackward1>) tensor(-2.6031, grad_fn=<MinBackward1>)\n",
      "Episode 848, Loss: 0.06110316514968872\n",
      "Gradient for fc1.weight: 0.014927527867257595\n",
      "Gradient for fc1.bias: 0.0360211506485939\n",
      "Gradient for fc2.weight: 0.01975354179739952\n",
      "Gradient for fc2.bias: 0.006014344748109579\n",
      "torch.Size([79]) tensor(-5.2334e-05, grad_fn=<MaxBackward1>) tensor(-1.9326, grad_fn=<MinBackward1>)\n",
      "Episode 849, Loss: 0.0020978415850549936\n",
      "Gradient for fc1.weight: 0.026767004281282425\n",
      "Gradient for fc1.bias: 0.054837632924318314\n",
      "Gradient for fc2.weight: 0.03554408252239227\n",
      "Gradient for fc2.bias: 0.010251135565340519\n",
      "torch.Size([75]) tensor(-3.9578e-05, grad_fn=<MaxBackward1>) tensor(-2.0954, grad_fn=<MinBackward1>)\n",
      "Episode 850, Loss: 0.07654999196529388\n",
      "Gradient for fc1.weight: 0.010256905108690262\n",
      "Gradient for fc1.bias: 0.05841509997844696\n",
      "Gradient for fc2.weight: 0.033129557967185974\n",
      "Gradient for fc2.bias: 0.016960719600319862\n",
      "torch.Size([81]) tensor(-2.5750e-05, grad_fn=<MaxBackward1>) tensor(-3.8238, grad_fn=<MinBackward1>)\n",
      "Episode 851, Loss: 0.011666611768305302\n",
      "Gradient for fc1.weight: 0.08015616983175278\n",
      "Gradient for fc1.bias: 0.12089467793703079\n",
      "Gradient for fc2.weight: 0.09950602799654007\n",
      "Gradient for fc2.bias: 0.017308291047811508\n",
      "torch.Size([38]) tensor(-6.2348e-05, grad_fn=<MaxBackward1>) tensor(-1.7511, grad_fn=<MinBackward1>)\n",
      "Episode 852, Loss: 0.0992283746600151\n",
      "Gradient for fc1.weight: 0.05517341196537018\n",
      "Gradient for fc1.bias: 0.1694086790084839\n",
      "Gradient for fc2.weight: 0.11063773930072784\n",
      "Gradient for fc2.bias: 0.05665946751832962\n",
      "torch.Size([53]) tensor(-8.7086e-05, grad_fn=<MaxBackward1>) tensor(-1.2590, grad_fn=<MinBackward1>)\n",
      "Episode 853, Loss: 0.05684136599302292\n",
      "Gradient for fc1.weight: 0.015066607855260372\n",
      "Gradient for fc1.bias: 0.021290965378284454\n",
      "Gradient for fc2.weight: 0.013711616396903992\n",
      "Gradient for fc2.bias: 0.001858977833762765\n",
      "torch.Size([51]) tensor(-3.3975e-05, grad_fn=<MaxBackward1>) tensor(-2.2150, grad_fn=<MinBackward1>)\n",
      "Episode 854, Loss: 0.10443335026502609\n",
      "Gradient for fc1.weight: 0.03153703361749649\n",
      "Gradient for fc1.bias: 0.14419327676296234\n",
      "Gradient for fc2.weight: 0.08383183181285858\n",
      "Gradient for fc2.bias: 0.045374371111392975\n",
      "torch.Size([50]) tensor(-5.1738e-05, grad_fn=<MaxBackward1>) tensor(-1.5312, grad_fn=<MinBackward1>)\n",
      "Episode 855, Loss: 0.03994772955775261\n",
      "Gradient for fc1.weight: 0.01631500944495201\n",
      "Gradient for fc1.bias: 0.03113042376935482\n",
      "Gradient for fc2.weight: 0.022634094581007957\n",
      "Gradient for fc2.bias: 0.004592462908476591\n",
      "torch.Size([64]) tensor(-1.7703e-05, grad_fn=<MaxBackward1>) tensor(-2.8732, grad_fn=<MinBackward1>)\n",
      "Episode 856, Loss: 0.01714102178812027\n",
      "Gradient for fc1.weight: 0.019889360293745995\n",
      "Gradient for fc1.bias: 0.08853248506784439\n",
      "Gradient for fc2.weight: 0.038224708288908005\n",
      "Gradient for fc2.bias: 0.023049509152770042\n",
      "torch.Size([87]) tensor(-4.2201e-05, grad_fn=<MaxBackward1>) tensor(-1.9170, grad_fn=<MinBackward1>)\n",
      "Episode 857, Loss: -0.00998151395469904\n",
      "Gradient for fc1.weight: 0.03587557375431061\n",
      "Gradient for fc1.bias: 0.15229925513267517\n",
      "Gradient for fc2.weight: 0.0904463455080986\n",
      "Gradient for fc2.bias: 0.049840234220027924\n",
      "torch.Size([34]) tensor(-8.8576e-05, grad_fn=<MaxBackward1>) tensor(-0.7603, grad_fn=<MinBackward1>)\n",
      "Episode 858, Loss: 0.02193014696240425\n",
      "Gradient for fc1.weight: 0.042717449367046356\n",
      "Gradient for fc1.bias: 0.089524045586586\n",
      "Gradient for fc2.weight: 0.05611877143383026\n",
      "Gradient for fc2.bias: 0.01615602895617485\n",
      "torch.Size([48]) tensor(-9.1318e-05, grad_fn=<MaxBackward1>) tensor(-0.7855, grad_fn=<MinBackward1>)\n",
      "Episode 859, Loss: -0.007932471111416817\n",
      "Gradient for fc1.weight: 0.03346802666783333\n",
      "Gradient for fc1.bias: 0.052880287170410156\n",
      "Gradient for fc2.weight: 0.03990757092833519\n",
      "Gradient for fc2.bias: 0.006898338906466961\n",
      "torch.Size([65]) tensor(-4.1605e-05, grad_fn=<MaxBackward1>) tensor(-2.1546, grad_fn=<MinBackward1>)\n",
      "Episode 860, Loss: -0.08503168821334839\n",
      "Gradient for fc1.weight: 0.04550373926758766\n",
      "Gradient for fc1.bias: 0.1097143292427063\n",
      "Gradient for fc2.weight: 0.07338737696409225\n",
      "Gradient for fc2.bias: 0.03524018079042435\n",
      "torch.Size([68]) tensor(-6.3181e-06, grad_fn=<MaxBackward1>) tensor(-4.7356, grad_fn=<MinBackward1>)\n",
      "Episode 861, Loss: -0.1483851671218872\n",
      "Gradient for fc1.weight: 0.06950400769710541\n",
      "Gradient for fc1.bias: 0.2250387966632843\n",
      "Gradient for fc2.weight: 0.16156744956970215\n",
      "Gradient for fc2.bias: 0.08018222451210022\n",
      "torch.Size([41]) tensor(-4.5181e-05, grad_fn=<MaxBackward1>) tensor(-1.7461, grad_fn=<MinBackward1>)\n",
      "Episode 862, Loss: 0.059386007487773895\n",
      "Gradient for fc1.weight: 0.05919746682047844\n",
      "Gradient for fc1.bias: 0.2847309410572052\n",
      "Gradient for fc2.weight: 0.1651047170162201\n",
      "Gradient for fc2.bias: 0.09214190393686295\n",
      "torch.Size([79]) tensor(-2.4736e-05, grad_fn=<MaxBackward1>) tensor(-2.9153, grad_fn=<MinBackward1>)\n",
      "Episode 863, Loss: -0.01376346219331026\n",
      "Gradient for fc1.weight: 0.07327432930469513\n",
      "Gradient for fc1.bias: 0.3290551006793976\n",
      "Gradient for fc2.weight: 0.19898216426372528\n",
      "Gradient for fc2.bias: 0.10874021798372269\n",
      "torch.Size([58]) tensor(-6.1752e-05, grad_fn=<MaxBackward1>) tensor(-1.0602, grad_fn=<MinBackward1>)\n",
      "Episode 864, Loss: -0.015688400715589523\n",
      "Gradient for fc1.weight: 0.039882201701402664\n",
      "Gradient for fc1.bias: 0.13885629177093506\n",
      "Gradient for fc2.weight: 0.10005486756563187\n",
      "Gradient for fc2.bias: 0.046064071357250214\n",
      "torch.Size([61]) tensor(-6.8846e-05, grad_fn=<MaxBackward1>) tensor(-1.3147, grad_fn=<MinBackward1>)\n",
      "Episode 865, Loss: 0.09326722472906113\n",
      "Gradient for fc1.weight: 0.047555699944496155\n",
      "Gradient for fc1.bias: 0.1308111697435379\n",
      "Gradient for fc2.weight: 0.08237458020448685\n",
      "Gradient for fc2.bias: 0.0427822470664978\n",
      "torch.Size([46]) tensor(-3.3975e-05, grad_fn=<MaxBackward1>) tensor(-2.3800, grad_fn=<MinBackward1>)\n",
      "Episode 866, Loss: 0.08115072548389435\n",
      "Gradient for fc1.weight: 0.026010531932115555\n",
      "Gradient for fc1.bias: 0.06676211208105087\n",
      "Gradient for fc2.weight: 0.0443928986787796\n",
      "Gradient for fc2.bias: 0.017695585265755653\n",
      "torch.Size([55]) tensor(-6.6104e-05, grad_fn=<MaxBackward1>) tensor(-1.2446, grad_fn=<MinBackward1>)\n",
      "Episode 867, Loss: 0.07986126095056534\n",
      "Gradient for fc1.weight: 0.023976530879735947\n",
      "Gradient for fc1.bias: 0.19050924479961395\n",
      "Gradient for fc2.weight: 0.10572447627782822\n",
      "Gradient for fc2.bias: 0.05973387509584427\n",
      "torch.Size([61]) tensor(-4.0651e-05, grad_fn=<MaxBackward1>) tensor(-1.7255, grad_fn=<MinBackward1>)\n",
      "Episode 868, Loss: 0.05463865026831627\n",
      "Gradient for fc1.weight: 0.02663896232843399\n",
      "Gradient for fc1.bias: 0.03378715738654137\n",
      "Gradient for fc2.weight: 0.028609849512577057\n",
      "Gradient for fc2.bias: 0.004608997143805027\n",
      "torch.Size([43]) tensor(-1.5378e-05, grad_fn=<MaxBackward1>) tensor(-3.4425, grad_fn=<MinBackward1>)\n",
      "Episode 869, Loss: 0.09468211978673935\n",
      "Gradient for fc1.weight: 0.03758853301405907\n",
      "Gradient for fc1.bias: 0.11131943017244339\n",
      "Gradient for fc2.weight: 0.07179396599531174\n",
      "Gradient for fc2.bias: 0.02889251336455345\n",
      "torch.Size([53]) tensor(-5.9845e-05, grad_fn=<MaxBackward1>) tensor(-1.4146, grad_fn=<MinBackward1>)\n",
      "Episode 870, Loss: 0.0172142181545496\n",
      "Gradient for fc1.weight: 0.012881571426987648\n",
      "Gradient for fc1.bias: 0.08176274597644806\n",
      "Gradient for fc2.weight: 0.042689379304647446\n",
      "Gradient for fc2.bias: 0.024108583107590675\n",
      "torch.Size([57]) tensor(-7.6058e-05, grad_fn=<MaxBackward1>) tensor(-0.9664, grad_fn=<MinBackward1>)\n",
      "Episode 871, Loss: -0.04381273686885834\n",
      "Gradient for fc1.weight: 0.03839276358485222\n",
      "Gradient for fc1.bias: 0.07214467227458954\n",
      "Gradient for fc2.weight: 0.06309002637863159\n",
      "Gradient for fc2.bias: 0.024046726524829865\n",
      "torch.Size([40]) tensor(-3.0935e-05, grad_fn=<MaxBackward1>) tensor(-2.5767, grad_fn=<MinBackward1>)\n",
      "Episode 872, Loss: -0.056555431336164474\n",
      "Gradient for fc1.weight: 0.027686642482876778\n",
      "Gradient for fc1.bias: 0.024657225236296654\n",
      "Gradient for fc2.weight: 0.03069491684436798\n",
      "Gradient for fc2.bias: 0.00604682182893157\n",
      "torch.Size([69]) tensor(-2.7419e-05, grad_fn=<MaxBackward1>) tensor(-2.2357, grad_fn=<MinBackward1>)\n",
      "Episode 873, Loss: 0.01602603867650032\n",
      "Gradient for fc1.weight: 0.039566341787576675\n",
      "Gradient for fc1.bias: 0.07866579294204712\n",
      "Gradient for fc2.weight: 0.05111590027809143\n",
      "Gradient for fc2.bias: 0.02513202652335167\n",
      "torch.Size([38]) tensor(-6.0977e-05, grad_fn=<MaxBackward1>) tensor(-1.3628, grad_fn=<MinBackward1>)\n",
      "Episode 874, Loss: 0.014037790708243847\n",
      "Gradient for fc1.weight: 0.04441206157207489\n",
      "Gradient for fc1.bias: 0.08270499110221863\n",
      "Gradient for fc2.weight: 0.06204148009419441\n",
      "Gradient for fc2.bias: 0.02158006653189659\n",
      "torch.Size([45]) tensor(-5.1619e-05, grad_fn=<MaxBackward1>) tensor(-1.7516, grad_fn=<MinBackward1>)\n",
      "Episode 875, Loss: -0.03205733746290207\n",
      "Gradient for fc1.weight: 0.039701931178569794\n",
      "Gradient for fc1.bias: 0.07376744598150253\n",
      "Gradient for fc2.weight: 0.04971164092421532\n",
      "Gradient for fc2.bias: 0.02346990816295147\n",
      "torch.Size([46]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-1.3291, grad_fn=<MinBackward1>)\n",
      "Episode 876, Loss: -0.01994144171476364\n",
      "Gradient for fc1.weight: 0.015119955874979496\n",
      "Gradient for fc1.bias: 0.07719949632883072\n",
      "Gradient for fc2.weight: 0.038163259625434875\n",
      "Gradient for fc2.bias: 0.021545734256505966\n",
      "torch.Size([57]) tensor(-1.6391e-05, grad_fn=<MaxBackward1>) tensor(-2.6852, grad_fn=<MinBackward1>)\n",
      "Episode 877, Loss: -0.07514259219169617\n",
      "Gradient for fc1.weight: 0.03754787892103195\n",
      "Gradient for fc1.bias: 0.0498000867664814\n",
      "Gradient for fc2.weight: 0.054631028324365616\n",
      "Gradient for fc2.bias: 0.01686154678463936\n",
      "torch.Size([35]) tensor(-5.0069e-05, grad_fn=<MaxBackward1>) tensor(-1.7987, grad_fn=<MinBackward1>)\n",
      "Episode 878, Loss: -0.05307241156697273\n",
      "Gradient for fc1.weight: 0.031452201306819916\n",
      "Gradient for fc1.bias: 0.14571328461170197\n",
      "Gradient for fc2.weight: 0.07463076710700989\n",
      "Gradient for fc2.bias: 0.039436668157577515\n",
      "torch.Size([40]) tensor(-2.6822e-05, grad_fn=<MaxBackward1>) tensor(-2.9326, grad_fn=<MinBackward1>)\n",
      "Episode 879, Loss: -0.02528880536556244\n",
      "Gradient for fc1.weight: 0.08294244855642319\n",
      "Gradient for fc1.bias: 0.24144823849201202\n",
      "Gradient for fc2.weight: 0.14855435490608215\n",
      "Gradient for fc2.bias: 0.07873973995447159\n",
      "torch.Size([71]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-0.9019, grad_fn=<MinBackward1>)\n",
      "Episode 880, Loss: -0.039433080703020096\n",
      "Gradient for fc1.weight: 0.010260002687573433\n",
      "Gradient for fc1.bias: 0.014487705193459988\n",
      "Gradient for fc2.weight: 0.008525632321834564\n",
      "Gradient for fc2.bias: 0.0021119697485119104\n",
      "torch.Size([61]) tensor(-8.5000e-05, grad_fn=<MaxBackward1>) tensor(-1.0468, grad_fn=<MinBackward1>)\n",
      "Episode 881, Loss: -0.042195528745651245\n",
      "Gradient for fc1.weight: 0.04031352698802948\n",
      "Gradient for fc1.bias: 0.10349150747060776\n",
      "Gradient for fc2.weight: 0.06289872527122498\n",
      "Gradient for fc2.bias: 0.03313901275396347\n",
      "torch.Size([83]) tensor(-1.0312e-05, grad_fn=<MaxBackward1>) tensor(-3.6368, grad_fn=<MinBackward1>)\n",
      "Episode 882, Loss: 0.026881996542215347\n",
      "Gradient for fc1.weight: 0.024879686534404755\n",
      "Gradient for fc1.bias: 0.019389908760786057\n",
      "Gradient for fc2.weight: 0.02914387732744217\n",
      "Gradient for fc2.bias: 0.004290363285690546\n",
      "torch.Size([52]) tensor(-8.7023e-06, grad_fn=<MaxBackward1>) tensor(-4.4926, grad_fn=<MinBackward1>)\n",
      "Episode 883, Loss: -0.08810405433177948\n",
      "Gradient for fc1.weight: 0.00952500756829977\n",
      "Gradient for fc1.bias: 0.060335252434015274\n",
      "Gradient for fc2.weight: 0.03699339181184769\n",
      "Gradient for fc2.bias: 0.01945672184228897\n",
      "torch.Size([60]) tensor(-4.6850e-05, grad_fn=<MaxBackward1>) tensor(-1.5441, grad_fn=<MinBackward1>)\n",
      "Episode 884, Loss: -0.016825852915644646\n",
      "Gradient for fc1.weight: 0.026411190629005432\n",
      "Gradient for fc1.bias: 0.046187449246644974\n",
      "Gradient for fc2.weight: 0.032306548207998276\n",
      "Gradient for fc2.bias: 0.015063632279634476\n",
      "torch.Size([60]) tensor(-8.1185e-05, grad_fn=<MaxBackward1>) tensor(-1.3018, grad_fn=<MinBackward1>)\n",
      "Episode 885, Loss: -0.030901722609996796\n",
      "Gradient for fc1.weight: 0.01422814466059208\n",
      "Gradient for fc1.bias: 0.025767778977751732\n",
      "Gradient for fc2.weight: 0.015751514583826065\n",
      "Gradient for fc2.bias: 0.007184108719229698\n",
      "torch.Size([42]) tensor(-7.9695e-05, grad_fn=<MaxBackward1>) tensor(-1.2042, grad_fn=<MinBackward1>)\n",
      "Episode 886, Loss: -0.0045771291479468346\n",
      "Gradient for fc1.weight: 0.04787101969122887\n",
      "Gradient for fc1.bias: 0.08813176304101944\n",
      "Gradient for fc2.weight: 0.05711246281862259\n",
      "Gradient for fc2.bias: 0.02751586027443409\n",
      "torch.Size([56]) tensor(-3.0935e-05, grad_fn=<MaxBackward1>) tensor(-2.9351, grad_fn=<MinBackward1>)\n",
      "Episode 887, Loss: -0.10186272859573364\n",
      "Gradient for fc1.weight: 0.033295389264822006\n",
      "Gradient for fc1.bias: 0.04189760610461235\n",
      "Gradient for fc2.weight: 0.03467627242207527\n",
      "Gradient for fc2.bias: 0.011350908316671848\n",
      "torch.Size([59]) tensor(-2.2233e-05, grad_fn=<MaxBackward1>) tensor(-2.9589, grad_fn=<MinBackward1>)\n",
      "Episode 888, Loss: 0.06996918469667435\n",
      "Gradient for fc1.weight: 0.02412392944097519\n",
      "Gradient for fc1.bias: 0.09631343930959702\n",
      "Gradient for fc2.weight: 0.05407671257853508\n",
      "Gradient for fc2.bias: 0.030244475230574608\n",
      "torch.Size([50]) tensor(-5.3407e-05, grad_fn=<MaxBackward1>) tensor(-1.4143, grad_fn=<MinBackward1>)\n",
      "Episode 889, Loss: -0.006345590576529503\n",
      "Gradient for fc1.weight: 0.021409470587968826\n",
      "Gradient for fc1.bias: 0.04212323948740959\n",
      "Gradient for fc2.weight: 0.028194032609462738\n",
      "Gradient for fc2.bias: 0.013610100373625755\n",
      "torch.Size([78]) tensor(-5.5672e-05, grad_fn=<MaxBackward1>) tensor(-1.1462, grad_fn=<MinBackward1>)\n",
      "Episode 890, Loss: -0.03197142854332924\n",
      "Gradient for fc1.weight: 0.040919143706560135\n",
      "Gradient for fc1.bias: 0.03762853890657425\n",
      "Gradient for fc2.weight: 0.04917224496603012\n",
      "Gradient for fc2.bias: 0.012386308051645756\n",
      "torch.Size([85]) tensor(-7.6535e-05, grad_fn=<MaxBackward1>) tensor(-2.3298, grad_fn=<MinBackward1>)\n",
      "Episode 891, Loss: -0.11289460957050323\n",
      "Gradient for fc1.weight: 0.02024955302476883\n",
      "Gradient for fc1.bias: 0.062385667115449905\n",
      "Gradient for fc2.weight: 0.049857355654239655\n",
      "Gradient for fc2.bias: 0.019925400614738464\n",
      "torch.Size([81]) tensor(-2.6644e-05, grad_fn=<MaxBackward1>) tensor(-2.0353, grad_fn=<MinBackward1>)\n",
      "Episode 892, Loss: -0.056610848754644394\n",
      "Gradient for fc1.weight: 0.03424163535237312\n",
      "Gradient for fc1.bias: 0.06931901723146439\n",
      "Gradient for fc2.weight: 0.061345480382442474\n",
      "Gradient for fc2.bias: 0.02338319644331932\n",
      "torch.Size([40]) tensor(-1.4842e-05, grad_fn=<MaxBackward1>) tensor(-4.2090, grad_fn=<MinBackward1>)\n",
      "Episode 893, Loss: 0.040219224989414215\n",
      "Gradient for fc1.weight: 0.039704468101263046\n",
      "Gradient for fc1.bias: 0.06437105685472488\n",
      "Gradient for fc2.weight: 0.043661944568157196\n",
      "Gradient for fc2.bias: 0.018777908757328987\n",
      "torch.Size([91]) tensor(-5.1261e-05, grad_fn=<MaxBackward1>) tensor(-1.9363, grad_fn=<MinBackward1>)\n",
      "Episode 894, Loss: -0.053644195199012756\n",
      "Gradient for fc1.weight: 0.046472564339637756\n",
      "Gradient for fc1.bias: 0.039660632610321045\n",
      "Gradient for fc2.weight: 0.042843498289585114\n",
      "Gradient for fc2.bias: 0.01059935986995697\n",
      "torch.Size([50]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-0.9847, grad_fn=<MinBackward1>)\n",
      "Episode 895, Loss: -0.030532805249094963\n",
      "Gradient for fc1.weight: 0.02219834178686142\n",
      "Gradient for fc1.bias: 0.040387626737356186\n",
      "Gradient for fc2.weight: 0.024083517491817474\n",
      "Gradient for fc2.bias: 0.0056701586581766605\n",
      "torch.Size([62]) tensor(-5.6686e-05, grad_fn=<MaxBackward1>) tensor(-1.7670, grad_fn=<MinBackward1>)\n",
      "Episode 896, Loss: -0.024601437151432037\n",
      "Gradient for fc1.weight: 0.0069175949320197105\n",
      "Gradient for fc1.bias: 0.021591730415821075\n",
      "Gradient for fc2.weight: 0.009822619147598743\n",
      "Gradient for fc2.bias: 0.0046795206144452095\n",
      "torch.Size([57]) tensor(-7.5102e-06, grad_fn=<MaxBackward1>) tensor(-4.2083, grad_fn=<MinBackward1>)\n",
      "Episode 897, Loss: 0.0381246879696846\n",
      "Gradient for fc1.weight: 0.048759978264570236\n",
      "Gradient for fc1.bias: 0.04832113906741142\n",
      "Gradient for fc2.weight: 0.05422220006585121\n",
      "Gradient for fc2.bias: 0.005667842458933592\n",
      "torch.Size([33]) tensor(-2.4200e-05, grad_fn=<MaxBackward1>) tensor(-3.1333, grad_fn=<MinBackward1>)\n",
      "Episode 898, Loss: -0.11748047173023224\n",
      "Gradient for fc1.weight: 0.1524076610803604\n",
      "Gradient for fc1.bias: 0.31190556287765503\n",
      "Gradient for fc2.weight: 0.2047833800315857\n",
      "Gradient for fc2.bias: 0.09993625432252884\n",
      "torch.Size([38]) tensor(-3.5406e-05, grad_fn=<MaxBackward1>) tensor(-1.9596, grad_fn=<MinBackward1>)\n",
      "Episode 899, Loss: 0.048913829028606415\n",
      "Gradient for fc1.weight: 0.10635631531476974\n",
      "Gradient for fc1.bias: 0.2242702692747116\n",
      "Gradient for fc2.weight: 0.15122781693935394\n",
      "Gradient for fc2.bias: 0.07382158935070038\n",
      "torch.Size([57]) tensor(-3.7730e-05, grad_fn=<MaxBackward1>) tensor(-2.4148, grad_fn=<MinBackward1>)\n",
      "Episode 900, Loss: 0.009818775579333305\n",
      "Gradient for fc1.weight: 0.02728118747472763\n",
      "Gradient for fc1.bias: 0.25548675656318665\n",
      "Gradient for fc2.weight: 0.13891862332820892\n",
      "Gradient for fc2.bias: 0.07868117839097977\n",
      "Running reward: 42.64918501490457\n",
      "torch.Size([37]) tensor(-1.2875e-05, grad_fn=<MaxBackward1>) tensor(-4.0652, grad_fn=<MinBackward1>)\n",
      "Episode 901, Loss: 0.05556265637278557\n",
      "Gradient for fc1.weight: 0.02416939288377762\n",
      "Gradient for fc1.bias: 0.11645365506410599\n",
      "Gradient for fc2.weight: 0.06559997797012329\n",
      "Gradient for fc2.bias: 0.031205235049128532\n",
      "torch.Size([63]) tensor(-1.1683e-05, grad_fn=<MaxBackward1>) tensor(-4.2347, grad_fn=<MinBackward1>)\n",
      "Episode 902, Loss: 0.024087557569146156\n",
      "Gradient for fc1.weight: 0.08027596026659012\n",
      "Gradient for fc1.bias: 0.22607789933681488\n",
      "Gradient for fc2.weight: 0.18629853427410126\n",
      "Gradient for fc2.bias: 0.07579324394464493\n",
      "torch.Size([46]) tensor(-8.4285e-05, grad_fn=<MaxBackward1>) tensor(-2.4134, grad_fn=<MinBackward1>)\n",
      "Episode 903, Loss: -0.11925694346427917\n",
      "Gradient for fc1.weight: 0.07267022877931595\n",
      "Gradient for fc1.bias: 0.1539762169122696\n",
      "Gradient for fc2.weight: 0.1197093203663826\n",
      "Gradient for fc2.bias: 0.0506325401365757\n",
      "torch.Size([65]) tensor(-4.6552e-05, grad_fn=<MaxBackward1>) tensor(-1.9150, grad_fn=<MinBackward1>)\n",
      "Episode 904, Loss: -0.026672786101698875\n",
      "Gradient for fc1.weight: 0.013863871805369854\n",
      "Gradient for fc1.bias: 0.038226302713155746\n",
      "Gradient for fc2.weight: 0.01968221925199032\n",
      "Gradient for fc2.bias: 0.010761886835098267\n",
      "torch.Size([39]) tensor(-8.4285e-05, grad_fn=<MaxBackward1>) tensor(-1.0510, grad_fn=<MinBackward1>)\n",
      "Episode 905, Loss: -0.005650151055306196\n",
      "Gradient for fc1.weight: 0.01893523894250393\n",
      "Gradient for fc1.bias: 0.05201706290245056\n",
      "Gradient for fc2.weight: 0.034436095505952835\n",
      "Gradient for fc2.bias: 0.01722737029194832\n",
      "torch.Size([47]) tensor(-8.1781e-05, grad_fn=<MaxBackward1>) tensor(-1.7110, grad_fn=<MinBackward1>)\n",
      "Episode 906, Loss: -0.06716932356357574\n",
      "Gradient for fc1.weight: 0.05009843409061432\n",
      "Gradient for fc1.bias: 0.22999891638755798\n",
      "Gradient for fc2.weight: 0.1241801381111145\n",
      "Gradient for fc2.bias: 0.07008900493383408\n",
      "torch.Size([86]) tensor(-4.9950e-05, grad_fn=<MaxBackward1>) tensor(-3.0178, grad_fn=<MinBackward1>)\n",
      "Episode 907, Loss: -0.08509726077318192\n",
      "Gradient for fc1.weight: 0.011786974966526031\n",
      "Gradient for fc1.bias: 0.033254630863666534\n",
      "Gradient for fc2.weight: 0.024949882179498672\n",
      "Gradient for fc2.bias: 0.01059432327747345\n",
      "torch.Size([57]) tensor(-4.1724e-05, grad_fn=<MaxBackward1>) tensor(-2.2742, grad_fn=<MinBackward1>)\n",
      "Episode 908, Loss: -0.06177130341529846\n",
      "Gradient for fc1.weight: 0.017380645498633385\n",
      "Gradient for fc1.bias: 0.057062745094299316\n",
      "Gradient for fc2.weight: 0.03556150570511818\n",
      "Gradient for fc2.bias: 0.01834334433078766\n",
      "torch.Size([38]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-0.7723, grad_fn=<MinBackward1>)\n",
      "Episode 909, Loss: -0.013205423019826412\n",
      "Gradient for fc1.weight: 0.03552284091711044\n",
      "Gradient for fc1.bias: 0.04461871460080147\n",
      "Gradient for fc2.weight: 0.04065136983990669\n",
      "Gradient for fc2.bias: 0.008013531565666199\n",
      "torch.Size([39]) tensor(-2.6107e-05, grad_fn=<MaxBackward1>) tensor(-2.4296, grad_fn=<MinBackward1>)\n",
      "Episode 910, Loss: -0.04304603114724159\n",
      "Gradient for fc1.weight: 0.14777837693691254\n",
      "Gradient for fc1.bias: 0.35436660051345825\n",
      "Gradient for fc2.weight: 0.23952630162239075\n",
      "Gradient for fc2.bias: 0.11780460178852081\n",
      "torch.Size([34]) tensor(-2.6584e-05, grad_fn=<MaxBackward1>) tensor(-3.3479, grad_fn=<MinBackward1>)\n",
      "Episode 911, Loss: -0.18317830562591553\n",
      "Gradient for fc1.weight: 0.09690071642398834\n",
      "Gradient for fc1.bias: 0.1331685483455658\n",
      "Gradient for fc2.weight: 0.12423162907361984\n",
      "Gradient for fc2.bias: 0.04451363906264305\n",
      "torch.Size([49]) tensor(-9.8651e-05, grad_fn=<MaxBackward1>) tensor(-0.9350, grad_fn=<MinBackward1>)\n",
      "Episode 912, Loss: -0.02289087139070034\n",
      "Gradient for fc1.weight: 0.03948794677853584\n",
      "Gradient for fc1.bias: 0.046080414205789566\n",
      "Gradient for fc2.weight: 0.04000021517276764\n",
      "Gradient for fc2.bias: 0.012260857969522476\n",
      "torch.Size([51]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-0.5855, grad_fn=<MinBackward1>)\n",
      "Episode 913, Loss: -0.03461141511797905\n",
      "Gradient for fc1.weight: 0.019166817888617516\n",
      "Gradient for fc1.bias: 0.025129567831754684\n",
      "Gradient for fc2.weight: 0.02197139896452427\n",
      "Gradient for fc2.bias: 0.006264545023441315\n",
      "torch.Size([66]) tensor(-2.1458e-05, grad_fn=<MaxBackward1>) tensor(-3.2569, grad_fn=<MinBackward1>)\n",
      "Episode 914, Loss: -0.06834322959184647\n",
      "Gradient for fc1.weight: 0.011310260742902756\n",
      "Gradient for fc1.bias: 0.042865265160799026\n",
      "Gradient for fc2.weight: 0.025463614612817764\n",
      "Gradient for fc2.bias: 0.012860463932156563\n",
      "torch.Size([47]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-0.5777, grad_fn=<MinBackward1>)\n",
      "Episode 915, Loss: -0.0009494855185039341\n",
      "Gradient for fc1.weight: 0.022282887250185013\n",
      "Gradient for fc1.bias: 0.10056951642036438\n",
      "Gradient for fc2.weight: 0.05586351826786995\n",
      "Gradient for fc2.bias: 0.03134015202522278\n",
      "torch.Size([41]) tensor(-2.6584e-05, grad_fn=<MaxBackward1>) tensor(-2.8221, grad_fn=<MinBackward1>)\n",
      "Episode 916, Loss: -0.022318892180919647\n",
      "Gradient for fc1.weight: 0.01734851859509945\n",
      "Gradient for fc1.bias: 0.05957777798175812\n",
      "Gradient for fc2.weight: 0.03561955317854881\n",
      "Gradient for fc2.bias: 0.01878897100687027\n",
      "torch.Size([52]) tensor(-7.0574e-05, grad_fn=<MaxBackward1>) tensor(-2.4029, grad_fn=<MinBackward1>)\n",
      "Episode 917, Loss: -0.07213205844163895\n",
      "Gradient for fc1.weight: 0.08373607695102692\n",
      "Gradient for fc1.bias: 0.07346592843532562\n",
      "Gradient for fc2.weight: 0.10844997316598892\n",
      "Gradient for fc2.bias: 0.023011209443211555\n",
      "torch.Size([40]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-0.5083, grad_fn=<MinBackward1>)\n",
      "Episode 918, Loss: 0.009281087666749954\n",
      "Gradient for fc1.weight: 0.018579620867967606\n",
      "Gradient for fc1.bias: 0.034107062965631485\n",
      "Gradient for fc2.weight: 0.02569885551929474\n",
      "Gradient for fc2.bias: 0.007147309370338917\n",
      "torch.Size([60]) tensor(-3.0160e-05, grad_fn=<MaxBackward1>) tensor(-2.6399, grad_fn=<MinBackward1>)\n",
      "Episode 919, Loss: -0.061062317341566086\n",
      "Gradient for fc1.weight: 0.04716186970472336\n",
      "Gradient for fc1.bias: 0.05678366869688034\n",
      "Gradient for fc2.weight: 0.052885156124830246\n",
      "Gradient for fc2.bias: 0.0157767403870821\n",
      "torch.Size([91]) tensor(-2.5869e-05, grad_fn=<MaxBackward1>) tensor(-3.3205, grad_fn=<MinBackward1>)\n",
      "Episode 920, Loss: 0.08784890919923782\n",
      "Gradient for fc1.weight: 0.06729134172201157\n",
      "Gradient for fc1.bias: 0.18605241179466248\n",
      "Gradient for fc2.weight: 0.1532502919435501\n",
      "Gradient for fc2.bias: 0.058713626116514206\n",
      "torch.Size([36]) tensor(-5.0248e-05, grad_fn=<MaxBackward1>) tensor(-1.6966, grad_fn=<MinBackward1>)\n",
      "Episode 921, Loss: 0.020225752145051956\n",
      "Gradient for fc1.weight: 0.050525568425655365\n",
      "Gradient for fc1.bias: 0.048792190849781036\n",
      "Gradient for fc2.weight: 0.05053716152906418\n",
      "Gradient for fc2.bias: 0.01078684814274311\n",
      "torch.Size([48]) tensor(-4.7923e-05, grad_fn=<MaxBackward1>) tensor(-2.4481, grad_fn=<MinBackward1>)\n",
      "Episode 922, Loss: -0.10254030674695969\n",
      "Gradient for fc1.weight: 0.04621407762169838\n",
      "Gradient for fc1.bias: 0.050934452563524246\n",
      "Gradient for fc2.weight: 0.051993824541568756\n",
      "Gradient for fc2.bias: 0.0037800490390509367\n",
      "torch.Size([135]) tensor(-7.1469e-05, grad_fn=<MaxBackward1>) tensor(-2.4591, grad_fn=<MinBackward1>)\n",
      "Episode 923, Loss: 0.014227415435016155\n",
      "Gradient for fc1.weight: 0.032662082463502884\n",
      "Gradient for fc1.bias: 0.03954850137233734\n",
      "Gradient for fc2.weight: 0.038951095193624496\n",
      "Gradient for fc2.bias: 0.01159566082060337\n",
      "torch.Size([35]) tensor(-3.8625e-05, grad_fn=<MaxBackward1>) tensor(-2.3013, grad_fn=<MinBackward1>)\n",
      "Episode 924, Loss: -0.023168735206127167\n",
      "Gradient for fc1.weight: 0.0702202096581459\n",
      "Gradient for fc1.bias: 0.10401704907417297\n",
      "Gradient for fc2.weight: 0.08194512128829956\n",
      "Gradient for fc2.bias: 0.031129127368330956\n",
      "torch.Size([104]) tensor(-3.3200e-05, grad_fn=<MaxBackward1>) tensor(-1.8202, grad_fn=<MinBackward1>)\n",
      "Episode 925, Loss: -0.051270388066768646\n",
      "Gradient for fc1.weight: 0.03988061100244522\n",
      "Gradient for fc1.bias: 0.06949866563081741\n",
      "Gradient for fc2.weight: 0.05290292203426361\n",
      "Gradient for fc2.bias: 0.022650685161352158\n",
      "torch.Size([108]) tensor(-4.9950e-05, grad_fn=<MaxBackward1>) tensor(-1.7729, grad_fn=<MinBackward1>)\n",
      "Episode 926, Loss: 0.03898444026708603\n",
      "Gradient for fc1.weight: 0.01590000092983246\n",
      "Gradient for fc1.bias: 0.03913047909736633\n",
      "Gradient for fc2.weight: 0.03388912230730057\n",
      "Gradient for fc2.bias: 0.012061304412782192\n",
      "torch.Size([116]) tensor(-9.3580e-06, grad_fn=<MaxBackward1>) tensor(-3.5967, grad_fn=<MinBackward1>)\n",
      "Episode 927, Loss: 0.0661894679069519\n",
      "Gradient for fc1.weight: 0.040628936141729355\n",
      "Gradient for fc1.bias: 0.09546289592981339\n",
      "Gradient for fc2.weight: 0.08792579174041748\n",
      "Gradient for fc2.bias: 0.031064948067069054\n",
      "torch.Size([54]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-0.6812, grad_fn=<MinBackward1>)\n",
      "Episode 928, Loss: -0.027888163924217224\n",
      "Gradient for fc1.weight: 0.025377165526151657\n",
      "Gradient for fc1.bias: 0.03095991164445877\n",
      "Gradient for fc2.weight: 0.03270011022686958\n",
      "Gradient for fc2.bias: 0.009896443225443363\n",
      "torch.Size([35]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-0.4159, grad_fn=<MinBackward1>)\n",
      "Episode 929, Loss: 0.00714713241904974\n",
      "Gradient for fc1.weight: 0.007797138765454292\n",
      "Gradient for fc1.bias: 0.03286439925432205\n",
      "Gradient for fc2.weight: 0.019031455740332603\n",
      "Gradient for fc2.bias: 0.009448966011404991\n",
      "torch.Size([50]) tensor(-4.1963e-05, grad_fn=<MaxBackward1>) tensor(-2.5223, grad_fn=<MinBackward1>)\n",
      "Episode 930, Loss: -0.0192246176302433\n",
      "Gradient for fc1.weight: 0.06686907261610031\n",
      "Gradient for fc1.bias: 0.19200293719768524\n",
      "Gradient for fc2.weight: 0.12177985906600952\n",
      "Gradient for fc2.bias: 0.06217842921614647\n",
      "torch.Size([65]) tensor(-8.2139e-05, grad_fn=<MaxBackward1>) tensor(-0.9205, grad_fn=<MinBackward1>)\n",
      "Episode 931, Loss: -0.027607856318354607\n",
      "Gradient for fc1.weight: 0.0084293307736516\n",
      "Gradient for fc1.bias: 0.013650941662490368\n",
      "Gradient for fc2.weight: 0.011601394973695278\n",
      "Gradient for fc2.bias: 0.0007147150463424623\n",
      "torch.Size([37]) tensor(-8.5834e-05, grad_fn=<MaxBackward1>) tensor(-1.1032, grad_fn=<MinBackward1>)\n",
      "Episode 932, Loss: 0.0206326674669981\n",
      "Gradient for fc1.weight: 0.024434367194771767\n",
      "Gradient for fc1.bias: 0.06549519300460815\n",
      "Gradient for fc2.weight: 0.03962252661585808\n",
      "Gradient for fc2.bias: 0.019715258851647377\n",
      "torch.Size([86]) tensor(-7.8678e-06, grad_fn=<MaxBackward1>) tensor(-5.3179, grad_fn=<MinBackward1>)\n",
      "Episode 933, Loss: -0.09180110692977905\n",
      "Gradient for fc1.weight: 0.023650681599974632\n",
      "Gradient for fc1.bias: 0.06974522769451141\n",
      "Gradient for fc2.weight: 0.05037236958742142\n",
      "Gradient for fc2.bias: 0.01981433667242527\n",
      "torch.Size([53]) tensor(-2.0266e-05, grad_fn=<MaxBackward1>) tensor(-4.1091, grad_fn=<MinBackward1>)\n",
      "Episode 934, Loss: -0.17370541393756866\n",
      "Gradient for fc1.weight: 0.0448908694088459\n",
      "Gradient for fc1.bias: 0.09650330990552902\n",
      "Gradient for fc2.weight: 0.08228340744972229\n",
      "Gradient for fc2.bias: 0.030489150434732437\n",
      "torch.Size([47]) tensor(-1.8954e-05, grad_fn=<MaxBackward1>) tensor(-2.6138, grad_fn=<MinBackward1>)\n",
      "Episode 935, Loss: 0.040942125022411346\n",
      "Gradient for fc1.weight: 0.09498011320829391\n",
      "Gradient for fc1.bias: 0.06973329186439514\n",
      "Gradient for fc2.weight: 0.11463561654090881\n",
      "Gradient for fc2.bias: 0.004778949078172445\n",
      "torch.Size([40]) tensor(-4.0651e-05, grad_fn=<MaxBackward1>) tensor(-2.3533, grad_fn=<MinBackward1>)\n",
      "Episode 936, Loss: -0.043714068830013275\n",
      "Gradient for fc1.weight: 0.02510494738817215\n",
      "Gradient for fc1.bias: 0.021394573152065277\n",
      "Gradient for fc2.weight: 0.02446637488901615\n",
      "Gradient for fc2.bias: 0.003658089553937316\n",
      "torch.Size([40]) tensor(-8.3447e-06, grad_fn=<MaxBackward1>) tensor(-4.9833, grad_fn=<MinBackward1>)\n",
      "Episode 937, Loss: -0.1976509839296341\n",
      "Gradient for fc1.weight: 0.0508442297577858\n",
      "Gradient for fc1.bias: 0.05863218382000923\n",
      "Gradient for fc2.weight: 0.05984853208065033\n",
      "Gradient for fc2.bias: 0.006489045452326536\n",
      "torch.Size([36]) tensor(-4.0890e-05, grad_fn=<MaxBackward1>) tensor(-2.8781, grad_fn=<MinBackward1>)\n",
      "Episode 938, Loss: -0.1078549474477768\n",
      "Gradient for fc1.weight: 0.0641111209988594\n",
      "Gradient for fc1.bias: 0.09033933281898499\n",
      "Gradient for fc2.weight: 0.07519615441560745\n",
      "Gradient for fc2.bias: 0.0034024736378341913\n",
      "torch.Size([54]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-0.9684, grad_fn=<MinBackward1>)\n",
      "Episode 939, Loss: -0.05123598128557205\n",
      "Gradient for fc1.weight: 0.050866540521383286\n",
      "Gradient for fc1.bias: 0.07360605895519257\n",
      "Gradient for fc2.weight: 0.06644463539123535\n",
      "Gradient for fc2.bias: 0.0239532720297575\n",
      "torch.Size([43]) tensor(-2.5094e-05, grad_fn=<MaxBackward1>) tensor(-2.0593, grad_fn=<MinBackward1>)\n",
      "Episode 940, Loss: 0.03557340055704117\n",
      "Gradient for fc1.weight: 0.0663747489452362\n",
      "Gradient for fc1.bias: 0.11195074021816254\n",
      "Gradient for fc2.weight: 0.08466078341007233\n",
      "Gradient for fc2.bias: 0.03275749832391739\n",
      "torch.Size([127]) tensor(-2.2173e-05, grad_fn=<MaxBackward1>) tensor(-2.6847, grad_fn=<MinBackward1>)\n",
      "Episode 941, Loss: 0.006110755261033773\n",
      "Gradient for fc1.weight: 0.017622003331780434\n",
      "Gradient for fc1.bias: 0.06520183384418488\n",
      "Gradient for fc2.weight: 0.03752806782722473\n",
      "Gradient for fc2.bias: 0.018558776006102562\n",
      "torch.Size([43]) tensor(-4.3870e-05, grad_fn=<MaxBackward1>) tensor(-1.9973, grad_fn=<MinBackward1>)\n",
      "Episode 942, Loss: 0.015777379274368286\n",
      "Gradient for fc1.weight: 0.02587735466659069\n",
      "Gradient for fc1.bias: 0.11272643506526947\n",
      "Gradient for fc2.weight: 0.05572144314646721\n",
      "Gradient for fc2.bias: 0.026424508541822433\n",
      "torch.Size([72]) tensor(-4.9950e-05, grad_fn=<MaxBackward1>) tensor(-1.2058, grad_fn=<MinBackward1>)\n",
      "Episode 943, Loss: -0.03342210501432419\n",
      "Gradient for fc1.weight: 0.05767114460468292\n",
      "Gradient for fc1.bias: 0.09318245202302933\n",
      "Gradient for fc2.weight: 0.07754968106746674\n",
      "Gradient for fc2.bias: 0.02856696955859661\n",
      "torch.Size([76]) tensor(-4.5956e-05, grad_fn=<MaxBackward1>) tensor(-2.4494, grad_fn=<MinBackward1>)\n",
      "Episode 944, Loss: 0.035941559821367264\n",
      "Gradient for fc1.weight: 0.0658353939652443\n",
      "Gradient for fc1.bias: 0.21322818100452423\n",
      "Gradient for fc2.weight: 0.13768260180950165\n",
      "Gradient for fc2.bias: 0.06656350940465927\n",
      "torch.Size([61]) tensor(-5.3109e-05, grad_fn=<MaxBackward1>) tensor(-1.3103, grad_fn=<MinBackward1>)\n",
      "Episode 945, Loss: -0.03119857795536518\n",
      "Gradient for fc1.weight: 0.03316335752606392\n",
      "Gradient for fc1.bias: 0.04753129556775093\n",
      "Gradient for fc2.weight: 0.04576370120048523\n",
      "Gradient for fc2.bias: 0.014969753101468086\n",
      "torch.Size([82]) tensor(-1.5974e-05, grad_fn=<MaxBackward1>) tensor(-4.2871, grad_fn=<MinBackward1>)\n",
      "Episode 946, Loss: 0.07340516149997711\n",
      "Gradient for fc1.weight: 0.08423289656639099\n",
      "Gradient for fc1.bias: 0.29834988713264465\n",
      "Gradient for fc2.weight: 0.19251570105552673\n",
      "Gradient for fc2.bias: 0.09503001719713211\n",
      "torch.Size([47]) tensor(-7.6297e-05, grad_fn=<MaxBackward1>) tensor(-0.9690, grad_fn=<MinBackward1>)\n",
      "Episode 947, Loss: -0.021666638553142548\n",
      "Gradient for fc1.weight: 0.06151159852743149\n",
      "Gradient for fc1.bias: 0.08290109783411026\n",
      "Gradient for fc2.weight: 0.079658143222332\n",
      "Gradient for fc2.bias: 0.0264348853379488\n",
      "torch.Size([46]) tensor(-8.8215e-06, grad_fn=<MaxBackward1>) tensor(-3.6114, grad_fn=<MinBackward1>)\n",
      "Episode 948, Loss: 0.0875999927520752\n",
      "Gradient for fc1.weight: 0.05009928345680237\n",
      "Gradient for fc1.bias: 0.036824893206357956\n",
      "Gradient for fc2.weight: 0.06073899194598198\n",
      "Gradient for fc2.bias: 0.010291576385498047\n",
      "torch.Size([47]) tensor(-6.6161e-06, grad_fn=<MaxBackward1>) tensor(-3.9337, grad_fn=<MinBackward1>)\n",
      "Episode 949, Loss: 0.2147768884897232\n",
      "Gradient for fc1.weight: 0.061207160353660583\n",
      "Gradient for fc1.bias: 0.3402438461780548\n",
      "Gradient for fc2.weight: 0.19767342507839203\n",
      "Gradient for fc2.bias: 0.10147785395383835\n",
      "torch.Size([59]) tensor(-4.6075e-05, grad_fn=<MaxBackward1>) tensor(-1.7522, grad_fn=<MinBackward1>)\n",
      "Episode 950, Loss: -0.04902192950248718\n",
      "Gradient for fc1.weight: 0.07422987371683121\n",
      "Gradient for fc1.bias: 0.1042671874165535\n",
      "Gradient for fc2.weight: 0.10090325772762299\n",
      "Gradient for fc2.bias: 0.0330236479640007\n",
      "torch.Size([56]) tensor(-2.0683e-05, grad_fn=<MaxBackward1>) tensor(-2.3541, grad_fn=<MinBackward1>)\n",
      "Episode 951, Loss: 0.028930604457855225\n",
      "Gradient for fc1.weight: 0.018595213070511818\n",
      "Gradient for fc1.bias: 0.05710715055465698\n",
      "Gradient for fc2.weight: 0.03585316613316536\n",
      "Gradient for fc2.bias: 0.013698333874344826\n",
      "torch.Size([94]) tensor(-1.7285e-05, grad_fn=<MaxBackward1>) tensor(-3.3101, grad_fn=<MinBackward1>)\n",
      "Episode 952, Loss: -0.04972792789340019\n",
      "Gradient for fc1.weight: 0.01316056214272976\n",
      "Gradient for fc1.bias: 0.014634892344474792\n",
      "Gradient for fc2.weight: 0.012737884186208248\n",
      "Gradient for fc2.bias: 0.0014795387396588922\n",
      "torch.Size([158]) tensor(-7.5224e-05, grad_fn=<MaxBackward1>) tensor(-1.4813, grad_fn=<MinBackward1>)\n",
      "Episode 953, Loss: -0.0012502745958045125\n",
      "Gradient for fc1.weight: 0.03358786180615425\n",
      "Gradient for fc1.bias: 0.025651274248957634\n",
      "Gradient for fc2.weight: 0.030671127140522003\n",
      "Gradient for fc2.bias: 0.0036358924116939306\n",
      "torch.Size([56]) tensor(-9.9187e-05, grad_fn=<MaxBackward1>) tensor(-0.9024, grad_fn=<MinBackward1>)\n",
      "Episode 954, Loss: -0.033902350813150406\n",
      "Gradient for fc1.weight: 0.012121163308620453\n",
      "Gradient for fc1.bias: 0.019937146455049515\n",
      "Gradient for fc2.weight: 0.013008498586714268\n",
      "Gradient for fc2.bias: 0.0011966368183493614\n",
      "torch.Size([50]) tensor(-1.6093e-05, grad_fn=<MaxBackward1>) tensor(-2.9532, grad_fn=<MinBackward1>)\n",
      "Episode 955, Loss: 0.06603836268186569\n",
      "Gradient for fc1.weight: 0.05154095217585564\n",
      "Gradient for fc1.bias: 0.1281460076570511\n",
      "Gradient for fc2.weight: 0.09045254439115524\n",
      "Gradient for fc2.bias: 0.03180905431509018\n",
      "torch.Size([50]) tensor(-5.6507e-05, grad_fn=<MaxBackward1>) tensor(-1.7838, grad_fn=<MinBackward1>)\n",
      "Episode 956, Loss: -0.057335615158081055\n",
      "Gradient for fc1.weight: 0.08743846416473389\n",
      "Gradient for fc1.bias: 0.1700221300125122\n",
      "Gradient for fc2.weight: 0.122050441801548\n",
      "Gradient for fc2.bias: 0.05192146077752113\n",
      "torch.Size([52]) tensor(-9.0599e-06, grad_fn=<MaxBackward1>) tensor(-5.2371, grad_fn=<MinBackward1>)\n",
      "Episode 957, Loss: 0.010557115077972412\n",
      "Gradient for fc1.weight: 0.02979058027267456\n",
      "Gradient for fc1.bias: 0.08267476409673691\n",
      "Gradient for fc2.weight: 0.05550647899508476\n",
      "Gradient for fc2.bias: 0.021183637902140617\n",
      "torch.Size([118]) tensor(-7.1290e-05, grad_fn=<MaxBackward1>) tensor(-1.1910, grad_fn=<MinBackward1>)\n",
      "Episode 958, Loss: -0.03462637960910797\n",
      "Gradient for fc1.weight: 0.023067517206072807\n",
      "Gradient for fc1.bias: 0.07157225906848907\n",
      "Gradient for fc2.weight: 0.0426490381360054\n",
      "Gradient for fc2.bias: 0.021707216277718544\n",
      "torch.Size([88]) tensor(-1.7762e-05, grad_fn=<MaxBackward1>) tensor(-4.4118, grad_fn=<MinBackward1>)\n",
      "Episode 959, Loss: 0.056351661682128906\n",
      "Gradient for fc1.weight: 0.10977167636156082\n",
      "Gradient for fc1.bias: 0.1110353171825409\n",
      "Gradient for fc2.weight: 0.14410540461540222\n",
      "Gradient for fc2.bias: 0.03271591663360596\n",
      "torch.Size([50]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-0.6667, grad_fn=<MinBackward1>)\n",
      "Episode 960, Loss: -0.005664837546646595\n",
      "Gradient for fc1.weight: 0.02449786476790905\n",
      "Gradient for fc1.bias: 0.09902942180633545\n",
      "Gradient for fc2.weight: 0.06135337054729462\n",
      "Gradient for fc2.bias: 0.03180737793445587\n",
      "torch.Size([56]) tensor(-4.0770e-05, grad_fn=<MaxBackward1>) tensor(-2.6391, grad_fn=<MinBackward1>)\n",
      "Episode 961, Loss: -0.005080107133835554\n",
      "Gradient for fc1.weight: 0.04182402789592743\n",
      "Gradient for fc1.bias: 0.12623731791973114\n",
      "Gradient for fc2.weight: 0.07888512313365936\n",
      "Gradient for fc2.bias: 0.03845902159810066\n",
      "torch.Size([79]) tensor(-1.8180e-05, grad_fn=<MaxBackward1>) tensor(-2.6800, grad_fn=<MinBackward1>)\n",
      "Episode 962, Loss: 0.0030259753111749887\n",
      "Gradient for fc1.weight: 0.019105661660432816\n",
      "Gradient for fc1.bias: 0.09445414692163467\n",
      "Gradient for fc2.weight: 0.04797649756073952\n",
      "Gradient for fc2.bias: 0.027783319354057312\n",
      "torch.Size([49]) tensor(-5.0963e-05, grad_fn=<MaxBackward1>) tensor(-2.1084, grad_fn=<MinBackward1>)\n",
      "Episode 963, Loss: -0.008196653798222542\n",
      "Gradient for fc1.weight: 0.017548104748129845\n",
      "Gradient for fc1.bias: 0.10091949999332428\n",
      "Gradient for fc2.weight: 0.06039125844836235\n",
      "Gradient for fc2.bias: 0.03115382418036461\n",
      "torch.Size([73]) tensor(-5.9010e-05, grad_fn=<MaxBackward1>) tensor(-1.5303, grad_fn=<MinBackward1>)\n",
      "Episode 964, Loss: -0.06526067852973938\n",
      "Gradient for fc1.weight: 0.07840000838041306\n",
      "Gradient for fc1.bias: 0.12968973815441132\n",
      "Gradient for fc2.weight: 0.11561841517686844\n",
      "Gradient for fc2.bias: 0.04316527768969536\n",
      "torch.Size([78]) tensor(-1.1504e-05, grad_fn=<MaxBackward1>) tensor(-3.7772, grad_fn=<MinBackward1>)\n",
      "Episode 965, Loss: -0.011016584932804108\n",
      "Gradient for fc1.weight: 0.04993046447634697\n",
      "Gradient for fc1.bias: 0.19524510204792023\n",
      "Gradient for fc2.weight: 0.11832355707883835\n",
      "Gradient for fc2.bias: 0.061084721237421036\n",
      "torch.Size([68]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-1.0523, grad_fn=<MinBackward1>)\n",
      "Episode 966, Loss: -0.01700344681739807\n",
      "Gradient for fc1.weight: 0.051156532019376755\n",
      "Gradient for fc1.bias: 0.11360687017440796\n",
      "Gradient for fc2.weight: 0.092314213514328\n",
      "Gradient for fc2.bias: 0.03820408135652542\n",
      "torch.Size([83]) tensor(-8.4821e-05, grad_fn=<MaxBackward1>) tensor(-1.0778, grad_fn=<MinBackward1>)\n",
      "Episode 967, Loss: -0.029574494808912277\n",
      "Gradient for fc1.weight: 0.021558234468102455\n",
      "Gradient for fc1.bias: 0.03211427852511406\n",
      "Gradient for fc2.weight: 0.03335025534033775\n",
      "Gradient for fc2.bias: 0.009636619128286839\n",
      "torch.Size([71]) tensor(-4.6016e-05, grad_fn=<MaxBackward1>) tensor(-1.8196, grad_fn=<MinBackward1>)\n",
      "Episode 968, Loss: 0.07166153937578201\n",
      "Gradient for fc1.weight: 0.02815297432243824\n",
      "Gradient for fc1.bias: 0.05288727208971977\n",
      "Gradient for fc2.weight: 0.04570818692445755\n",
      "Gradient for fc2.bias: 0.01771855726838112\n",
      "torch.Size([67]) tensor(-9.4772e-06, grad_fn=<MaxBackward1>) tensor(-4.0824, grad_fn=<MinBackward1>)\n",
      "Episode 969, Loss: 0.15376988053321838\n",
      "Gradient for fc1.weight: 0.02088906243443489\n",
      "Gradient for fc1.bias: 0.15365086495876312\n",
      "Gradient for fc2.weight: 0.09205390512943268\n",
      "Gradient for fc2.bias: 0.04604000598192215\n",
      "torch.Size([72]) tensor(-4.3095e-05, grad_fn=<MaxBackward1>) tensor(-2.5017, grad_fn=<MinBackward1>)\n",
      "Episode 970, Loss: -0.013002576306462288\n",
      "Gradient for fc1.weight: 0.07012628018856049\n",
      "Gradient for fc1.bias: 0.24189719557762146\n",
      "Gradient for fc2.weight: 0.16278135776519775\n",
      "Gradient for fc2.bias: 0.07932000607252121\n",
      "torch.Size([160]) tensor(-6.4256e-05, grad_fn=<MaxBackward1>) tensor(-3.5034, grad_fn=<MinBackward1>)\n",
      "Episode 971, Loss: -0.0481937900185585\n",
      "Gradient for fc1.weight: 0.04899846762418747\n",
      "Gradient for fc1.bias: 0.04000210016965866\n",
      "Gradient for fc2.weight: 0.07071365416049957\n",
      "Gradient for fc2.bias: 0.01320660300552845\n",
      "torch.Size([70]) tensor(-5.4480e-05, grad_fn=<MaxBackward1>) tensor(-1.4480, grad_fn=<MinBackward1>)\n",
      "Episode 972, Loss: -0.04359433799982071\n",
      "Gradient for fc1.weight: 0.06286932528018951\n",
      "Gradient for fc1.bias: 0.22367316484451294\n",
      "Gradient for fc2.weight: 0.14356978237628937\n",
      "Gradient for fc2.bias: 0.07038484513759613\n",
      "torch.Size([63]) tensor(-3.0697e-05, grad_fn=<MaxBackward1>) tensor(-2.7720, grad_fn=<MinBackward1>)\n",
      "Episode 973, Loss: -0.007850323803722858\n",
      "Gradient for fc1.weight: 0.12037421017885208\n",
      "Gradient for fc1.bias: 0.11994283646345139\n",
      "Gradient for fc2.weight: 0.16731123626232147\n",
      "Gradient for fc2.bias: 0.03758693486452103\n",
      "torch.Size([72]) tensor(-3.7373e-05, grad_fn=<MaxBackward1>) tensor(-2.3489, grad_fn=<MinBackward1>)\n",
      "Episode 974, Loss: 0.11843153834342957\n",
      "Gradient for fc1.weight: 0.03135092183947563\n",
      "Gradient for fc1.bias: 0.25590401887893677\n",
      "Gradient for fc2.weight: 0.1556607186794281\n",
      "Gradient for fc2.bias: 0.0805739164352417\n",
      "torch.Size([94]) tensor(-8.7384e-05, grad_fn=<MaxBackward1>) tensor(-1.4995, grad_fn=<MinBackward1>)\n",
      "Episode 975, Loss: 0.03894748166203499\n",
      "Gradient for fc1.weight: 0.01304326206445694\n",
      "Gradient for fc1.bias: 0.019864805042743683\n",
      "Gradient for fc2.weight: 0.011020256206393242\n",
      "Gradient for fc2.bias: 0.0007373512489721179\n",
      "torch.Size([92]) tensor(-4.2201e-05, grad_fn=<MaxBackward1>) tensor(-3.1771, grad_fn=<MinBackward1>)\n",
      "Episode 976, Loss: 0.04408015310764313\n",
      "Gradient for fc1.weight: 0.02396419085562229\n",
      "Gradient for fc1.bias: 0.0294791292399168\n",
      "Gradient for fc2.weight: 0.021290723234415054\n",
      "Gradient for fc2.bias: 0.0045945183373987675\n",
      "torch.Size([71]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-1.5581, grad_fn=<MinBackward1>)\n",
      "Episode 977, Loss: -0.0633004754781723\n",
      "Gradient for fc1.weight: 0.021816127002239227\n",
      "Gradient for fc1.bias: 0.10698271542787552\n",
      "Gradient for fc2.weight: 0.05145002901554108\n",
      "Gradient for fc2.bias: 0.03030412644147873\n",
      "torch.Size([77]) tensor(-1.4067e-05, grad_fn=<MaxBackward1>) tensor(-3.2628, grad_fn=<MinBackward1>)\n",
      "Episode 978, Loss: 0.1023000180721283\n",
      "Gradient for fc1.weight: 0.024365847930312157\n",
      "Gradient for fc1.bias: 0.14062988758087158\n",
      "Gradient for fc2.weight: 0.0841170996427536\n",
      "Gradient for fc2.bias: 0.04101990535855293\n",
      "torch.Size([81]) tensor(-2.1518e-05, grad_fn=<MaxBackward1>) tensor(-4.1280, grad_fn=<MinBackward1>)\n",
      "Episode 979, Loss: -0.013083272613584995\n",
      "Gradient for fc1.weight: 0.061781201511621475\n",
      "Gradient for fc1.bias: 0.05308617278933525\n",
      "Gradient for fc2.weight: 0.08465524762868881\n",
      "Gradient for fc2.bias: 0.01751800626516342\n",
      "torch.Size([128]) tensor(-3.4452e-05, grad_fn=<MaxBackward1>) tensor(-3.0246, grad_fn=<MinBackward1>)\n",
      "Episode 980, Loss: -0.01030021533370018\n",
      "Gradient for fc1.weight: 0.040372446179389954\n",
      "Gradient for fc1.bias: 0.03891223669052124\n",
      "Gradient for fc2.weight: 0.053770117461681366\n",
      "Gradient for fc2.bias: 0.010618734173476696\n",
      "torch.Size([59]) tensor(-5.1321e-05, grad_fn=<MaxBackward1>) tensor(-2.7882, grad_fn=<MinBackward1>)\n",
      "Episode 981, Loss: 0.033616840839385986\n",
      "Gradient for fc1.weight: 0.015665968880057335\n",
      "Gradient for fc1.bias: 0.03331483155488968\n",
      "Gradient for fc2.weight: 0.023523712530732155\n",
      "Gradient for fc2.bias: 0.006921328604221344\n",
      "torch.Size([90]) tensor(-3.7432e-05, grad_fn=<MaxBackward1>) tensor(-2.7601, grad_fn=<MinBackward1>)\n",
      "Episode 982, Loss: 0.0307166688144207\n",
      "Gradient for fc1.weight: 0.02943044900894165\n",
      "Gradient for fc1.bias: 0.030290910974144936\n",
      "Gradient for fc2.weight: 0.03353476524353027\n",
      "Gradient for fc2.bias: 0.005559990648180246\n",
      "torch.Size([89]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-1.7752, grad_fn=<MinBackward1>)\n",
      "Episode 983, Loss: 0.056403085589408875\n",
      "Gradient for fc1.weight: 0.016085205599665642\n",
      "Gradient for fc1.bias: 0.24283762276172638\n",
      "Gradient for fc2.weight: 0.13264183700084686\n",
      "Gradient for fc2.bias: 0.07391000539064407\n",
      "torch.Size([155]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-1.5949, grad_fn=<MinBackward1>)\n",
      "Episode 984, Loss: 0.06842205673456192\n",
      "Gradient for fc1.weight: 0.022579632699489594\n",
      "Gradient for fc1.bias: 0.028322990983724594\n",
      "Gradient for fc2.weight: 0.03284395858645439\n",
      "Gradient for fc2.bias: 0.008491230197250843\n",
      "torch.Size([62]) tensor(-9.8949e-05, grad_fn=<MaxBackward1>) tensor(-1.5742, grad_fn=<MinBackward1>)\n",
      "Episode 985, Loss: 0.04092828929424286\n",
      "Gradient for fc1.weight: 0.08261910825967789\n",
      "Gradient for fc1.bias: 0.12313999980688095\n",
      "Gradient for fc2.weight: 0.13994595408439636\n",
      "Gradient for fc2.bias: 0.04425995051860809\n",
      "torch.Size([73]) tensor(-4.7685e-05, grad_fn=<MaxBackward1>) tensor(-3.6306, grad_fn=<MinBackward1>)\n",
      "Episode 986, Loss: 0.04697442427277565\n",
      "Gradient for fc1.weight: 0.06552787870168686\n",
      "Gradient for fc1.bias: 0.10595909506082535\n",
      "Gradient for fc2.weight: 0.11357910931110382\n",
      "Gradient for fc2.bias: 0.03737744688987732\n",
      "torch.Size([96]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-1.0602, grad_fn=<MinBackward1>)\n",
      "Episode 987, Loss: -0.03852096572518349\n",
      "Gradient for fc1.weight: 0.0390898622572422\n",
      "Gradient for fc1.bias: 0.06081673875451088\n",
      "Gradient for fc2.weight: 0.07328877598047256\n",
      "Gradient for fc2.bias: 0.020962294191122055\n",
      "torch.Size([68]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-1.2091, grad_fn=<MinBackward1>)\n",
      "Episode 988, Loss: -0.005422685295343399\n",
      "Gradient for fc1.weight: 0.05023828521370888\n",
      "Gradient for fc1.bias: 0.041385017335414886\n",
      "Gradient for fc2.weight: 0.06312476843595505\n",
      "Gradient for fc2.bias: 0.004730899818241596\n",
      "torch.Size([128]) tensor(-1.5676e-05, grad_fn=<MaxBackward1>) tensor(-3.5225, grad_fn=<MinBackward1>)\n",
      "Episode 989, Loss: 0.010700013488531113\n",
      "Gradient for fc1.weight: 0.03853055462241173\n",
      "Gradient for fc1.bias: 0.050117552280426025\n",
      "Gradient for fc2.weight: 0.05872194841504097\n",
      "Gradient for fc2.bias: 0.01636362448334694\n",
      "torch.Size([100]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-2.7676, grad_fn=<MinBackward1>)\n",
      "Episode 990, Loss: 0.029079698026180267\n",
      "Gradient for fc1.weight: 0.07436520606279373\n",
      "Gradient for fc1.bias: 0.12920764088630676\n",
      "Gradient for fc2.weight: 0.09582511335611343\n",
      "Gradient for fc2.bias: 0.026100941002368927\n",
      "torch.Size([63]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-1.5042, grad_fn=<MinBackward1>)\n",
      "Episode 991, Loss: -0.016867155209183693\n",
      "Gradient for fc1.weight: 0.01638038456439972\n",
      "Gradient for fc1.bias: 0.15623198449611664\n",
      "Gradient for fc2.weight: 0.08365991711616516\n",
      "Gradient for fc2.bias: 0.04686570540070534\n",
      "torch.Size([80]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-2.3597, grad_fn=<MinBackward1>)\n",
      "Episode 992, Loss: -0.05559353157877922\n",
      "Gradient for fc1.weight: 0.04507352039217949\n",
      "Gradient for fc1.bias: 0.10408486425876617\n",
      "Gradient for fc2.weight: 0.0993579775094986\n",
      "Gradient for fc2.bias: 0.038439519703388214\n",
      "torch.Size([104]) tensor(-5.4599e-05, grad_fn=<MaxBackward1>) tensor(-2.0362, grad_fn=<MinBackward1>)\n",
      "Episode 993, Loss: -0.031960777938365936\n",
      "Gradient for fc1.weight: 0.007738073822110891\n",
      "Gradient for fc1.bias: 0.03042563982307911\n",
      "Gradient for fc2.weight: 0.018189379945397377\n",
      "Gradient for fc2.bias: 0.008321266621351242\n",
      "torch.Size([58]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-1.4762, grad_fn=<MinBackward1>)\n",
      "Episode 994, Loss: 0.06246776878833771\n",
      "Gradient for fc1.weight: 0.01767498441040516\n",
      "Gradient for fc1.bias: 0.07119908928871155\n",
      "Gradient for fc2.weight: 0.050033897161483765\n",
      "Gradient for fc2.bias: 0.021713264286518097\n",
      "torch.Size([75]) tensor(-0.0001, grad_fn=<MaxBackward1>) tensor(-1.3390, grad_fn=<MinBackward1>)\n",
      "Episode 995, Loss: -0.02371341735124588\n",
      "Gradient for fc1.weight: 0.04328957572579384\n",
      "Gradient for fc1.bias: 0.07253749668598175\n",
      "Gradient for fc2.weight: 0.07981100678443909\n",
      "Gradient for fc2.bias: 0.026813596487045288\n",
      "torch.Size([46]) tensor(-6.8786e-05, grad_fn=<MaxBackward1>) tensor(-3.2493, grad_fn=<MinBackward1>)\n",
      "Episode 996, Loss: 0.0011525970185175538\n",
      "Gradient for fc1.weight: 0.040894899517297745\n",
      "Gradient for fc1.bias: 0.06282369047403336\n",
      "Gradient for fc2.weight: 0.04850337281823158\n",
      "Gradient for fc2.bias: 0.01149185560643673\n",
      "torch.Size([72]) tensor(-0.0002, grad_fn=<MaxBackward1>) tensor(-1.9208, grad_fn=<MinBackward1>)\n",
      "Episode 997, Loss: -0.03459958732128143\n",
      "Gradient for fc1.weight: 0.0994628444314003\n",
      "Gradient for fc1.bias: 0.257551908493042\n",
      "Gradient for fc2.weight: 0.2256224900484085\n",
      "Gradient for fc2.bias: 0.09268485754728317\n",
      "torch.Size([91]) tensor(-5.3169e-05, grad_fn=<MaxBackward1>) tensor(-2.1946, grad_fn=<MinBackward1>)\n",
      "Episode 998, Loss: -0.015583066269755363\n",
      "Gradient for fc1.weight: 0.016603244468569756\n",
      "Gradient for fc1.bias: 0.047852274030447006\n",
      "Gradient for fc2.weight: 0.02419998124241829\n",
      "Gradient for fc2.bias: 0.01357112918049097\n",
      "torch.Size([153]) tensor(-7.0098e-05, grad_fn=<MaxBackward1>) tensor(-2.9935, grad_fn=<MinBackward1>)\n",
      "Episode 999, Loss: -0.03586414456367493\n",
      "Gradient for fc1.weight: 0.021790286526083946\n",
      "Gradient for fc1.bias: 0.041788771748542786\n",
      "Gradient for fc2.weight: 0.0419347807765007\n",
      "Gradient for fc2.bias: 0.013808920979499817\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In the new version of Gymnasium you need different environments for rendering and no rendering.\n",
    "# Here we instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "# down), and another that does not animate.\n",
    "seed = 2112\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "env = gymnasium.make('CartPole-v1')\n",
    "env.reset(seed=seed)\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# PyGame, please don't crash.\n",
    "pygame.display.init()\n",
    "\n",
    "# Make a policy network.\n",
    "policy = PolicyNet(env)\n",
    "\n",
    "# Train the agent.\n",
    "plt.plot(reinforce(policy, env, env_render, num_episodes=1000))\n",
    "\n",
    "# Close up everything\n",
    "env_render.close()\n",
    "env.close()\n",
    "pygame.display.quit()  # Fingers crossed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e40868e-7f32-4409-9f07-1fe781de45da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# And run the final agent for a few episodes.\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "for _ in range(10):\n",
    "    run_episode(env_render, policy)\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202cea9-aef2-46da-ace9-ddf25bca77be",
   "metadata": {},
   "source": [
    "## For your consideration\n",
    "\n",
    "There are many things that can be improved in this example. Some things you can think about:\n",
    "\n",
    "1. **Development Environment**. If there is *one* laboratory for which you don't use a Jupyter Notebook, *this should definitely be the one*. PyGame (for visualizing episodes in the environment) interacts very badly with the Jupyter Python kernels at times. Consider using PyCharm or VSCode.\n",
    " \n",
    "2. **Better Logging**. Monitoring just a moving average return is a pretty coarse metric of performance -- one that with also *depend on the discount factor $\\gamma$*. Think about what might be better statistics to collect and report, and also *how to collect and report them*. You should think about using Weights and Biases or Tensorboard for this.\n",
    "\n",
    "3. **Checkpointing**. When training is unstable, it can be a good idea to *checkpoint* your models both periodically (so you can restart training from a specific checkpoint) and for each *best* model as determined by you running performance measure(s). \n",
    "\n",
    "4. **Exploration**. The model is probably overfitting (or perhaps remaining too *plastic*, which can explain the unstable convergence). Our policy is *always* stochastic in that we sample from the output distribution. It would be interesting to add a temperature parameter to the policy so that we can control this behavior, or even implement a deterministic policy sampler that always selects the action with max probability to evaluate the quality of the learned policy network.\n",
    "\n",
    "5. **Discount Factor**: The discount factor (default $\\gamma = 0.99$) is an important hyperparameter that has an effect on the stability of training. Try different values for $\\gamma$ and see how it affects training. Can you think of other ways to stabilize training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2dd69-4806-44fb-acb4-1b26c0d93cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
